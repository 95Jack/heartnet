=== ComParE 2018, Heartbeat Task ===

2018/02/01

I. Overview

The Heartbeats Sub-Challenge is about classifying Heartbeat anomalies (0 = normal, 1 = abnormal-mild, 2 = abnormal-moderate/severe).

This Sub-Challenge is based on the Heart Sounds Shenzhen (SHS) database which is kindly provided by Prof Fengquan Dong and colleagues. 
It is a data set of heart sounds gathered from 170 subjects (115 male / 55 female, ages ranging from 21 to 88). 
The data set was collected by Shenzhen University, Shenzhen, China. Each instance (approx. 30 seconds) is labelled with one of the three classes:
'normal' (0), 'mild' (1) and 'moderate/severe' (2) as diagnosed by physicians specialised in heart disease. 
The labels need to be recognised from the acoustics of the signal.


Perl and Shell scripts are provided to reproduce the baseline on Linux systems.
They have been tested on Ubuntu 16.04 installation.

The participants are required to provide a Heartbeat label (0/1/2) for every instance in the test set, along with a confidence score.


II Baselines

IIa. Reproducing the standard baseline

- Make sure Weka's JAR file (weka.jar) is in an accessible location
- Adjust the path name in baseline*.sh to match the location of weka.jar on your system
- Change to the "baseline" directory.
- Execute
  sh baseline*.sh <complexity> <epsilon>
  where `complexity` is the SVM complexity (default: C=1.0E-4)
  and 'epsilon' is the epsilon-intensive loss L (default: L=0.1)
  to get results for the binary classification task.
- To access predictions and results, look in the baseline/eval/ folders.


IIb. Reproducing the auDeep baseline (folder baseline_audeep)

auDeep (https://github.com/audeep/audeep) is a Python toolkit for unsupervised feature learning with deep neural networks 
(DNNs). Currently, the main focus of this project is feature extraction from audio data with deep recurrent autoencoders. 
However, the core feature learning algorithms are not limited to audio data. For ComParE 2018, learned representations are 
obtained with a sequence-to-sequence autoencoder from spectrograms of the audio files.

- Install auDeep as per the instructions provided with the auDeep distribution
- Activate the Python virtualenv into which auDeep was installed (source <path-to-virtualenv>/bin/activate)
- Change to the "baseline_audeep" directory.
- Run the audeep_generate.sh script
- Execute
  sh baseline_train_devel.sh <complexity> <epsilon> <feature-variant>
  where `complexity` is the SVM complexity (default: C=1.0E-2)
  and 'epsilon' is the epsilon-intensive loss L (default: L=0.1)
  and 'feature-variant' is the specific type of learned representations to use (default fused).
  Possible options for 'feature-variant' are '30', '45', '60', '75' for representations learned on spectrograms in which
  power levels below -30 dB, -45 dB, -60 dB, -75 dB have been clipped, respectively. Additionally 'fused' can be
  specified to use all above representations after feature-level fusion.
- To access predictions and results, look in the baseline_audeep/eval/ folders.
- Parameters for auDeep can be adjusted in the audeep_generate.sh script. If you want to re-run the feature generation
  process after changing parameters, you need to delete the audeep_workspace folder. Also, adjusting the parameters may 
  affect the number of features in the learned representations. In this case, you will need to adjust the label_index_*
  variables in the baseline_*.sh scripts.  

If you use auDeep or any code from auDeep in your research work, you are kindly asked to acknowledge the use of auDeep in your 
publications:
- S. Amiriparian, M. Freitag, N. Cummins, und B. Schuller. 'Sequence to sequence autoencoders for unsupervised representation 
  learning from audio', Detection and Classification of Acoustic Scenes and Events 2017
- M. Freitag, S. Amiriparian, S. Pugachevskiy, N. Cummins, and B. Schuller. 'auDeep: Unsupervised Learning of Representations 
  from Audio with Deep Recurrent Neural Networks', arXiv preprint arXiv:1712.04382, 2017


IIc. Reproducing the BoAW baseline (folder baseline_boaw)

The bag-of-audio-words (BoAW) approach is based on acoustic low-level descriptors (LLDs) extracted with the openSMILE toolkit.
In the Snore sub-challenge, the codebook of BoAW is learnt from all training data.

- Make sure openSMILE 2.3 (http://www.audeering.com/research/opensmile) SMILExtract is in an accessible location
- Make sure Weka's JAR file (weka.jar) is in an accessible location
- Change to the "baseline" directory.
- Modify the path of the openSMILE executable and the ComParE_2016.conf configuration file in extract_llds.sh
- Run extract_llds.sh to extract the 130 ComParE low-level descriptors from the wav files of all partitions
- Adjust the path name in baseline*.sh to match the location of weka.jar on your system
- Execute
  sh baseline_boaw*.sh <complexity> <epsilon>
  where `complexity` is the SVM complexity (default: C=1.0E-4)
  and 'epsilon' is the epsilon-intensive loss L (default: L=0.1)
  to get results for the binary classification task.
- To access predictions and results, look in the baseline_boaw/eval/ folders.
- You can modify the parameters of openXBOW (a tutorial is given on https://github.com/openXBOW/openXBOW).
  NOTE: The parameters of openXBOW must be modified in both scripts baseline_boaw*.sh.
  NOTE: As the configuration of openXBOW is not encoded into the filenames of model files (codebook and Weka) and evaluation files, 
  you need to remove (or rename) all files when changing the BoAW configuration. In the baseline_boaw folder, execute
  rm ../arff/*.BoAW.*
  rm -r models/
  rm -r eval/


IId. Reproducing the End-to-End baseline (folder baseline_end2you)
    Run the script train_model_heartbeat.sh to convert the labels in the tsv file
    To download the end-to-end toolkit End2You, please visit: 
     https://github.com/end2you/end2you
    and follow the instructions there.
    
    General workflow:
    
    # Generation (for training and development sets only)
    python main.py --tfrecords_folder=tf_records --input_type=audio generate --data_file=/path/to/file.tsv
    
    # Training and Evaluation
    python main.py --tfrecords_folder=tf_records --input_type=audio --seq_length=0 --task=classification --num_classes=3 train --loss=cewl
    
    # Test Raw (need to provide a `test_file.tsv` with the path of the wav files)
    python main.py --input_type=audio --seq_length=0 --task=classification --num_classes=3 test --data_file=/path/to/test_file.tsv --model_path=/path/to/model.ckpt-XXXX 


III. Directory structure

wav/: Waveform files for training and test sets
	Filename convention: <set>_NNNN.wav where 
	`set` in { train, devel, test} and 
	NNNN = randomised sample number 

lab/: Label files
	ComParE2018_Heartbeat.tsv: Containing the labels and meta data of the training and development sets
	Fields:
	 file_name
	 label: (0 = normal, 1 = abnormal-mild, 2 = abnormal-moderate/severe)

arff/: Weka ARFF files containing instance supra-segmental features and labels
	(test: no labels given)
	Naming convention: ComParE2018_Heartbeat.ComParE.<set>.arff
	where 'set' in {train, devel, test}

baseline/: * Baseline recognition system *

	baseline*.sh: Scripts for development and test set evaluation
	Please adjust the file system paths in this script to match your
	system. A working Weka installation is required.

	This script calls the following Perl scripts:
	
	upsample.pl: Upsampling the training partitions by specified factors

	join_arffs.pl: Joining training and development ARFF files for evaluation on the test set.

	format_pred.pl: Converting Weka predictions to ARFF format (similar to final result submission format)

	score.pl: Calculate classification scores from reference and prediction ARFFs

	In addition, the following files are given:

	extract_features.sh: This script reproduces the extraction of the ComParE features provided in the folder arff/
	
	arff_targets.conf.inc: openSMILE arff targets file (used by extract_features.sh)

baseline_audeep/: * Baseline recognition system based on auDeep features *
	audeep_generate.sh: Script to extract the audeep features
	
	For the other files, please see the description of the folder baseline/

baseline_boaw/: * Baseline recognition system with bag-of-audio-words *
	extract_llds.sh: Extract all LLDs required for BoAW.
	A working openSMILE installation is required.
	Run this script first!

	baseline_boaw*.sh: Scripts for devel and test set evaluation

	Please see the description of baseline/
	In addition to that, the following files are included:

	openXBOW.jar: Please visit: https://github.com/openXBOW/openXBOW

baseline_end2you/: * Baseline recognition system based on end-to-end learning *
	train_model_heartbeat.sh: Train an End2You model for the Heartbeat data


IV. Baseline results (development set)

This is only for sanity checks, more results will be provided in the baseline
paper (check the webpage for updates).

$ sh baseline_train_devel.sh

Accuracy = 52.2%
UAR = 50.3%

