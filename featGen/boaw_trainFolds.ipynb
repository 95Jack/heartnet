{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import arff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_boaw/feat/fold1.train.4096.arff\n",
      "True\n",
      "baseline_boaw/feat/fold1.dev.4096.arff\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "foldnum = 1\n",
    "codebook = 4096\n",
    "feat_path = os.path.join('baseline_boaw/feat/')\n",
    "meta_path = os.path.join('..','..','feature','mfcc')\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = dict()\n",
    "y = dict()\n",
    "\n",
    "for subset in ['train','dev']:\n",
    "    BOAW_feat = os.path.join(feat_path,\n",
    "                             'fold%d.%s.%d.arff' % (foldnum,subset,codebook))\n",
    "    BOAW_meta = os.path.join(meta_path,'fold%d.%s.filenames.txt' % (foldnum,subset))\n",
    "    print(BOAW_feat)\n",
    "    print(os.path.isfile(BOAW_meta))\n",
    "    \n",
    "    df = arff.load(open(BOAW_feat,'r'))\n",
    "    boawData = pd.DataFrame(np.array(df['data'],dtype=float))\n",
    "    del df\n",
    "\n",
    "    boawFilenames = []\n",
    "    with open(BOAW_meta, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        boawFilenames.append(line.split('\\n')[0])\n",
    "        while line:\n",
    "                line = fp.readline()\n",
    "                boawFilenames.append(line.split('\\n')[0])\n",
    "    boawFilenames = boawFilenames[:-1]\n",
    "    boawData['filenames'] = boawFilenames\n",
    "    boawData.rename({len(boawData.columns)-2:'label'},axis='columns',inplace=True)\n",
    "    boawData.label = [int(each) for each in boawData.label]\n",
    "    boawData['dataset'] = [each[-1][0] for each in boawData.filenames.str.split('_')]\n",
    "    boawData.set_index('filenames',drop=True,inplace=True)\n",
    "\n",
    "    del boawFilenames\n",
    "    \n",
    "    X[subset] = scaler.fit_transform(boawData[range(4096)].values)\n",
    "    y[subset] = boawData.label.values\n",
    "    \n",
    "    del boawData\n",
    "    \n",
    "X_train = X['train']\n",
    "X_val = X['dev']\n",
    "y_train = y['train']\n",
    "y_val = y['dev']\n",
    "\n",
    "del X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verboseMetrics(y_val,softScores):\n",
    "    eps = 1.1e-5\n",
    "    if Counter(y_val).keys()[-1] == 1:\n",
    "        TN, FP, FN, TP = confusion_matrix(y_val, np.argmax(softScores,axis=-1), labels=[0,1]).ravel()\n",
    "        sensitivity = TP / (TP + FN + eps)\n",
    "        specificity = TN / (TN + FP + eps)\n",
    "        precision = TP / (TP + FP + eps)\n",
    "        F1 = 2 * (precision * sensitivity) / (precision + sensitivity + eps)\n",
    "        Macc = (sensitivity + specificity) / 2\n",
    "        print(\"TN:{},FP:{},FN:{},TP:{},Sensitivity:{},Specificity{},Macc:{},F1:{}\".format(TN, FP, FN, TP,sensitivity,specificity,Macc,F1))\n",
    "        return Macc\n",
    "    elif Counter(y_val).keys()[-1] == 2:\n",
    "        logs = dict()\n",
    "        confmat = confusion_matrix(y_pred=np.argmax(softScores,axis=-1), y_true=y_val)\n",
    "        logs['recall0'] = confmat[0, 0] / np.sum(confmat[0, :])\n",
    "        logs['recall1'] = confmat[1, 1] / np.sum(confmat[1, :])\n",
    "        logs['recall2'] = confmat[2, 2] / np.sum(confmat[2, :])\n",
    "        logs['UAR'] = np.mean([logs['recall0'], logs['recall1'], logs['recall2']])\n",
    "        print(logs.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Bag Score Boaw 0.840982\n",
      "Validation split accuracy Boaw 0.809942\n",
      "TN:161,FP:10,FN:55,TP:116,Sensitivity:0.678362529462,Specificity0.941520407271,Macc:0.809941468366,F1:0.781139349582\n"
     ]
    }
   ],
   "source": [
    "## BOAW Train\n",
    "\n",
    "n_estimators = 100\n",
    "rand_state = 1\n",
    "\n",
    "clf = NuSVC(kernel='linear',probability=True,verbose=False,\n",
    "          random_state=rand_state,\n",
    "            nu = .5)\n",
    "\n",
    "boawBbc = BalancedBaggingClassifier(base_estimator=clf,\n",
    "                                n_estimators=n_estimators,\n",
    "                                oob_score = True,\n",
    "                                random_state=rand_state,\n",
    "                                sampling_strategy = 'not minority'\n",
    "                               )\n",
    "boawBbc.fit(X_train,y_train)\n",
    "# print(\"Out of Bag Score Boaw %f\" % boawBbc.oob_score_)\n",
    "# print(\"Validation split accuracy Boaw %f\" % accuracy_score(y_val,boawBbc.predict(X_val)))\n",
    "verboseMetrics(y_val,boawBbc.predict_proba(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM 0\n",
      "TN:154,FP:17,FN:39,TP:132,Sensitivity:0.771929774905,Specificity0.900584737389,Macc:0.836257256147,F1:0.824994469314\n",
      "Training SVM 1\n",
      "TN:155,FP:16,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.906432690229,Macc:0.827485326887,F1:0.812692896383\n",
      "Training SVM 2\n",
      "TN:162,FP:9,FN:46,TP:125,Sensitivity:0.730994105024,Specificity0.947368360111,Macc:0.839181232567,F1:0.819666653\n",
      "Training SVM 3\n",
      "TN:157,FP:14,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.91812859591,Macc:0.809941468366,F1:0.786879770121\n",
      "Training SVM 4\n",
      "TN:160,FP:11,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.93567245443,Macc:0.827485326887,F1:0.806551899848\n",
      "Training SVM 5\n",
      "TN:159,FP:12,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.92982450159,Macc:0.824561350467,F1:0.80391608699\n",
      "Training SVM 6\n",
      "TN:157,FP:14,FN:42,TP:129,Sensitivity:0.754385916385,Specificity0.91812859591,Macc:0.836257256147,F1:0.821650537157\n",
      "Training SVM 7\n",
      "TN:161,FP:10,FN:42,TP:129,Sensitivity:0.754385916385,Specificity0.941520407271,Macc:0.847953161828,F1:0.832252564094\n",
      "Training SVM 8\n",
      "TN:154,FP:17,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.900584737389,Macc:0.824561350467,F1:0.810121063148\n",
      "Training SVM 9\n",
      "TN:161,FP:10,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.941520407271,Macc:0.842105208987,F1:0.824669832828\n",
      "Training SVM 10\n",
      "TN:162,FP:9,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.947368360111,Macc:0.824561350467,F1:0.79999454917\n",
      "Training SVM 11\n",
      "TN:160,FP:11,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.93567245443,Macc:0.824561350467,F1:0.802626106836\n",
      "Training SVM 12\n",
      "TN:158,FP:13,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.92397654875,Macc:0.824561350467,F1:0.80518931474\n",
      "Training SVM 13\n",
      "TN:159,FP:12,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.92982450159,Macc:0.824561350467,F1:0.80391608699\n",
      "Training SVM 14\n",
      "TN:156,FP:15,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.91228064307,Macc:0.818713397627,F1:0.799994501868\n",
      "Training SVM 15\n",
      "TN:155,FP:16,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.906432690229,Macc:0.812865444786,F1:0.793542889423\n",
      "Training SVM 16\n",
      "TN:161,FP:10,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.941520407271,Macc:0.833333279727,F1:0.813109276424\n",
      "Training SVM 17\n",
      "TN:155,FP:16,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.906432690229,Macc:0.815789421207,F1:0.797422151007\n",
      "Training SVM 18\n",
      "TN:155,FP:16,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.906432690229,Macc:0.827485326887,F1:0.812692896383\n",
      "Training SVM 19\n",
      "TN:159,FP:12,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.92982450159,Macc:0.836257256147,F1:0.819349339204\n",
      "Training SVM 20\n",
      "TN:157,FP:14,FN:45,TP:126,Sensitivity:0.736842057864,Specificity0.91812859591,Macc:0.827485326887,F1:0.810283886431\n",
      "Training SVM 21\n",
      "TN:156,FP:15,FN:45,TP:126,Sensitivity:0.736842057864,Specificity0.91228064307,Macc:0.824561350467,F1:0.807686801627\n",
      "Training SVM 22\n",
      "TN:154,FP:17,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.900584737389,Macc:0.812865444786,F1:0.794866289711\n",
      "Training SVM 23\n",
      "TN:159,FP:12,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.92982450159,Macc:0.815789421207,F1:0.792073741566\n",
      "Training SVM 24\n",
      "TN:154,FP:17,FN:40,TP:131,Sensitivity:0.766081822065,Specificity0.900584737389,Macc:0.833333279727,F1:0.821311086405\n",
      "Training SVM 25\n",
      "TN:155,FP:16,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.906432690229,Macc:0.812865444786,F1:0.793542889423\n",
      "Training SVM 26\n",
      "TN:160,FP:11,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.93567245443,Macc:0.818713397627,F1:0.794696525386\n",
      "Training SVM 27\n",
      "TN:155,FP:16,FN:50,TP:121,Sensitivity:0.707602293663,Specificity0.906432690229,Macc:0.807017491946,F1:0.785708796652\n",
      "Training SVM 28\n",
      "TN:160,FP:11,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.93567245443,Macc:0.818713397627,F1:0.794696525386\n",
      "Training SVM 29\n",
      "TN:161,FP:10,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.941520407271,Macc:0.845029185407,F1:0.828473468181\n",
      "Training SVM 30\n",
      "TN:160,FP:11,FN:52,TP:119,Sensitivity:0.695906387983,Specificity0.93567245443,Macc:0.815789421207,F1:0.79069221871\n",
      "Training SVM 31\n",
      "TN:157,FP:14,FN:45,TP:126,Sensitivity:0.736842057864,Specificity0.91812859591,Macc:0.827485326887,F1:0.810283886431\n",
      "Training SVM 32\n",
      "TN:155,FP:16,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.906432690229,Macc:0.809941468366,F1:0.789638519492\n",
      "Training SVM 33\n",
      "TN:159,FP:12,FN:45,TP:126,Sensitivity:0.736842057864,Specificity0.92982450159,Macc:0.833333279727,F1:0.815528485285\n",
      "Training SVM 34\n",
      "TN:157,FP:14,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.91812859591,Macc:0.809941468366,F1:0.786879770121\n",
      "Training SVM 35\n",
      "TN:156,FP:15,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.91228064307,Macc:0.815789421207,F1:0.79611101094\n",
      "Training SVM 36\n",
      "TN:156,FP:15,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.91228064307,Macc:0.827485326887,F1:0.811496087656\n",
      "Training SVM 37\n",
      "TN:156,FP:15,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.91228064307,Macc:0.827485326887,F1:0.811496087656\n",
      "Training SVM 38\n",
      "TN:157,FP:14,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.91812859591,Macc:0.821637374047,F1:0.802583502388\n",
      "Training SVM 39\n",
      "TN:158,FP:13,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.92397654875,Macc:0.833333279727,F1:0.816714754143\n",
      "Training SVM 40\n",
      "TN:158,FP:13,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.92397654875,Macc:0.821637374047,F1:0.801297445697\n",
      "Training SVM 41\n",
      "TN:160,FP:11,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.93567245443,Macc:0.827485326887,F1:0.806551899848\n",
      "Training SVM 42\n",
      "TN:158,FP:13,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.92397654875,Macc:0.818713397627,F1:0.797380139748\n",
      "Training SVM 43\n",
      "TN:161,FP:10,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.941520407271,Macc:0.842105208987,F1:0.824669832828\n",
      "Training SVM 44\n",
      "TN:156,FP:15,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.91228064307,Macc:0.827485326887,F1:0.811496087656\n",
      "Training SVM 45\n",
      "TN:157,FP:14,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.91812859591,Macc:0.818713397627,F1:0.79869580871\n",
      "Training SVM 46\n",
      "TN:157,FP:14,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.91812859591,Macc:0.830409303307,F1:0.814097057585\n",
      "Training SVM 47\n",
      "TN:157,FP:14,FN:54,TP:117,Sensitivity:0.684210482302,Specificity0.91812859591,Macc:0.801169539106,F1:0.774828977166\n",
      "Training SVM 48\n",
      "TN:156,FP:15,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.91228064307,Macc:0.830409303307,F1:0.815281110852\n",
      "Training SVM 49\n",
      "TN:160,FP:11,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.93567245443,Macc:0.830409303307,F1:0.810452034233\n",
      "Training SVM 50\n",
      "TN:164,FP:7,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.959064265791,Macc:0.850877138248,F1:0.832781406152\n",
      "Training SVM 51\n",
      "TN:161,FP:10,FN:39,TP:132,Sensitivity:0.771929774905,Specificity0.941520407271,Macc:0.856725091088,F1:0.843444967198\n",
      "Training SVM 52\n",
      "TN:159,FP:12,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.92982450159,Macc:0.815789421207,F1:0.792073741566\n",
      "Training SVM 53\n",
      "TN:159,FP:12,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.92982450159,Macc:0.824561350467,F1:0.80391608699\n",
      "Training SVM 54\n",
      "TN:156,FP:15,FN:52,TP:119,Sensitivity:0.695906387983,Specificity0.91228064307,Macc:0.804093515526,F1:0.780322393545\n",
      "Training SVM 55\n",
      "TN:153,FP:18,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.894736784549,Macc:0.804093515526,F1:0.784560415584\n",
      "Training SVM 56\n",
      "TN:163,FP:8,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.953216312951,Macc:0.847953161828,F1:0.830059875959\n",
      "Training SVM 57\n",
      "TN:160,FP:11,FN:46,TP:125,Sensitivity:0.730994105024,Specificity0.93567245443,Macc:0.833333279727,F1:0.814326760723\n",
      "Training SVM 58\n",
      "TN:160,FP:11,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.93567245443,Macc:0.824561350467,F1:0.802626106836\n",
      "Training SVM 59\n",
      "TN:161,FP:10,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.941520407271,Macc:0.842105208987,F1:0.824669832828\n",
      "Training SVM 60\n",
      "TN:152,FP:19,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.888888831709,Macc:0.818713397627,F1:0.805025922212\n",
      "Training SVM 61\n",
      "TN:158,FP:13,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.92397654875,Macc:0.833333279727,F1:0.816714754143\n",
      "Training SVM 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:158,FP:13,FN:50,TP:121,Sensitivity:0.707602293663,Specificity0.92397654875,Macc:0.815789421207,F1:0.793437146697\n",
      "Training SVM 63\n",
      "TN:155,FP:16,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.906432690229,Macc:0.809941468366,F1:0.789638519492\n",
      "Training SVM 64\n",
      "TN:157,FP:14,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.91812859591,Macc:0.821637374047,F1:0.802583502388\n",
      "Training SVM 65\n",
      "TN:161,FP:10,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.941520407271,Macc:0.845029185407,F1:0.828473468181\n",
      "Training SVM 66\n",
      "TN:162,FP:9,FN:41,TP:130,Sensitivity:0.760233869225,Specificity0.947368360111,Macc:0.853801114668,F1:0.838704176539\n",
      "Training SVM 67\n",
      "TN:159,FP:12,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.92982450159,Macc:0.815789421207,F1:0.792073741566\n",
      "Training SVM 68\n",
      "TN:158,FP:13,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.92397654875,Macc:0.833333279727,F1:0.816714754143\n",
      "Training SVM 69\n",
      "TN:158,FP:13,FN:46,TP:125,Sensitivity:0.730994105024,Specificity0.92397654875,Macc:0.827485326887,F1:0.809055993836\n",
      "Training SVM 70\n",
      "TN:156,FP:15,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.91228064307,Macc:0.815789421207,F1:0.79611101094\n",
      "Training SVM 71\n",
      "TN:156,FP:15,FN:50,TP:121,Sensitivity:0.707602293663,Specificity0.91228064307,Macc:0.809941468366,F1:0.78826813067\n",
      "Training SVM 72\n",
      "TN:158,FP:13,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.92397654875,Macc:0.821637374047,F1:0.801297445697\n",
      "Training SVM 73\n",
      "TN:156,FP:15,FN:50,TP:121,Sensitivity:0.707602293663,Specificity0.91228064307,Macc:0.809941468366,F1:0.78826813067\n",
      "Training SVM 74\n",
      "TN:156,FP:15,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.91228064307,Macc:0.830409303307,F1:0.815281110852\n",
      "Training SVM 75\n",
      "TN:156,FP:15,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.91228064307,Macc:0.807017491946,F1:0.784308245264\n",
      "Training SVM 76\n",
      "TN:161,FP:10,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.941520407271,Macc:0.845029185407,F1:0.828473468181\n",
      "Training SVM 77\n",
      "TN:162,FP:9,FN:46,TP:125,Sensitivity:0.730994105024,Specificity0.947368360111,Macc:0.839181232567,F1:0.819666653\n",
      "Training SVM 78\n",
      "TN:157,FP:14,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.91812859591,Macc:0.815789421207,F1:0.794782788183\n",
      "Training SVM 79\n",
      "TN:158,FP:13,FN:41,TP:130,Sensitivity:0.760233869225,Specificity0.92397654875,Macc:0.842105208987,F1:0.828019963462\n",
      "Training SVM 80\n",
      "TN:153,FP:18,FN:52,TP:119,Sensitivity:0.695906387983,Specificity0.894736784549,Macc:0.795321586266,F1:0.772721784593\n",
      "Training SVM 81\n",
      "TN:159,FP:12,FN:45,TP:126,Sensitivity:0.736842057864,Specificity0.92982450159,Macc:0.833333279727,F1:0.815528485285\n",
      "Training SVM 82\n",
      "TN:159,FP:12,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.92982450159,Macc:0.821637374047,F1:0.799994523272\n",
      "Training SVM 83\n",
      "TN:163,FP:8,FN:51,TP:120,Sensitivity:0.701754340823,Specificity0.953216312951,Macc:0.827485326887,F1:0.802670140012\n",
      "Training SVM 84\n",
      "TN:162,FP:9,FN:54,TP:117,Sensitivity:0.684210482302,Specificity0.947368360111,Macc:0.815789421207,F1:0.787873355817\n",
      "Training SVM 85\n",
      "TN:157,FP:14,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.91812859591,Macc:0.821637374047,F1:0.802583502388\n",
      "Training SVM 86\n",
      "TN:157,FP:14,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.91812859591,Macc:0.821637374047,F1:0.802583502388\n",
      "Training SVM 87\n",
      "TN:159,FP:12,FN:52,TP:119,Sensitivity:0.695906387983,Specificity0.92982450159,Macc:0.812865444786,F1:0.788074009313\n",
      "Training SVM 88\n",
      "TN:155,FP:16,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.906432690229,Macc:0.809941468366,F1:0.789638519492\n",
      "Training SVM 89\n",
      "TN:158,FP:13,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.92397654875,Macc:0.821637374047,F1:0.801297445697\n",
      "Training SVM 90\n",
      "TN:157,FP:14,FN:49,TP:122,Sensitivity:0.713450246503,Specificity0.91812859591,Macc:0.815789421207,F1:0.794782788183\n",
      "Training SVM 91\n",
      "TN:157,FP:14,FN:48,TP:123,Sensitivity:0.719298199343,Specificity0.91812859591,Macc:0.818713397627,F1:0.79869580871\n",
      "Training SVM 92\n",
      "TN:159,FP:12,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.92982450159,Macc:0.827485326887,F1:0.80781210321\n",
      "Training SVM 93\n",
      "TN:157,FP:14,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.91812859591,Macc:0.821637374047,F1:0.802583502388\n",
      "Training SVM 94\n",
      "TN:159,FP:12,FN:47,TP:124,Sensitivity:0.725146152184,Specificity0.92982450159,Macc:0.827485326887,F1:0.80781210321\n",
      "Training SVM 95\n",
      "TN:155,FP:16,FN:43,TP:128,Sensitivity:0.748537963544,Specificity0.906432690229,Macc:0.827485326887,F1:0.812692896383\n",
      "Training SVM 96\n",
      "TN:156,FP:15,FN:50,TP:121,Sensitivity:0.707602293663,Specificity0.91228064307,Macc:0.809941468366,F1:0.78826813067\n",
      "Training SVM 97\n",
      "TN:158,FP:13,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.92397654875,Macc:0.833333279727,F1:0.816714754143\n",
      "Training SVM 98\n",
      "TN:157,FP:14,FN:44,TP:127,Sensitivity:0.742690010704,Specificity0.91812859591,Macc:0.830409303307,F1:0.814097057585\n",
      "Training SVM 99\n",
      "TN:162,FP:9,FN:52,TP:119,Sensitivity:0.695906387983,Specificity0.947368360111,Macc:0.821637374047,F1:0.795981177294\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-8f87a0f119d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftScores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2956\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2957\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "rand_states = np.random.randint(20000,size=(n_estimators,))\n",
    "\n",
    "results = []\n",
    "softScores = []\n",
    "\n",
    "for idx,rand_state in enumerate(rand_states):\n",
    "    rus = RandomUnderSampler(random_state=rand_state)    \n",
    "    X,y = rus.fit_resample(X_train,y_train)\n",
    "    \n",
    "    clf = NuSVC(kernel='linear',probability=True,verbose=False,\n",
    "              random_state=rand_state, nu = .5)\n",
    "    \n",
    "    print(\"Training SVM %d\" % idx)\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    softScores.append(clf.predict_proba(X_val))\n",
    "    results.append(verboseMetrics(y_val,softScores[-1]))\n",
    "    \n",
    "    del clf\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.std(results))\n",
    "\n",
    "init = np.asarray(np.zeros((342,2)))\n",
    "for each in softScores:\n",
    "    init+=each\n",
    "print(\"Posterior Fusion Score:\")\n",
    "verboseMetrics(y_val,init)\n",
    "\n",
    "sns.distplot(results,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior Fusion Score:\n",
      "TN:158,FP:13,FN:46,TP:125,Sensitivity:0.730994105024,Specificity0.92397654875,Macc:0.827485326887,F1:0.809055993836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8274853268869088"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = np.asarray(np.zeros((342,2)))\n",
    "for each in softScores:\n",
    "    init+=each\n",
    "print(\"Posterior Fusion Score:\")\n",
    "verboseMetrics(y_val,init)\n",
    "\n",
    "# sns.distplot(results,20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM 0\n",
      "TN:163,FP:8,FN:68,TP:103,Sensitivity:0.60233914254,Specificity0.953216312951,Macc:0.777777727745,F1:0.730491145931\n",
      "Training SVM 1\n",
      "TN:166,FP:5,FN:74,TP:97,Sensitivity:0.567251425498,Specificity0.970760171472,Macc:0.769005798485,F1:0.71061750474\n",
      "Training SVM 2\n",
      "TN:167,FP:4,FN:77,TP:94,Sensitivity:0.549707566978,Specificity0.976608124312,Macc:0.763157845645,F1:0.698879606289\n",
      "Training SVM 3\n",
      "TN:164,FP:7,FN:72,TP:99,Sensitivity:0.578947331179,Specificity0.959064265791,Macc:0.769005798485,F1:0.714796190161\n",
      "Training SVM 4\n",
      "TN:164,FP:7,FN:69,TP:102,Sensitivity:0.596491189699,Specificity0.959064265791,Macc:0.777777727745,F1:0.728566141032\n",
      "0.7713449796210832\n",
      "0.005669800638102118\n",
      "Posterior Fusion Score:\n",
      "TN:165,FP:6,FN:70,TP:101,Sensitivity:0.590643236859,Specificity0.964912218631,Macc:0.777777727745,F1:0.726613439067\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEDCAYAAAA4FgP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJqcgEBExOQWBiIjJKQhEREzu/wMal5PTJBVHywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2129ec050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_estimators = 5\n",
    "\n",
    "np.random.seed(1)\n",
    "rand_states = np.random.randint(20000,size=(n_estimators,))\n",
    "\n",
    "results = []\n",
    "softScores = []\n",
    "\n",
    "for idx,rand_state in enumerate(rand_states):\n",
    "    rus = RandomOverSampler(random_state=rand_state)    \n",
    "    X,y = rus.fit_resample(X_train,y_train)\n",
    "    \n",
    "    clf = NuSVC(kernel='linear',probability=True,verbose=False,\n",
    "              random_state=rand_state, nu = .5)\n",
    "    \n",
    "    print(\"Training SVM %d\" % idx)\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    softScores.append(clf.predict_proba(X_val))\n",
    "    results.append(verboseMetrics(y_val,softScores[-1]))\n",
    "    \n",
    "    del clf\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.std(results))\n",
    "\n",
    "init = np.asarray(np.zeros((342,2)))\n",
    "for each in softScores:\n",
    "    init+=each\n",
    "print(\"Posterior Fusion Score:\")\n",
    "verboseMetrics(y_val,init)\n",
    "\n",
    "sns.distplot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "softScores_boaw = softScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "FBANK_feat = os.path.join('FBANK','binnedFeat.mat')\n",
    "FBANK_label = os.path.join('FBANK','binnedLabels.csv')\n",
    "ComParE_feat = os.path.join('baseline','openSMILEall_PCG.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
