{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import arff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.svm import libsvm\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FBANK_feat = os.path.join('FBANK','binnedFeat.mat')\n",
    "FBANK_label = os.path.join('FBANK','binnedLabels.csv')\n",
    "ComParE_feat = os.path.join('baseline','openSMILEall_PCG.mat')\n",
    "BOAW_feat = os.path.join('baseline_boaw','feat','boawFeat.4096.arff')\n",
    "BOAW_meta = os.path.join('baseline_boaw','feat','BOAW_filenames.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def severe2abnormal(labels):\n",
    "    for idx,each in enumerate(labels):\n",
    "        if each == 2:\n",
    "            labels[idx]=1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_boaw/feat/fold1.train.4096.arff\n",
      "True\n",
      "baseline_boaw/feat/fold1.dev.4096.arff\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Load BoaW fixed\n",
    "\n",
    "foldnum = 1\n",
    "codebook = 4096\n",
    "feat_path = os.path.join('baseline_boaw/feat/')\n",
    "meta_path = os.path.join('..','..','feature','mfcc')\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "filenames = []\n",
    "\n",
    "for subset in ['train','dev']:\n",
    "    BOAW_feat = os.path.join(feat_path,\n",
    "                             'fold%d.%s.%d.arff' % (foldnum,subset,codebook))\n",
    "    BOAW_meta = os.path.join(meta_path,'fold%d.%s.filenames.txt' % (foldnum,subset))\n",
    "    print(BOAW_feat)\n",
    "    print(os.path.isfile(BOAW_meta))\n",
    "    \n",
    "    df = arff.load(open(BOAW_feat,'r'))\n",
    "    boawData = pd.DataFrame(np.array(df['data'],dtype=float))\n",
    "    del df\n",
    "\n",
    "    boawFilenames = []\n",
    "    with open(BOAW_meta, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        boawFilenames.append(line.split('\\n')[0])\n",
    "        while line:\n",
    "                line = fp.readline()\n",
    "                boawFilenames.append(line.split('\\n')[0])\n",
    "    boawFilenames = boawFilenames[:-1]\n",
    "    boawData['filenames'] = boawFilenames\n",
    "    boawData.rename({len(boawData.columns)-2:'label'},axis='columns',inplace=True)\n",
    "    boawData.label = [int(each) for each in boawData.label]\n",
    "    boawData['dataset'] = [each[-1][0] for each in boawData.filenames.str.split('_')]\n",
    "    boawData.set_index('filenames',drop=True,inplace=True)\n",
    "    \n",
    "    filenames.append(boawFilenames)\n",
    "    del boawFilenames\n",
    "    \n",
    "    X.append(boawData[range(4096)].values)\n",
    "    y.append(boawData.label.values)\n",
    "    \n",
    "    del boawData\n",
    "    \n",
    "X = np.vstack(X)\n",
    "y = np.hstack(y)\n",
    "filenames = np.hstack(filenames)\n",
    "\n",
    "boawData = pd.DataFrame(X)\n",
    "boawData['filenames'] = filenames\n",
    "boawData['label'] = y\n",
    "boawData.set_index('filenames',drop=True,inplace=True)\n",
    "boawData['dataset'] = [each[-1][0] for each in boawData.index.str.split('_')]\n",
    "boawData.head()\n",
    "\n",
    "del X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3835, 2709)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load FBANK: \n",
    "\n",
    "df = loadmat(FBANK_feat)\n",
    "fbankData = pd.DataFrame(df['feats'])\n",
    "fbankMeta = pd.read_csv(FBANK_label)\n",
    "fbankMeta.file_name = [each[0] for each in fbankMeta.file_name.str.split('.')]\n",
    "fbankData = fbankData.join(fbankMeta)\n",
    "fbankData.dropna(inplace=True)\n",
    "fbankData.rename({'file_name':'filenames'},axis=\"columns\",inplace=True)\n",
    "fbankData.label = [int(each) for each in fbankData.label]\n",
    "fbankData['dataset'] = [each[-1][0] for each in fbankData.filenames.str.split('_')]\n",
    "fbankData.set_index('filenames',drop=True,inplace=True)\n",
    "\n",
    "del df, fbankMeta\n",
    "\n",
    "fbankData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3835, 6375)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load ComParE:\n",
    "\n",
    "df = loadmat(ComParE_feat)\n",
    "\n",
    "compareData = pd.DataFrame(df['dataTrain'])\n",
    "compareData = pd.concat((compareData,pd.DataFrame(df['dataDev'])),axis=\"rows\").reset_index(drop=True)\n",
    "compareData['label'] = np.concatenate((np.hstack(df['TrainLabels']),np.hstack(df['DevLabels'])),axis=0)\n",
    "compareData['filenames'] = np.concatenate((np.hstack(df['TrainFiles']),np.hstack(df['DevFiles'])),axis=0)\n",
    "compareData.filenames = [each[0] for each in compareData.filenames.str.split('.')]\n",
    "compareData.label = [int(each) for each in compareData.label]\n",
    "compareData['dataset'] = [each[-1][0] for each in compareData.filenames.str.split('_')]\n",
    "compareData.set_index('filenames',drop=True,inplace=True)\n",
    "\n",
    "del df\n",
    "\n",
    "compareData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Size 13178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3153, 13178)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fuse dataframes\n",
    "\n",
    "print(\"Target Size %d\" % (len(boawData.columns)-2+len(fbankData.columns)-2+len(compareData.columns)))\n",
    "fused = boawData[range(len(boawData.columns)-2)].join(fbankData[range(len(fbankData.columns)-2)],rsuffix='fbank')\n",
    "fused = fused.join(compareData,lsuffix='fused')\n",
    "fused.dropna()\n",
    "fused.columns = range(len(fused.columns))\n",
    "fused.rename({len(fused.columns)-2:'label',len(fused.columns)-1:'dataset'},axis=\"columns\",inplace=True)\n",
    "\n",
    "# del boawData, fbankData, compareData\n",
    "\n",
    "fused.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 114\n",
    "n_estimators = 10\n",
    "dataset = '0'\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mask = fused.dataset == dataset\n",
    "\n",
    "X = scaler.fit_transform(fused[range(len(fused.columns)-2)][mask]) ## .4489999\n",
    "y = fused.label[mask]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size = 0.3,\n",
    "                                                  random_state = rand_state,\n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev set of Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 1\n",
    "dataset = '0'\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mask1 = fused.dataset == dataset\n",
    "mask2 = fused[mask1].index.str.contains('train')\n",
    "mask3 = fused[mask1].index.str.contains('devel')\n",
    "\n",
    "X = scaler.fit_transform(fused[range(len(fused.columns)-2)][mask1]) ## .4489999\n",
    "y = fused.label[mask1]\n",
    "\n",
    "X_train = X[mask2]\n",
    "y_train = y[mask2]\n",
    "X_val = X[mask3]\n",
    "y_val = y[mask3]\n",
    "\n",
    "del mask1, mask2, mask3, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold1_noFIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 1\n",
    "n_estimators = 5\n",
    "scaler = StandardScaler()\n",
    "\n",
    "fold1_val = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/validation1.txt'\n",
    "df = pd.read_csv(fold1_val,header=None)\n",
    "df[0] = ['train_'+each for each in df[0]]\n",
    "df.columns = ['filenames']\n",
    "df.set_index('filenames',drop=True,inplace=True)\n",
    "mask = fused.dataset == '0'\n",
    "fold1_data = fused[~mask]\n",
    "\n",
    "mask1 = [each in df.index.values for each in fold1_data.index.values]\n",
    "mask2 = [not each for each in mask1]\n",
    "X = fold1_data[range(len(fold1_data.columns)-2)]\n",
    "y = fold1_data.label\n",
    "del fold1_data\n",
    "\n",
    "X_train = scaler.fit_transform(X[mask2])\n",
    "y_train = y[mask2]\n",
    "X_val = scaler.transform(X[mask1])\n",
    "y_val = y[mask1]\n",
    "\n",
    "del mask, mask1, mask2, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold2_noFIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 1\n",
    "n_estimators = 5\n",
    "\n",
    "fold1_val = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/validation2.txt'\n",
    "df = pd.read_csv(fold1_val,header=None)\n",
    "df[0] = ['train_'+each for each in df[0]]\n",
    "df.columns = ['filenames']\n",
    "df.set_index('filenames',drop=True,inplace=True)\n",
    "mask = fused.dataset == '0'\n",
    "fold1_data = fused[~mask]\n",
    "\n",
    "mask1 = [each in df.index.values for each in fold1_data.index.values]\n",
    "mask2 = [not each for each in mask1]\n",
    "X = scaler.fit_transform(fold1_data[range(len(fold1_data.columns)-2)])\n",
    "y = fold1_data.label\n",
    "del fold1_data\n",
    "\n",
    "X_train = X[mask2]\n",
    "y_train = y[mask2]\n",
    "X_val = X[mask1]\n",
    "y_val = y[mask1]\n",
    "\n",
    "del mask, mask1, mask2, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold3_noFIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 1\n",
    "n_estimators = 5\n",
    "\n",
    "fold1_val = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/validation3.txt'\n",
    "df = pd.read_csv(fold1_val,header=None)\n",
    "df[0] = ['train_'+each for each in df[0]]\n",
    "df.columns = ['filenames']\n",
    "df.set_index('filenames',drop=True,inplace=True)\n",
    "mask = fused.dataset == '0'\n",
    "fold1_data = fused[~mask]\n",
    "\n",
    "mask1 = [each in df.index.values for each in fold1_data.index.values]\n",
    "mask2 = [not each for each in mask1]\n",
    "X = scaler.fit_transform(fold1_data[range(len(fold1_data.columns)-2)])\n",
    "y = fold1_data.label\n",
    "del fold1_data\n",
    "\n",
    "X_train = X[mask2]\n",
    "y_train = y[mask2]\n",
    "X_val = X[mask1]\n",
    "y_val = y[mask1]\n",
    "\n",
    "del mask, mask1, mask2, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold0_noFIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 1\n",
    "n_estimators = 5\n",
    "\n",
    "fold1_val = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/validation0.txt'\n",
    "df = pd.read_csv(fold1_val,header=None)\n",
    "df[0] = ['train_'+each for each in df[0]]\n",
    "df.columns = ['filenames']\n",
    "df.set_index('filenames',drop=True,inplace=True)\n",
    "mask = fused.dataset == '0'\n",
    "fold1_data = fused[~mask]\n",
    "\n",
    "mask1 = [each in df.index.values for each in fold1_data.index.values]\n",
    "mask2 = [not each for each in mask1]\n",
    "X = scaler.fit_transform(fold1_data[range(len(fold1_data.columns)-2)])\n",
    "y = fold1_data.label\n",
    "del fold1_data\n",
    "\n",
    "X_train = X[mask2]\n",
    "y_train = y[mask2]\n",
    "X_val = X[mask1]\n",
    "y_val = y[mask1]\n",
    "\n",
    "del mask, mask1, mask2, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verboseMetrics(y_val,softScores):\n",
    "    eps = 1.1e-5\n",
    "    if Counter(y_val).keys()[-1] == 1:\n",
    "        TN, FP, FN, TP = confusion_matrix(y_val, np.argmax(softScores,axis=-1), labels=[0,1]).ravel()\n",
    "        sensitivity = TP / (TP + FN + eps)\n",
    "        specificity = TN / (TN + FP + eps)\n",
    "        precision = TP / (TP + FP + eps)\n",
    "        F1 = 2 * (precision * sensitivity) / (precision + sensitivity + eps)\n",
    "        Macc = (sensitivity + specificity) / 2\n",
    "        print(\"TN:{},FP:{},FN:{},TP:{},Sensitivity:{},Specificity{},Macc:{},F1:{}\".format(TN, FP, FN, TP,sensitivity,specificity,Macc,F1))\n",
    "        return Macc\n",
    "    elif Counter(y_val).keys()[-1] == 2:\n",
    "        logs = dict()\n",
    "        confmat = confusion_matrix(y_pred=np.argmax(softScores,axis=-1), y_true=y_val)\n",
    "        logs['recall0'] = confmat[0, 0] / np.sum(confmat[0, :])\n",
    "        logs['recall1'] = confmat[1, 1] / np.sum(confmat[1, :])\n",
    "        logs['recall2'] = confmat[2, 2] / np.sum(confmat[2, :])\n",
    "        logs['UAR'] = np.mean([logs['recall0'], logs['recall1'], logs['recall2']])\n",
    "        print(logs.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
