{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function,division,absolute_import\n",
    "import tables\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from custom_layers import Conv1D_linearphase, DCT1D\n",
    "from heartnet_v1 import reshape_folds\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = 'fold1+compare 2018-05-05 17:04:36.995687'\n",
    "checkpoint_name = \"/media/taufiq/Data1/heart_sound/models/fold1+compare 2018-05-05 17:04:36.995687/weights.0007-0.8148.hdf5\"\n",
    "min_epoch = 3\n",
    "min_metric = .7\n",
    "foldname = 'fold1+compare'\n",
    "fold_dir = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/'\n",
    "model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "log_dir = '/media/taufiq/Data1/heart_sound/logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = checkpoint_name[:checkpoint_name.find('fold')]\n",
    "log_name = checkpoint_name[checkpoint_name.find('fold'):checkpoint_name.find('weights')-1]\n",
    "print(log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(model_dir+log_name):\n",
    "    print(\"Model directory found\")\n",
    "    if os.path.isfile(os.path.join(model_dir+log_name,\"model.json\")):\n",
    "        print(\"model.json found. Importing\")\n",
    "    else:\n",
    "        raise ImportError(\"model.json not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir+log_name,\"model.json\")) as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model = model_from_json(loaded_model_json,{'Conv1D_linearphase':Conv1D_linearphase,'DCT1D':DCT1D})\n",
    "model.load_weights(checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283\n",
      "515\n",
      "(93942, 2500, 1)\n",
      "(93942, 1)\n",
      "(15511, 2500, 1)\n",
      "(15511, 1)\n"
     ]
    }
   ],
   "source": [
    "############## Importing data ############\n",
    "feat = tables.open_file(fold_dir + foldname + '.mat')\n",
    "x_train = feat.root.trainX[:]\n",
    "y_train = feat.root.trainY[0, :]\n",
    "x_val = feat.root.valX[:]\n",
    "y_val = feat.root.valY[0, :]\n",
    "train_parts = feat.root.train_parts[:]\n",
    "val_parts = feat.root.val_parts[0, :]\n",
    "\n",
    "############## Relabeling ################\n",
    "\n",
    "for i in range(0, y_train.shape[0]):\n",
    "    if y_train[i] == -1:\n",
    "        y_train[i] = 0  ## Label 0 for normal 1 for abnormal\n",
    "for i in range(0, y_val.shape[0]):\n",
    "    if y_val[i] == -1:\n",
    "        y_val[i] = 0\n",
    "\n",
    "############# Parse Database names ########\n",
    "\n",
    "train_files = []\n",
    "for each in feat.root.train_files[:][0]:\n",
    "    train_files.append(chr(each))\n",
    "print(len(train_files))\n",
    "val_files = []\n",
    "for each in feat.root.val_files[:][0]:\n",
    "    val_files.append(chr(each))\n",
    "print(len(val_files))\n",
    "\n",
    "################### Reshaping ############\n",
    "\n",
    "x_train, y_train, x_val, y_val = reshape_folds(x_train, x_val, y_train, y_val)\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_val = to_categorical(y_val, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model from log name\n",
    "\n",
    "\n",
    "Given a log_name it scans through the directories and finds the best three weights considering Sensitivity, Specificity and Macc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(model_dir+log_name):\n",
    "    print(\"Model directory found\")\n",
    "    if os.path.isfile(os.path.join(model_dir+log_name,\"model.json\")):\n",
    "        print(\"model.json found. Importing\")\n",
    "    else:\n",
    "        raise ImportError(\"model.json not found\")\n",
    "\n",
    "with open(os.path.join(model_dir+log_name,\"model.json\")) as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model = model_from_json(loaded_model_json,{'Conv1D_linearphase':Conv1D_linearphase,'DCT1D':DCT1D})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv = os.path.join(log_dir+log_name,\"training.csv\")\n",
    "df = pd.read_csv(training_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_idx = df['val_sensitivity'][df.epoch>min_epoch][df.val_specificity>min_metric].idxmax()\n",
    "spec_idx = df['val_specificity'][df.epoch>min_epoch][df.val_sensitivity>min_metric].idxmax()\n",
    "macc_idx = df['val_macc'][df.epoch>min_epoch].idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dict()\n",
    "weights['sens_weight'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[sens_idx],df.val_acc.iloc[sens_idx])\n",
    "weights['spec_weight'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[spec_idx],df.val_acc.iloc[spec_idx])\n",
    "weights['macc_weight'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[macc_idx],df.val_acc.iloc[macc_idx])\n",
    "print(\"Best Sensitivity model: \\t{}\".format(weights['sens_weight']))\n",
    "print(\"Best Specificity model: \\t{}\".format(weights['spec_weight']))\n",
    "print(\"Best Macc model: \\t\\t{}\".format(weights['macc_weight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Validation Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283\n",
      "515\n"
     ]
    }
   ],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "fold1_filenames = eng.load(os.path.join(fold_dir,'fold1_filenames.mat'))\n",
    "compare_filenames = eng.load(os.path.join(fold_dir,'compare_filenames.mat'))\n",
    "eng.quit()\n",
    "\n",
    "train_filenames = fold1_filenames['train_files']\n",
    "train_filenames.extend(compare_filenames['train_files'])\n",
    "val_filenames = fold1_filenames['val_files']\n",
    "val_filenames.extend(compare_filenames['val_files'])\n",
    "print(\"Filenames Loaded : Train files {} and Validation Files {}\".format(len(train_filenames),len(val_filenames)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
