{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function,division,absolute_import\n",
    "import tables\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from custom_layers import Conv1D_linearphase, DCT1D\n",
    "from heartnet_v1 import reshape_folds\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = 'fold1+compare 2018-04-17 22:03:55.445492'\n",
    "checkpoint_name = \"/media/taufiq/Data1/heart_sound/models/fold1+compare 2018-05-05 17:04:36.995687/weights.0007-0.8148.hdf5\"\n",
    "min_epoch = 100\n",
    "min_metric = .7\n",
    "confidence_thresh = 0.2\n",
    "\n",
    "foldname = 'fold1+compare'\n",
    "fold_dir = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/'\n",
    "model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "log_dir = '/media/taufiq/Data1/heart_sound/logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = checkpoint_name[:checkpoint_name.find('fold')]\n",
    "log_name = checkpoint_name[checkpoint_name.find('fold'):checkpoint_name.find('weights')-1]\n",
    "print(log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(model_dir+log_name):\n",
    "    print(\"Model directory found\")\n",
    "    if os.path.isfile(os.path.join(model_dir+log_name,\"model.json\")):\n",
    "        print(\"model.json found. Importing\")\n",
    "    else:\n",
    "        raise ImportError(\"model.json not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir+log_name,\"model.json\")) as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model = model_from_json(loaded_model_json,{'Conv1D_linearphase':Conv1D_linearphase,'DCT1D':DCT1D})\n",
    "model.load_weights(checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283\n",
      "515\n",
      "(93942, 2500, 1)\n",
      "(93942, 1)\n",
      "(15511, 2500, 1)\n",
      "(15511, 1)\n"
     ]
    }
   ],
   "source": [
    "############## Importing data ############\n",
    "feat = tables.open_file(fold_dir + foldname + '.mat')\n",
    "x_train = feat.root.trainX[:]\n",
    "y_train = feat.root.trainY[0, :]\n",
    "x_val = feat.root.valX[:]\n",
    "y_val = feat.root.valY[0, :]\n",
    "train_parts = feat.root.train_parts[0, :]\n",
    "val_parts = feat.root.val_parts[0, :]\n",
    "\n",
    "############## Relabeling ################\n",
    "\n",
    "for i in range(0, y_train.shape[0]):\n",
    "    if y_train[i] == -1:\n",
    "        y_train[i] = 0  ## Label 0 for normal 1 for abnormal\n",
    "for i in range(0, y_val.shape[0]):\n",
    "    if y_val[i] == -1:\n",
    "        y_val[i] = 0\n",
    "\n",
    "############# Parse Database names ########\n",
    "\n",
    "train_files = []\n",
    "for each in feat.root.train_files[:][0]:\n",
    "    train_files.append(chr(each))\n",
    "print(len(train_files))\n",
    "val_files = []\n",
    "for each in feat.root.val_files[:][0]:\n",
    "    val_files.append(chr(each))\n",
    "print(len(val_files))\n",
    "\n",
    "################### Reshaping ############\n",
    "\n",
    "x_train, y_train, x_val, y_val = reshape_folds(x_train, x_val, y_train, y_val)\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_val = to_categorical(y_val, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Validation Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filenames Loaded : Train files 3283 and Validation Files 515\n"
     ]
    }
   ],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "fold1_filenames = eng.load(os.path.join(fold_dir,'fold1_filenames.mat'))\n",
    "compare_filenames = eng.load(os.path.join(fold_dir,'compare_filenames.mat'))\n",
    "eng.quit()\n",
    "\n",
    "train_filenames = fold1_filenames['train_files']\n",
    "train_filenames.extend(compare_filenames['train_files'])\n",
    "val_filenames = fold1_filenames['val_files']\n",
    "val_filenames.extend(compare_filenames['val_files'])\n",
    "print(\"Filenames Loaded : Train files {} and Validation Files {}\".format(len(train_filenames),len(val_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a0156.wav</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a0148.wav</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a0099.wav</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b0265.wav</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0319.wav</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filenames dataset\n",
       "0  a0156.wav       a\n",
       "1  a0148.wav       a\n",
       "2  a0099.wav       a\n",
       "3  b0265.wav       b\n",
       "4  b0319.wav       b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = pd.DataFrame(train_filenames,columns={\"filenames\"})\n",
    "dfVal = pd.DataFrame(val_filenames,columns={\"filenames\"})\n",
    "dfTrain['dataset'] = train_files\n",
    "dfVal['dataset'] = val_files\n",
    "dfVal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get true labels per recording and append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = []\n",
    "start_idx = 0\n",
    "y_val_cc = np.transpose(np.argmax(y_val, axis=-1))\n",
    "for s in val_parts:\n",
    "    if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "        continue\n",
    "    # ~ print \"part {} start {} stop {}\".format(s,star_idx,start_idx+int(s)-1)\n",
    "    temp_ = y_val_cc[start_idx:start_idx + int(s) - 1]\n",
    "    if (sum(temp_ == 0) > sum(temp_ == 1)):\n",
    "        true.append(0)\n",
    "    else:\n",
    "        true.append(1)\n",
    "    start_idx = start_idx + int(s)\n",
    "dfVal['true'] = true\n",
    "\n",
    "true = []\n",
    "start_idx = 0\n",
    "y_train_cc = np.transpose(np.argmax(y_train, axis=-1))\n",
    "for s in train_parts:\n",
    "    if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "        continue\n",
    "    # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "    temp_ = y_train_cc[start_idx:start_idx + int(s) - 1]\n",
    "    if (sum(temp_ == 0) > sum(temp_ == 1)):\n",
    "        true.append(0)\n",
    "    else:\n",
    "        true.append(1)\n",
    "    start_idx = start_idx + int(s)\n",
    "dfTrain['true'] = true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append signal quality labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = []\n",
    "start_idx = 0\n",
    "y_val_cc = feat.root.valY[1,:]\n",
    "for s in val_parts:\n",
    "    if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "        continue\n",
    "    # ~ print \"part {} start {} stop {}\".format(s,star_idx,start_idx+int(s)-1)\n",
    "    temp_ = y_val_cc[start_idx:start_idx + int(s) - 1]\n",
    "    if (sum(temp_ == 0) > sum(temp_ == 1)):\n",
    "        true.append(0)\n",
    "    else:\n",
    "        true.append(1)\n",
    "    start_idx = start_idx + int(s)\n",
    "dfVal['quality'] = true\n",
    "\n",
    "true = []\n",
    "start_idx = 0\n",
    "y_train_cc = feat.root.trainY[1,:]\n",
    "for s in train_parts:\n",
    "    if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "        continue\n",
    "    # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "    temp_ = y_train_cc[start_idx:start_idx + int(s) - 1]\n",
    "    if (sum(temp_ == 0) > sum(temp_ == 1)):\n",
    "        true.append(0)\n",
    "    else:\n",
    "        true.append(1)\n",
    "    start_idx = start_idx + int(s)\n",
    "dfTrain['quality'] = true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model from log name\n",
    "\n",
    "\n",
    "Given a log_name it scans through the directories and finds the best three weights considering Sensitivity, Specificity and Macc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory found\n",
      "model.json found. Importing\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_1 (Conv1D_li (None, 2500, 1)      31          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_2 (Conv1D_li (None, 2500, 1)      31          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_3 (Conv1D_li (None, 2500, 1)      31          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_4 (Conv1D_li (None, 2500, 1)      31          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2496, 8)      40          conv1d_linearphase_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2496, 8)      40          conv1d_linearphase_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 2496, 8)      40          conv1d_linearphase_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 2496, 8)      40          conv1d_linearphase_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 2496, 8)      32          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 2496, 8)      32          conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 2496, 8)      32          conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 2496, 8)      32          conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 2496, 8)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 2496, 8)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 2496, 8)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 2496, 8)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2496, 8)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2496, 8)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2496, 8)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 2496, 8)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1248, 8)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1248, 8)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1248, 8)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1248, 8)      0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1244, 4)      160         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1244, 4)      160         max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1244, 4)      160         max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 1244, 4)      160         max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1244, 4)      16          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1244, 4)      16          conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1244, 4)      16          conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1244, 4)      16          conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1244, 4)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1244, 4)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1244, 4)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1244, 4)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1244, 4)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1244, 4)      0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1244, 4)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1244, 4)      0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 622, 4)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 622, 4)       0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 622, 4)       0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 622, 4)       0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 622, 16)      0           max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9952)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Dense (Dense)                   (None, 20)           199040      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20)           0           Dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 2)            42          dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 200,198\n",
      "Trainable params: 199,178\n",
      "Non-trainable params: 1,020\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(model_dir+log_name):\n",
    "    print(\"Model directory found\")\n",
    "    if os.path.isfile(os.path.join(model_dir+log_name,\"model.json\")):\n",
    "        print(\"model.json found. Importing\")\n",
    "    else:\n",
    "        raise ImportError(\"model.json not found\")\n",
    "\n",
    "with open(os.path.join(model_dir+log_name,\"model.json\")) as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model = model_from_json(loaded_model_json,{'Conv1D_linearphase':Conv1D_linearphase,'DCT1D':DCT1D})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv = os.path.join(log_dir+log_name,\"training.csv\")\n",
    "df = pd.read_csv(training_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_idx = df['val_sensitivity'][df.epoch>min_epoch][df.val_specificity>min_metric].idxmax()\n",
    "spec_idx = df['val_specificity'][df.epoch>min_epoch][df.val_sensitivity>min_metric].idxmax()\n",
    "macc_idx = df['val_macc'][df.epoch>min_epoch].idxmax()\n",
    "val_idx = df['val_acc'][df.epoch>min_epoch].idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Sensitivity model: 0.926282018625 \t\tweights.0113-0.8270.hdf5\n",
      "Best Specificity model: 0.81280783773 \t\tweights.0127-0.7633.hdf5\n",
      "Best Macc model: 0.825920451026 \t\tweights.0142-0.8053.hdf5\n",
      "Best Val model: 0.83624524541 \t\t\tweights.0161-0.8362.hdf5\n"
     ]
    }
   ],
   "source": [
    "weights = dict()\n",
    "weights['val_sensitivity'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[sens_idx]+1,df.val_acc.iloc[sens_idx])\n",
    "weights['val_specificity'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[spec_idx]+1,df.val_acc.iloc[spec_idx])\n",
    "weights['val_macc'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[macc_idx]+1,df.val_acc.iloc[macc_idx])\n",
    "weights['val_acc'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[val_idx]+1,df.val_acc.iloc[val_idx])\n",
    "print(\"Best Sensitivity model: {} \\t\\t{}\".format(df.val_sensitivity.iloc[sens_idx],weights['val_sensitivity']))\n",
    "print(\"Best Specificity model: {} \\t\\t{}\".format(df.val_specificity.iloc[spec_idx],weights['val_specificity']))\n",
    "print(\"Best Macc model: {} \\t\\t{}\".format(df.val_macc.iloc[macc_idx],weights['val_macc']))\n",
    "print(\"Best Val model: {} \\t\\t\\t{}\".format(df.val_acc.iloc[val_idx],weights['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = 'val_'\n",
    "checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "model.load_weights(checkpoint_name)\n",
    "print(\"Checkpoint loaded:\\n %s\" % checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Validation Predictions, get per recording Confidence and append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val, verbose=1)\n",
    "y_pred_hard = np.argmax(y_pred,axis=-1)\n",
    "predicted_confidence = np.asarray([y_pred[i,j] for i,j in zip(range(len(y_pred)),np.nditer(y_val_cc))])\n",
    "start_idx=0\n",
    "pred = []\n",
    "for s in val_parts:\n",
    "    if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "        continue\n",
    "    # ~ print \"part {} start {} stop {}\".format(s,star_idx,start_idx+int(s)-1)\n",
    "    temp = sum(predicted_confidence[start_idx:start_idx + int(s) - 1])/s\n",
    "    pred.append(temp)\n",
    "    start_idx = start_idx + int(s)\n",
    "dfVal['conf_'+metric]=pred\n",
    "# plt.figure(figsize=(10,10))\n",
    "# dfVal['conf_'+metric].hist(bins=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Predictions, get per recording Confidence and append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train, verbose=1)\n",
    "y_pred_hard = np.argmax(y_pred,axis=-1)\n",
    "predicted_confidence = np.asarray([y_pred[i,j] for i,j in zip(range(len(y_pred)),np.nditer(y_train_cc))])\n",
    "start_idx=0\n",
    "pred = []\n",
    "for s in train_parts:\n",
    "    if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "        continue\n",
    "    # ~ print \"part {} start {} stop {}\".format(s,star_idx,start_idx+int(s)-1)\n",
    "    temp = sum(predicted_confidence[start_idx:start_idx + int(s) - 1])/s\n",
    "    pred.append(temp)\n",
    "    start_idx = start_idx + int(s)\n",
    "dfTrain['conf_'+metric]=pred\n",
    "# plt.figure(figsize=(10,10))\n",
    "# dfTrain['conf_'+metric].hist(bins=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# sns.distplot(dfTrain['conf_'+metric])\n",
    "# plt.xlim(0,1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mask = dfVal['conf_'+metric]<=confidence_thresh\n",
    "sns.barplot(x=dfVal['conf_'+metric][mask],y=dfVal.filenames[mask])\n",
    "plt.show()\n",
    "for names in zip(dfVal.filenames[mask],dfVal.true[mask]):\n",
    "    print(\"%s %d\" % names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mask = dfTrain['conf_'+metric]<=confidence_thresh\n",
    "sns.barplot(x=dfTrain['conf_'+metric][mask],y=dfTrain.filenames[mask])\n",
    "plt.show()\n",
    "for names in zip(dfTrain.filenames[mask],dfTrain.true[mask]):\n",
    "    print(\"%s %d\" % names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out the recordings that have huge fluctuations in confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93942/93942 [==============================] - 10s 108us/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d6bd90b273c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_pred_hard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpredicted_confidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_cc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstart_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "for epoch in range(min_epoch,df.epoch.count()):\n",
    "    checkpoint_name = os.path.join(model_dir+log_name,\n",
    "                                   \"weights.%.4d-%.4f.hdf5\" % (epoch+1,df.val_acc.iloc[epoch]))\n",
    "    model.load_weights(checkpoint_name)\n",
    "    y_pred = model.predict(x_train, verbose=1)\n",
    "    y_pred_hard = np.argmax(y_pred,axis=-1)\n",
    "    predicted_confidence = np.asarray([y_pred[i,j] for i,j in zip(range(len(y_pred)),np.nditer(y_train_cc))])\n",
    "    start_idx=0\n",
    "    pred = []\n",
    "    for s in train_parts:\n",
    "        if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "            continue\n",
    "        # ~ print \"part {} start {} stop {}\".format(s,star_idx,start_idx+int(s)-1)\n",
    "        temp = sum(predicted_confidence[start_idx:start_idx + int(s) - 1])/s\n",
    "        pred.append(temp)\n",
    "        start_idx = start_idx + int(s)\n",
    "    dfTrain[\"weights.%.4d-%.4f.hdf5\" % (epoch+1,df.val_acc.iloc[epoch])]=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_sweep = dfTrain.columns[-(df.epoch.count()-min_epoch):] # list of columns for sweep\n",
    "# plt.figure(figsize=(20,10))\n",
    "# sns.boxplot(x=\"filenames\",data=dfTrain[list_sweep.append(\"filenames\")])\n",
    "# plt.show()\n",
    "dfTrain['conf_mean'] = dfTrain[list(dfTrain.columns.values)[-100:]].mean(axis=1).values\n",
    "dfTrain['conf_var'] = dfTrain[list(dfTrain.columns.values)[-100:]].var(axis=1).values\n",
    "plt.figure(figsize=(10,7))\n",
    "dfTrain.conf_var.hist(bins=100)\n",
    "plt.xlabel(\"Variance of Confidence with Epochs\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize confidence distributions accross recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dfTrain[dfTrain.conf_var > .004][dfTrain.conf_val_macc < .5]\n",
    "test.set_index(\"filenames\",inplace=True)\n",
    "test = test.transpose()\n",
    "test = test.iloc[6:-2]\n",
    "test.set_index(np.r_[0:test.count()[0]],inplace=True)\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.violinplot(data=test,jitter=True)\n",
    "plt.ylabel(\"Confidence for true class\")\n",
    "plt.title(\"Recordings which are confusing during training\")\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "for names in test.columns.values:\n",
    "    print(\"%.1s\" % names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(.2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfTrain['conf_mean'] = dfTrain[list(dfTrain.columns.values)[-100:]].mean(axis=1).values\n",
    "# dfTrain['conf_var'] = dfTrain[list(dfTrain.columns.values)[-100:]].var(axis=1).values\n",
    "plt.figure(figsize=(10,7))\n",
    "dfTrain.conf_var.hist(bins=100)\n",
    "plt.xlabel(\"Variance of Confidence with Epochs\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_= [\"b0114.wav\",\n",
    "\"b0226.wav\",\n",
    "\"b0300.wav\",\n",
    "\"d0001.wav\",\n",
    "\"d0004.wav\",\n",
    "\"d0015.wav\",\n",
    "\"d0030.wav\",\n",
    "\"d0037.wav\",\n",
    "\"d0054.wav\",\n",
    "\"train_0095.wav\",\n",
    "\"train_0106.wav\",\n",
    "\"train_0172.wav\",\n",
    "\"train_0282.wav\",\n",
    "\"train_0291.wav\",\n",
    "\"train_0311.wav\",\n",
    "\"train_0363.wav\",\n",
    "\"train_0431.wav\",\n",
    "\"train_0458.wav\",\n",
    "\"train_0468.wav\",\n",
    "\"train_0475.wav\",\n",
    "\"train_0484.wav\",\n",
    "\"train_0493.wav\",\n",
    "\"a0118.wav\",\n",
    "\"e00004.wav\",\n",
    "\"e00017.wav\",\n",
    "\"e00315.wav\",\n",
    "\"e00578.wav\",\n",
    "\"e00690.wav\",\n",
    "\"e00720.wav\",\n",
    "\"e00753.wav\",\n",
    "\"e00873.wav\",\n",
    "\"e00948.wav\",\n",
    "\"e00954.wav\",\n",
    "\"e01540.wav\",\n",
    "\"e01887.wav\",\n",
    "\"e02093.wav\",\n",
    "\"f0021.wav\",\n",
    "\"f0034.wav\",\n",
    "\"f0058.wav\",\n",
    "\"train_0109.wav\"]\n",
    "\n",
    "for name in list_:\n",
    "    print(dfTrain['true'].loc[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.to_csv('/media/taufiq/Data1/heart_sound/dfTrain.csv',index=None)\n",
    "dfVal.to_csv('/media/taufiq/Data1/heart_sound/dfVal.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
