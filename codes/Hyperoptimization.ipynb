{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# set_session(tf.Session(config=config))\n",
    "# from clr_callback import CyclicLR\n",
    "# import dill\n",
    "from AudioDataGenerator import AudioDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "import pandas as pd\n",
    "import tables\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Dense, Dropout, Flatten, Activation, AveragePooling1D\n",
    "from keras import initializers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import max_norm\n",
    "from keras.optimizers import Adam  # Nadam, Adamax\n",
    "from keras.callbacks import TensorBoard, Callback, ReduceLROnPlateau\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from custom_layers import Conv1D_zerophase_linear, Conv1D_linearphase, Conv1D_zerophase,\\\n",
    "    DCT1D, Conv1D_gammatone, Conv1D_linearphaseType\n",
    "from heartnet_v1 import log_macc, write_meta, compute_weight, reshape_folds, results_log\n",
    "from utils import DenseNet\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.losses import categorical_crossentropy\n",
    "sns.set()\n",
    "import ast # for list import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branch(input_tensor,num_filt,kernel_size,random_seed,padding,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable):\n",
    "\n",
    "    num_filt1, num_filt2 = num_filt\n",
    "    t = Conv1D(num_filt1, kernel_size=kernel_size,\n",
    "                kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "                padding=padding,\n",
    "                use_bias=bias,\n",
    "                kernel_constraint=max_norm(maxnorm),\n",
    "                trainable=trainable,\n",
    "                kernel_regularizer=l2(l2_reg))(input_tensor)\n",
    "    t = BatchNormalization(epsilon=eps, momentum=bn_momentum, axis=-1)(t)\n",
    "    t = Activation(activation_function)(t)\n",
    "    t = Dropout(rate=dropout_rate, seed=random_seed)(t)\n",
    "    t = MaxPooling1D(pool_size=subsam)(t)\n",
    "    t = Conv1D(num_filt2, kernel_size=kernel_size,\n",
    "               kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "               padding=padding,\n",
    "               use_bias=bias,\n",
    "               trainable=trainable,\n",
    "               kernel_constraint=max_norm(maxnorm),\n",
    "               kernel_regularizer=l2(l2_reg))(t)\n",
    "    t = BatchNormalization(epsilon=eps, momentum=bn_momentum, axis=-1)(t)\n",
    "    t = Activation(activation_function)(t)\n",
    "    t = Dropout(rate=dropout_rate, seed=random_seed)(t)\n",
    "    t = MaxPooling1D(pool_size=subsam)(t)\n",
    "    # t = Flatten()(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldname='fold1_noFIR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values initialized\n"
     ]
    }
   ],
   "source": [
    "random_seed=1\n",
    "batch_size=64\n",
    "fold_dir = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/folds_dec_2018/'\n",
    "log_dir= '/media/taufiq/Data/heart_sound/Heart_Sound/codes/logs/'\n",
    "bn_momentum = 0.99\n",
    "eps= 1.1e-5\n",
    "bias=False\n",
    "l2_reg=0.\n",
    "# l2_reg_dense=0.\n",
    "kernel_size=5\n",
    "maxnorm=10000.\n",
    "dropout_rate=0.5\n",
    "dropout_rate_dense=0.\n",
    "padding='valid'\n",
    "activation_function='relu'\n",
    "subsam=2\n",
    "load_path=False\n",
    "lr=0.0012843784 \n",
    "lr_decay=0.0001132885\n",
    "FIR_train=True\n",
    "num_filt=(8,4)\n",
    "num_dense=20\n",
    "params = dict()\n",
    "print(\"values initialized\")\n",
    "\n",
    "model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "fold_dir = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/folds_dec_2018/'\n",
    "log_name = foldname + ' ' + str(datetime.now())\n",
    "log_dir = '/media/taufiq/Data1/heart_sound/logs/'\n",
    "if not os.path.exists(model_dir + log_name):\n",
    "    os.makedirs(model_dir + log_name)\n",
    "checkpoint_name = model_dir + log_name + \"/\" + 'weights.{epoch:04d}-{val_acc:.4f}.hdf5'\n",
    "results_path = '/media/taufiq/Data1/heart_sound/results_2class.csv'\n",
    "\n",
    "num_filt = (8, 4)\n",
    "num_dense = 20\n",
    "\n",
    "bn_momentum = 0.99\n",
    "eps = 1.1e-5\n",
    "bias = False\n",
    "l2_reg = 0.04864911065093751\n",
    "l2_reg_dense = 0.\n",
    "kernel_size = 5\n",
    "maxnorm = 10000.\n",
    "dropout_rate = 0.5\n",
    "dropout_rate_dense = 0.\n",
    "padding = 'valid'\n",
    "activation_function = 'relu'\n",
    "subsam = 2\n",
    "FIR_train= True\n",
    "trainable = True\n",
    "decision = 'majority'  # Decision algorithm for inference over total recording ('majority','confidence')\n",
    "\n",
    "# lr =  0.00125 ## After bayesian optimization\n",
    "\n",
    "###### lr_decay optimization ######\n",
    "lr_decay =0.0001132885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76881\n",
      "9639\n",
      "(76881, 2500, 1)\n",
      "(76881, 1)\n",
      "(9639, 2500, 1)\n",
      "(9639, 1)\n"
     ]
    }
   ],
   "source": [
    "feat = tables.open_file(fold_dir + foldname + '.mat')\n",
    "x_train = feat.root.trainX[:]\n",
    "y_train = feat.root.trainY[0, :]\n",
    "x_val = feat.root.valX[:]\n",
    "y_val = feat.root.valY[0, :]\n",
    "train_parts = feat.root.train_parts[:]\n",
    "val_parts = feat.root.val_parts[0, :]\n",
    "\n",
    "############## Relabeling ################\n",
    "\n",
    "for i in range(0, y_train.shape[0]):\n",
    "    if y_train[i] == -1:\n",
    "        y_train[i] = 0  ## Label 0 for normal 1 for abnormal\n",
    "for i in range(0, y_val.shape[0]):\n",
    "    if y_val[i] == -1:\n",
    "        y_val[i] = 0\n",
    "\n",
    "############# Parse Database names ########\n",
    "\n",
    "train_files = []\n",
    "for each in feat.root.train_files[:][0]:\n",
    "    train_files.append(chr(each))\n",
    "print(len(train_files))\n",
    "val_files = []\n",
    "for each in feat.root.val_files[:][0]:\n",
    "    val_files.append(chr(each))\n",
    "print(len(val_files))\n",
    "\n",
    "################### Reshaping ############\n",
    "\n",
    "x_train, y_train, x_val, y_val = reshape_folds(x_train, x_val, y_train, y_val)\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_val = to_categorical(y_val, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = AudioDataGenerator(shift=.1,\n",
    "                             # roll_range=.1,\n",
    "                             # fill_mode='reflect',\n",
    "                             # featurewise_center=True,\n",
    "                             # zoom_range=.1,\n",
    "                             # zca_whitening=True,\n",
    "                             # samplewise_center=True,\n",
    "                             # samplewise_std_normalization=True,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heartnet(load_path,activation_function='relu', bn_momentum=0.99,\n",
    "             bias=False, dropout_rate=0.5, dropout_rate_dense=0.0,\n",
    "             eps=1.1e-5, kernel_size=5, l2_reg=0.0, l2_reg_dense=0.0,\n",
    "             lr=0.0012843784, lr_decay=0.0001132885, maxnorm=10000.,\n",
    "             padding='valid', random_seed=1, subsam=2, num_filt=(8, 4),\n",
    "             num_dense=20,FIR_train=False,trainable=True,type_=1,args=1):\n",
    "\n",
    "    input = Input(shape=(2500, 1))\n",
    "\n",
    "    coeff_path = '/media/taufiq/Data1/heart_sound/heartnetTransfer/filterbankcoeff60.mat'\n",
    "    coeff = tables.open_file(coeff_path)\n",
    "    b1 = coeff.root.b1[:]\n",
    "    b1 = np.hstack(b1)\n",
    "    b1 = np.reshape(b1, [b1.shape[0], 1, 1])\n",
    "\n",
    "    b2 = coeff.root.b2[:]\n",
    "    b2 = np.hstack(b2)\n",
    "    b2 = np.reshape(b2, [b2.shape[0], 1, 1])\n",
    "\n",
    "    b3 = coeff.root.b3[:]\n",
    "    b3 = np.hstack(b3)\n",
    "    b3 = np.reshape(b3, [b3.shape[0], 1, 1])\n",
    "\n",
    "    b4 = coeff.root.b4[:]\n",
    "    b4 = np.hstack(b4)\n",
    "    b4 = np.reshape(b4, [b4.shape[0], 1, 1])\n",
    "\n",
    "    ## Conv1D_linearphase\n",
    "\n",
    "    # input1 = Conv1D_linearphase(1 ,61, use_bias=False,\n",
    "    #                 # kernel_initializer=initializers.he_normal(random_seed),\n",
    "    #                 weights=[b1[30:]],\n",
    "    #                 padding='same',trainable=FIR_train)(input)\n",
    "    # input2 = Conv1D_linearphase(1, 61, use_bias=False,\n",
    "    #                 # kernel_initializer=initializers.he_normal(random_seed),\n",
    "    #                 weights=[b2[30:]],\n",
    "    #                 padding='same',trainable=FIR_train)(input)\n",
    "    # input3 = Conv1D_linearphase(1, 61, use_bias=False,\n",
    "    #                 # kernel_initializer=initializers.he_normal(random_seed),\n",
    "    #                 weights=[b3[30:]],\n",
    "    #                 padding='same',trainable=FIR_train)(input)\n",
    "    # input4 = Conv1D_linearphase(1, 61, use_bias=False,\n",
    "    #                 # kernel_initializer=initializers.he_normal(random_seed),\n",
    "    #                 weights=[b4[30:]],\n",
    "    #                 padding='same',trainable=FIR_train)(input)\n",
    "\n",
    "    ## Conv1D_linearphase Anti-Symmetric\n",
    "    #\n",
    "\n",
    "    if type_ % 2:\n",
    "        weight_idx = 30\n",
    "    else:\n",
    "        weight_idx = 31\n",
    "\n",
    "    input1 = Conv1D_linearphaseType(1 ,61, use_bias=False,\n",
    "                    # kernel_initializer=initializers.he_normal(random_seed),\n",
    "                    weights=[b1[weight_idx:]],\n",
    "                    padding='same',trainable=FIR_train, type = type_)(input)\n",
    "    input2 = Conv1D_linearphaseType(1, 61, use_bias=False,\n",
    "                    # kernel_initializer=initializers.he_normal(random_seed),\n",
    "                    weights=[b2[weight_idx:]],\n",
    "                    padding='same',trainable=FIR_train, type = type_)(input)\n",
    "    input3 = Conv1D_linearphaseType(1, 61, use_bias=False,\n",
    "                    # kernel_initializer=initializers.he_normal(random_seed),\n",
    "                    weights=[b3[weight_idx:]],\n",
    "                    padding='same',trainable=FIR_train, type = type_)(input)\n",
    "    input4 = Conv1D_linearphaseType(1, 61, use_bias=False,\n",
    "                    # kernel_initializer=initializers.he_normal(random_seed),\n",
    "                    weights=[b4[weight_idx:]],\n",
    "                    padding='same',trainable=FIR_train, type = type_)(input)\n",
    "\n",
    "    #Conv1D_gammatone\n",
    "\n",
    "    # input1 = Conv1D_gammatone(kernel_size=81,filters=1,fsHz=1000,use_bias=False,padding='same')(input)\n",
    "    # input2 = Conv1D_gammatone(kernel_size=81,filters=1,fsHz=1000,use_bias=False,padding='same')(input)\n",
    "    # input3 = Conv1D_gammatone(kernel_size=81,filters=1,fsHz=1000,use_bias=False,padding='same')(input)\n",
    "    # input4 = Conv1D_gammatone(kernel_size=81,filters=1,fsHz=1000,use_bias=False,padding='same')(input)\n",
    "\n",
    "    t1 = branch(input1,num_filt,kernel_size,random_seed,padding,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "    t2 = branch(input2,num_filt,kernel_size,random_seed,padding,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "    t3 = branch(input3,num_filt,kernel_size,random_seed,padding,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "    t4 = branch(input4,num_filt,kernel_size,random_seed,padding,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "\n",
    "    merged = Concatenate(axis=-1)([t1, t2, t3, t4])\n",
    "    merged = DenseNet(merged,\n",
    "                      depth=int(3*args[0]+4),\n",
    "                      nb_dense_block=args[1],\n",
    "                      growth_rate=int(args[2]),\n",
    "#                       kernel_size=args[3],\n",
    "                      kernel_size=5,\n",
    "#                       nb_filter=args[4],\n",
    "                      nb_filter=16,\n",
    "#                       dropout_rate=args[5],\n",
    "                      dropout_rate=dropout_rate,\n",
    "                     )\n",
    "    # 7,4,4,5,16; 7,1,4,5,16\n",
    "\n",
    "    # merged = DCT1D()(merged)\n",
    "    merged = Flatten()(merged)\n",
    "    merged = Dense(num_dense,\n",
    "                   activation=activation_function,\n",
    "                   kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "                   use_bias=bias,\n",
    "                   kernel_constraint=max_norm(maxnorm),\n",
    "                   kernel_regularizer=l2(l2_reg_dense))(merged)\n",
    "    # merged = BatchNormalization(epsilon=eps,momentum=bn_momentum,axis=-1) (merged)\n",
    "    # merged = Activation(activation_function)(merged)\n",
    "    merged = Dropout(rate=dropout_rate_dense, seed=random_seed)(merged)\n",
    "    merged = Dense(2, activation='softmax')(merged)\n",
    "\n",
    "    model = Model(inputs=input, outputs=merged)\n",
    "\n",
    "    if load_path:  # If path for loading model was specified\n",
    "        model.load_weights(filepath=load_path, by_name=False)\n",
    "\n",
    "    adam = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(args):\n",
    "    print(\"args {}\".format(args))\n",
    "    \n",
    "    log_name = \"hyperopt-{}\".format(args) + str(datetime.now())\n",
    "    if not os.path.exists(model_dir + log_name):\n",
    "        os.makedirs(model_dir + log_name)\n",
    "    checkpoint_name = model_dir + log_name + \"/\" + 'weights.{epoch:04d}-{val_acc:.4f}.hdf5'\n",
    "    \n",
    "    \n",
    "    csv_logger = CSVLogger(log_dir + log_name + '/training.csv')\n",
    "    tensbd = TensorBoard(log_dir=log_dir + log_name,\n",
    "                            batch_size=batch_size,\n",
    "                            write_grads=False)\n",
    "    modelcheckpnt = ModelCheckpoint(filepath=checkpoint_name,\n",
    "                                monitor='val_F1',\n",
    "                                save_best_only=True, mode='max')\n",
    "    \n",
    "    model = heartnet(load_path,activation_function, bn_momentum, bias, dropout_rate, dropout_rate_dense,\n",
    "                         eps, kernel_size, l2_reg, l2_reg_dense, lr, lr_decay, maxnorm,\n",
    "                         padding, random_seed, subsam, num_filt, num_dense, FIR_train, trainable, 1, args)\n",
    "#     model.summary()\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size, shuffle=True,\n",
    "                                     seed=random_seed),\n",
    "                            steps_per_epoch=len(x_train) // batch_size,\n",
    "                            # max_queue_size=20,\n",
    "                            use_multiprocessing=False,\n",
    "                            epochs=30,\n",
    "                            verbose=1,\n",
    "                            shuffle=True,\n",
    "                            callbacks=[modelcheckpnt,\n",
    "                                       log_macc(val_parts, decision=decision,verbose=1, val_files=val_files),\n",
    "                                       tensbd, csv_logger],\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            initial_epoch=0,\n",
    "                            )\n",
    "    y_pred = model.predict(x_val)\n",
    "    loss = K.eval(K.mean(K.variable(K.eval(categorical_crossentropy(K.variable(y_val),K.variable(y_pred))))))\n",
    "    params[str(args)]=loss\n",
    "    print(\"Loss: %f\" % loss)\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt for TPE Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (23.0, 1, 4.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_5 (Conv (None, 2500, 1)      31          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_6 (Conv (None, 2500, 1)      31          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_7 (Conv (None, 2500, 1)      31          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_8 (Conv (None, 2500, 1)      31          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 2496, 8)      40          conv1d_linearphase_type_5[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 2496, 8)      40          conv1d_linearphase_type_6[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 2496, 8)      40          conv1d_linearphase_type_7[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, 2496, 8)      40          conv1d_linearphase_type_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 2496, 8)      32          conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 2496, 8)      32          conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 2496, 8)      32          conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 2496, 8)      32          conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 2496, 8)      0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 2496, 8)      0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 2496, 8)      0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 2496, 8)      0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 2496, 8)      0           activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 2496, 8)      0           activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 2496, 8)      0           activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 2496, 8)      0           activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 1248, 8)      0           dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 1248, 8)      0           dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1248, 8)      0           dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 1248, 8)      0           dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 1244, 4)      160         max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 1244, 4)      160         max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, 1244, 4)      160         max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, 1244, 4)      160         max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 1244, 4)      16          conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 1244, 4)      16          conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 1244, 4)      16          conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 1244, 4)      16          conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 1244, 4)      0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 1244, 4)      0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 1244, 4)      0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 1244, 4)      0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 1244, 4)      0           activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 1244, 4)      0           activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 1244, 4)      0           activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 1244, 4)      0           activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 622, 4)       0           dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 622, 4)       0           dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 622, 4)       0           dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 622, 4)       0           dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 622, 16)      0           max_pooling1d_10[0][0]           \n",
      "                                                                 max_pooling1d_12[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "                                                                 max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 622, 16)      64          concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 622, 16)      0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_73 (Conv1D)              (None, 622, 4)       320         activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 622, 4)       0           conv1d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 622, 20)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 622, 20)      80          concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 622, 20)      0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_74 (Conv1D)              (None, 622, 4)       400         activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 622, 4)       0           conv1d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 622, 24)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 622, 24)      96          concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 622, 24)      0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_75 (Conv1D)              (None, 622, 4)       480         activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 622, 4)       0           conv1d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 622, 28)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 622, 28)      112         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 622, 28)      0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_76 (Conv1D)              (None, 622, 4)       560         activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 622, 4)       0           conv1d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 622, 32)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 622, 32)      128         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 622, 32)      0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_77 (Conv1D)              (None, 622, 4)       640         activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 622, 4)       0           conv1d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 622, 36)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 622, 36)      144         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 622, 36)      0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, 622, 4)       720         activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 622, 4)       0           conv1d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 622, 40)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 622, 40)      160         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 622, 40)      0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_79 (Conv1D)              (None, 622, 4)       800         activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 622, 4)       0           conv1d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 622, 44)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 622, 44)      176         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 622, 44)      0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_80 (Conv1D)              (None, 622, 4)       880         activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 622, 4)       0           conv1d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 622, 48)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 622, 48)      192         concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 622, 48)      0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, 622, 4)       960         activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 622, 4)       0           conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 622, 52)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 622, 52)      208         concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 622, 52)      0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, 622, 4)       1040        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 622, 4)       0           conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 622, 56)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 622, 56)      224         concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 622, 56)      0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, 622, 4)       1120        activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 622, 4)       0           conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 622, 60)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 622, 60)      240         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 622, 60)      0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, 622, 4)       1200        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 622, 4)       0           conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 622, 64)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 622, 64)      256         concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 622, 64)      0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, 622, 4)       1280        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 622, 4)       0           conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 622, 68)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 622, 68)      272         concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 622, 68)      0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_86 (Conv1D)              (None, 622, 4)       1360        activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 622, 4)       0           conv1d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 622, 72)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 622, 72)      288         concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 622, 72)      0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_87 (Conv1D)              (None, 622, 4)       1440        activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 622, 4)       0           conv1d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 622, 76)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 622, 76)      304         concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 622, 76)      0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, 622, 4)       1520        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)            (None, 622, 4)       0           conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 622, 80)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "                                                                 dropout_89[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 622, 80)      320         concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 622, 80)      0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, 622, 4)       1600        activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)            (None, 622, 4)       0           conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 622, 84)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "                                                                 dropout_89[0][0]                 \n",
      "                                                                 dropout_90[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 622, 84)      336         concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 622, 84)      0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, 622, 4)       1680        activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 622, 4)       0           conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 622, 88)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "                                                                 dropout_89[0][0]                 \n",
      "                                                                 dropout_90[0][0]                 \n",
      "                                                                 dropout_91[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 622, 88)      352         concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 622, 88)      0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_91 (Conv1D)              (None, 622, 4)       1760        activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)            (None, 622, 4)       0           conv1d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 622, 92)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "                                                                 dropout_89[0][0]                 \n",
      "                                                                 dropout_90[0][0]                 \n",
      "                                                                 dropout_91[0][0]                 \n",
      "                                                                 dropout_92[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 622, 92)      368         concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 622, 92)      0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, 622, 4)       1840        activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 622, 4)       0           conv1d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 622, 96)      0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "                                                                 dropout_89[0][0]                 \n",
      "                                                                 dropout_90[0][0]                 \n",
      "                                                                 dropout_91[0][0]                 \n",
      "                                                                 dropout_92[0][0]                 \n",
      "                                                                 dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 622, 96)      384         concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 622, 96)      0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_93 (Conv1D)              (None, 622, 4)       1920        activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 622, 4)       0           conv1d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 622, 100)     0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "                                                                 dropout_89[0][0]                 \n",
      "                                                                 dropout_90[0][0]                 \n",
      "                                                                 dropout_91[0][0]                 \n",
      "                                                                 dropout_92[0][0]                 \n",
      "                                                                 dropout_93[0][0]                 \n",
      "                                                                 dropout_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 622, 100)     400         concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 622, 100)     0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, 622, 4)       2000        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 622, 4)       0           conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 622, 104)     0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "                                                                 dropout_89[0][0]                 \n",
      "                                                                 dropout_90[0][0]                 \n",
      "                                                                 dropout_91[0][0]                 \n",
      "                                                                 dropout_92[0][0]                 \n",
      "                                                                 dropout_93[0][0]                 \n",
      "                                                                 dropout_94[0][0]                 \n",
      "                                                                 dropout_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 622, 104)     416         concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 622, 104)     0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, 622, 4)       2080        activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 622, 4)       0           conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 622, 108)     0           concatenate_56[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "                                                                 dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "                                                                 dropout_80[0][0]                 \n",
      "                                                                 dropout_81[0][0]                 \n",
      "                                                                 dropout_82[0][0]                 \n",
      "                                                                 dropout_83[0][0]                 \n",
      "                                                                 dropout_84[0][0]                 \n",
      "                                                                 dropout_85[0][0]                 \n",
      "                                                                 dropout_86[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "                                                                 dropout_88[0][0]                 \n",
      "                                                                 dropout_89[0][0]                 \n",
      "                                                                 dropout_90[0][0]                 \n",
      "                                                                 dropout_91[0][0]                 \n",
      "                                                                 dropout_92[0][0]                 \n",
      "                                                                 dropout_93[0][0]                 \n",
      "                                                                 dropout_94[0][0]                 \n",
      "                                                                 dropout_95[0][0]                 \n",
      "                                                                 dropout_96[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 622, 108)     432         concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 622, 108)     0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 67176)        0           activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           1343520     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 20)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            42          dropout_97[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,378,230\n",
      "Trainable params: 1,375,158\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 102s 82ms/step - loss: 0.8935 - acc: 0.8239 - val_loss: 0.7023 - val_acc: 0.6696\n",
      "6710/6710 [==============================] - 5s 721us/step\n",
      "TN:96,FP:50,FN:50,TP:88,Macc:0.647607652813,F1:0.637675608638\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 97s 77ms/step - loss: 0.3462 - acc: 0.8510 - val_loss: 0.6145 - val_acc: 0.6812\n",
      "6710/6710 [==============================] - 3s 513us/step\n",
      "TN:72,FP:74,FN:27,TP:111,Macc:0.648749204874,F1:0.68730107123\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 99s 79ms/step - loss: 0.3135 - acc: 0.8700 - val_loss: 0.6311 - val_acc: 0.6502\n",
      "6710/6710 [==============================] - 3s 510us/step\n",
      "TN:57,FP:89,FN:14,TP:124,Macc:0.644480791252,F1:0.70654741342\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2918 - acc: 0.8814 - val_loss: 0.6387 - val_acc: 0.6420\n",
      "6710/6710 [==============================] - 4s 523us/step\n",
      "TN:47,FP:99,FN:4,TP:134,Macc:0.64646609966,F1:0.722366785486\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2822 - acc: 0.8853 - val_loss: 0.6711 - val_acc: 0.6210\n",
      "6710/6710 [==============================] - 3s 510us/step\n",
      "TN:39,FP:107,FN:0,TP:138,Macc:0.633561593918,F1:0.720621519768\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 97s 78ms/step - loss: 0.2754 - acc: 0.8869 - val_loss: 0.6651 - val_acc: 0.6756\n",
      "6710/6710 [==============================] - 3s 513us/step\n",
      "TN:67,FP:79,FN:24,TP:114,Macc:0.642495482844,F1:0.688816358381\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2687 - acc: 0.8898 - val_loss: 0.6531 - val_acc: 0.6468\n",
      "6710/6710 [==============================] - 4s 525us/step\n",
      "TN:45,FP:101,FN:4,TP:134,Macc:0.639616785107,F1:0.718493489129\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2668 - acc: 0.8898 - val_loss: 0.7138 - val_acc: 0.5972\n",
      "6710/6710 [==============================] - 4s 526us/step\n",
      "TN:35,FP:111,FN:1,TP:137,Macc:0.616239776696,F1:0.70983946582\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 97s 78ms/step - loss: 0.2604 - acc: 0.8922 - val_loss: 0.6314 - val_acc: 0.6714\n",
      "6710/6710 [==============================] - 4s 541us/step\n",
      "TN:51,FP:95,FN:8,TP:130,Macc:0.645671976296,F1:0.716248216082\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 97s 77ms/step - loss: 0.2585 - acc: 0.8931 - val_loss: 0.6773 - val_acc: 0.6308\n",
      "6710/6710 [==============================] - 3s 515us/step\n",
      "TN:40,FP:106,FN:1,TP:137,Macc:0.633363063077,F1:0.719154981222\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2566 - acc: 0.8934 - val_loss: 0.6839 - val_acc: 0.6265\n",
      "6710/6710 [==============================] - 3s 506us/step\n",
      "TN:43,FP:103,FN:2,TP:136,Macc:0.640013846789,F1:0.721480263825\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2527 - acc: 0.8944 - val_loss: 0.6321 - val_acc: 0.6663\n",
      "6710/6710 [==============================] - 3s 517us/step\n",
      "TN:56,FP:90,FN:12,TP:126,Macc:0.64830251021,F1:0.711859129599\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2507 - acc: 0.8968 - val_loss: 0.6521 - val_acc: 0.6604\n",
      "6710/6710 [==============================] - 3s 518us/step\n",
      "TN:52,FP:94,FN:8,TP:130,Macc:0.649096633573,F1:0.718226811002\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2496 - acc: 0.8958 - val_loss: 0.6826 - val_acc: 0.6325\n",
      "6710/6710 [==============================] - 4s 528us/step\n",
      "TN:41,FP:105,FN:3,TP:135,Macc:0.629541344119,F1:0.714280573229\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2455 - acc: 0.8975 - val_loss: 0.6184 - val_acc: 0.6811\n",
      "6710/6710 [==============================] - 4s 523us/step\n",
      "TN:58,FP:88,FN:12,TP:126,Macc:0.655151824762,F1:0.715903802595\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2469 - acc: 0.8969 - val_loss: 0.6810 - val_acc: 0.6368\n",
      "6710/6710 [==============================] - 3s 517us/step\n",
      "TN:42,FP:104,FN:1,TP:137,Macc:0.640212377629,F1:0.722950009406\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2435 - acc: 0.8975 - val_loss: 0.6448 - val_acc: 0.6648\n",
      "6710/6710 [==============================] - 3s 504us/step\n",
      "TN:53,FP:93,FN:6,TP:132,Macc:0.659767667083,F1:0.72726749916\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2406 - acc: 0.8989 - val_loss: 0.6582 - val_acc: 0.6578\n",
      "6710/6710 [==============================] - 3s 517us/step\n",
      "TN:48,FP:98,FN:3,TP:135,Macc:0.653513945053,F1:0.727757620745\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2420 - acc: 0.8999 - val_loss: 0.6772 - val_acc: 0.6808\n",
      "6710/6710 [==============================] - 3s 516us/step\n",
      "TN:66,FP:80,FN:25,TP:113,Macc:0.635447637451,F1:0.68277406271\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 96s 77ms/step - loss: 0.2434 - acc: 0.8985 - val_loss: 0.6210 - val_acc: 0.6891\n",
      "6710/6710 [==============================] - 3s 515us/step\n",
      "TN:59,FP:87,FN:13,TP:125,Macc:0.654953293921,F1:0.714280415288\n",
      "WARNING:tensorflow:Variable /= will be deprecated. Use variable.assign_div if you want assignment to the variable value or 'x = x / y' if you want a new python Tensor object.\n",
      "Loss: 0\n",
      "args (20.0, 2, 3.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_9 (Conv (None, 2500, 1)      31          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_10 (Con (None, 2500, 1)      31          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_11 (Con (None, 2500, 1)      31          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_12 (Con (None, 2500, 1)      31          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, 2496, 8)      40          conv1d_linearphase_type_9[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 2496, 8)      40          conv1d_linearphase_type_10[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_11[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_12[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 2496, 8)      32          conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 2496, 8)      32          conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 2496, 8)      32          conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 2496, 8)      32          conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 2496, 8)      0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 2496, 8)      0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 2496, 8)      0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 2496, 8)      0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 2496, 8)      0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 2496, 8)      0           activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 2496, 8)      0           activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 2496, 8)      0           activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 1248, 8)      0           dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1248, 8)      0           dropout_100[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 1248, 8)      0           dropout_102[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 1248, 8)      0           dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 1244, 4)      160         max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 1244, 4)      160         max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 1244, 4)      16          conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 1244, 4)      16          conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 1244, 4)      16          conv1d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 1244, 4)      16          conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 1244, 4)      0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 1244, 4)      0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 1244, 4)      0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 1244, 4)      0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 1244, 4)      0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 1244, 4)      0           activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 1244, 4)      0           activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 1244, 4)      0           activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 622, 4)       0           dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 622, 4)       0           dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 622, 4)       0           dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 622, 4)       0           dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 622, 16)      0           max_pooling1d_18[0][0]           \n",
      "                                                                 max_pooling1d_20[0][0]           \n",
      "                                                                 max_pooling1d_22[0][0]           \n",
      "                                                                 max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 622, 16)      64          concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 622, 16)      0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_104 (Conv1D)             (None, 622, 3)       240         activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)           (None, 622, 3)       0           conv1d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 622, 19)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 622, 19)      76          concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 622, 19)      0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_105 (Conv1D)             (None, 622, 3)       285         activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 622, 3)       0           conv1d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 622, 22)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 622, 22)      88          concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 622, 22)      0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_106 (Conv1D)             (None, 622, 3)       330         activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, 622, 3)       0           conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 622, 25)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 622, 25)      100         concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 622, 25)      0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_107 (Conv1D)             (None, 622, 3)       375         activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 622, 3)       0           conv1d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 622, 28)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 622, 28)      112         concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 622, 28)      0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_108 (Conv1D)             (None, 622, 3)       420         activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 622, 3)       0           conv1d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 622, 31)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 622, 31)      124         concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 622, 31)      0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, 622, 3)       465         activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 622, 3)       0           conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 622, 34)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 622, 34)      136         concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 622, 34)      0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, 622, 3)       510         activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 622, 3)       0           conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 622, 37)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 622, 37)      148         concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 622, 37)      0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_111 (Conv1D)             (None, 622, 3)       555         activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 622, 3)       0           conv1d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 622, 40)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 622, 40)      160         concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 622, 40)      0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_112 (Conv1D)             (None, 622, 3)       600         activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_114 (Dropout)           (None, 622, 3)       0           conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 622, 43)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 622, 43)      172         concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 622, 43)      0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_113 (Conv1D)             (None, 622, 3)       645         activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_115 (Dropout)           (None, 622, 3)       0           conv1d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_90 (Concatenate)    (None, 622, 46)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 622, 46)      184         concatenate_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 622, 46)      0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, 622, 3)       690         activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)           (None, 622, 3)       0           conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_91 (Concatenate)    (None, 622, 49)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 622, 49)      196         concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 622, 49)      0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, 622, 3)       735         activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_117 (Dropout)           (None, 622, 3)       0           conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 622, 52)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "                                                                 dropout_117[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 622, 52)      208         concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 622, 52)      0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, 622, 3)       780         activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_118 (Dropout)           (None, 622, 3)       0           conv1d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 622, 55)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "                                                                 dropout_117[0][0]                \n",
      "                                                                 dropout_118[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 622, 55)      220         concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 622, 55)      0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_117 (Conv1D)             (None, 622, 3)       825         activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_119 (Dropout)           (None, 622, 3)       0           conv1d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 622, 58)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "                                                                 dropout_117[0][0]                \n",
      "                                                                 dropout_118[0][0]                \n",
      "                                                                 dropout_119[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 622, 58)      232         concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 622, 58)      0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_118 (Conv1D)             (None, 622, 3)       870         activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_120 (Dropout)           (None, 622, 3)       0           conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 622, 61)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "                                                                 dropout_117[0][0]                \n",
      "                                                                 dropout_118[0][0]                \n",
      "                                                                 dropout_119[0][0]                \n",
      "                                                                 dropout_120[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 622, 61)      244         concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 622, 61)      0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_119 (Conv1D)             (None, 622, 3)       915         activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_121 (Dropout)           (None, 622, 3)       0           conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 622, 64)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "                                                                 dropout_117[0][0]                \n",
      "                                                                 dropout_118[0][0]                \n",
      "                                                                 dropout_119[0][0]                \n",
      "                                                                 dropout_120[0][0]                \n",
      "                                                                 dropout_121[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 622, 64)      256         concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 622, 64)      0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_120 (Conv1D)             (None, 622, 3)       960         activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_122 (Dropout)           (None, 622, 3)       0           conv1d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 622, 67)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "                                                                 dropout_117[0][0]                \n",
      "                                                                 dropout_118[0][0]                \n",
      "                                                                 dropout_119[0][0]                \n",
      "                                                                 dropout_120[0][0]                \n",
      "                                                                 dropout_121[0][0]                \n",
      "                                                                 dropout_122[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 622, 67)      268         concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 622, 67)      0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, 622, 3)       1005        activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_123 (Dropout)           (None, 622, 3)       0           conv1d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 622, 70)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "                                                                 dropout_117[0][0]                \n",
      "                                                                 dropout_118[0][0]                \n",
      "                                                                 dropout_119[0][0]                \n",
      "                                                                 dropout_120[0][0]                \n",
      "                                                                 dropout_121[0][0]                \n",
      "                                                                 dropout_122[0][0]                \n",
      "                                                                 dropout_123[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 622, 70)      280         concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 622, 70)      0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_122 (Conv1D)             (None, 622, 3)       1050        activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_124 (Dropout)           (None, 622, 3)       0           conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_99 (Concatenate)    (None, 622, 73)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "                                                                 dropout_117[0][0]                \n",
      "                                                                 dropout_118[0][0]                \n",
      "                                                                 dropout_119[0][0]                \n",
      "                                                                 dropout_120[0][0]                \n",
      "                                                                 dropout_121[0][0]                \n",
      "                                                                 dropout_122[0][0]                \n",
      "                                                                 dropout_123[0][0]                \n",
      "                                                                 dropout_124[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 622, 73)      292         concatenate_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 622, 73)      0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_123 (Conv1D)             (None, 622, 3)       1095        activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_125 (Dropout)           (None, 622, 3)       0           conv1d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_100 (Concatenate)   (None, 622, 76)      0           concatenate_80[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "                                                                 dropout_108[0][0]                \n",
      "                                                                 dropout_109[0][0]                \n",
      "                                                                 dropout_110[0][0]                \n",
      "                                                                 dropout_111[0][0]                \n",
      "                                                                 dropout_112[0][0]                \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 dropout_114[0][0]                \n",
      "                                                                 dropout_115[0][0]                \n",
      "                                                                 dropout_116[0][0]                \n",
      "                                                                 dropout_117[0][0]                \n",
      "                                                                 dropout_118[0][0]                \n",
      "                                                                 dropout_119[0][0]                \n",
      "                                                                 dropout_120[0][0]                \n",
      "                                                                 dropout_121[0][0]                \n",
      "                                                                 dropout_122[0][0]                \n",
      "                                                                 dropout_123[0][0]                \n",
      "                                                                 dropout_124[0][0]                \n",
      "                                                                 dropout_125[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 622, 76)      304         concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 622, 76)      0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_124 (Conv1D)             (None, 622, 76)      28880       activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_126 (Dropout)           (None, 622, 76)      0           conv1d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_3 (AveragePoo (None, 311, 76)      0           dropout_126[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 311, 76)      304         average_pooling1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 311, 76)      0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, 311, 3)       1140        activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_127 (Dropout)           (None, 311, 3)       0           conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_101 (Concatenate)   (None, 311, 79)      0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 311, 79)      316         concatenate_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 311, 79)      0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_126 (Conv1D)             (None, 311, 3)       1185        activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_128 (Dropout)           (None, 311, 3)       0           conv1d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_102 (Concatenate)   (None, 311, 82)      0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 311, 82)      328         concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 311, 82)      0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_127 (Conv1D)             (None, 311, 3)       1230        activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_129 (Dropout)           (None, 311, 3)       0           conv1d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_103 (Concatenate)   (None, 311, 85)      0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 311, 85)      340         concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 311, 85)      0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_128 (Conv1D)             (None, 311, 3)       1275        activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_130 (Dropout)           (None, 311, 3)       0           conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_104 (Concatenate)   (None, 311, 88)      0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 311, 88)      352         concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 311, 88)      0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_129 (Conv1D)             (None, 311, 3)       1320        activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_131 (Dropout)           (None, 311, 3)       0           conv1d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_105 (Concatenate)   (None, 311, 91)      0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 311, 91)      364         concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 311, 91)      0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_130 (Conv1D)             (None, 311, 3)       1365        activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_132 (Dropout)           (None, 311, 3)       0           conv1d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_106 (Concatenate)   (None, 311, 94)      0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 311, 94)      376         concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 311, 94)      0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_131 (Conv1D)             (None, 311, 3)       1410        activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_133 (Dropout)           (None, 311, 3)       0           conv1d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_107 (Concatenate)   (None, 311, 97)      0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 311, 97)      388         concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 311, 97)      0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_132 (Conv1D)             (None, 311, 3)       1455        activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_134 (Dropout)           (None, 311, 3)       0           conv1d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_108 (Concatenate)   (None, 311, 100)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 311, 100)     400         concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 311, 100)     0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_133 (Conv1D)             (None, 311, 3)       1500        activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_135 (Dropout)           (None, 311, 3)       0           conv1d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_109 (Concatenate)   (None, 311, 103)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 311, 103)     412         concatenate_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 311, 103)     0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_134 (Conv1D)             (None, 311, 3)       1545        activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_136 (Dropout)           (None, 311, 3)       0           conv1d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_110 (Concatenate)   (None, 311, 106)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 311, 106)     424         concatenate_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 311, 106)     0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_135 (Conv1D)             (None, 311, 3)       1590        activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_137 (Dropout)           (None, 311, 3)       0           conv1d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_111 (Concatenate)   (None, 311, 109)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 311, 109)     436         concatenate_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 311, 109)     0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_136 (Conv1D)             (None, 311, 3)       1635        activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_138 (Dropout)           (None, 311, 3)       0           conv1d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_112 (Concatenate)   (None, 311, 112)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "                                                                 dropout_138[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 311, 112)     448         concatenate_112[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 311, 112)     0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_137 (Conv1D)             (None, 311, 3)       1680        activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_139 (Dropout)           (None, 311, 3)       0           conv1d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_113 (Concatenate)   (None, 311, 115)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "                                                                 dropout_138[0][0]                \n",
      "                                                                 dropout_139[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 311, 115)     460         concatenate_113[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 311, 115)     0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_138 (Conv1D)             (None, 311, 3)       1725        activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_140 (Dropout)           (None, 311, 3)       0           conv1d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_114 (Concatenate)   (None, 311, 118)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "                                                                 dropout_138[0][0]                \n",
      "                                                                 dropout_139[0][0]                \n",
      "                                                                 dropout_140[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 311, 118)     472         concatenate_114[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 311, 118)     0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_139 (Conv1D)             (None, 311, 3)       1770        activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_141 (Dropout)           (None, 311, 3)       0           conv1d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_115 (Concatenate)   (None, 311, 121)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "                                                                 dropout_138[0][0]                \n",
      "                                                                 dropout_139[0][0]                \n",
      "                                                                 dropout_140[0][0]                \n",
      "                                                                 dropout_141[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 311, 121)     484         concatenate_115[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 311, 121)     0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_140 (Conv1D)             (None, 311, 3)       1815        activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_142 (Dropout)           (None, 311, 3)       0           conv1d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_116 (Concatenate)   (None, 311, 124)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "                                                                 dropout_138[0][0]                \n",
      "                                                                 dropout_139[0][0]                \n",
      "                                                                 dropout_140[0][0]                \n",
      "                                                                 dropout_141[0][0]                \n",
      "                                                                 dropout_142[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 311, 124)     496         concatenate_116[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 311, 124)     0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_141 (Conv1D)             (None, 311, 3)       1860        activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_143 (Dropout)           (None, 311, 3)       0           conv1d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_117 (Concatenate)   (None, 311, 127)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "                                                                 dropout_138[0][0]                \n",
      "                                                                 dropout_139[0][0]                \n",
      "                                                                 dropout_140[0][0]                \n",
      "                                                                 dropout_141[0][0]                \n",
      "                                                                 dropout_142[0][0]                \n",
      "                                                                 dropout_143[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 311, 127)     508         concatenate_117[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 311, 127)     0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_142 (Conv1D)             (None, 311, 3)       1905        activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_144 (Dropout)           (None, 311, 3)       0           conv1d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_118 (Concatenate)   (None, 311, 130)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "                                                                 dropout_138[0][0]                \n",
      "                                                                 dropout_139[0][0]                \n",
      "                                                                 dropout_140[0][0]                \n",
      "                                                                 dropout_141[0][0]                \n",
      "                                                                 dropout_142[0][0]                \n",
      "                                                                 dropout_143[0][0]                \n",
      "                                                                 dropout_144[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 311, 130)     520         concatenate_118[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 311, 130)     0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_143 (Conv1D)             (None, 311, 3)       1950        activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_145 (Dropout)           (None, 311, 3)       0           conv1d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_119 (Concatenate)   (None, 311, 133)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "                                                                 dropout_138[0][0]                \n",
      "                                                                 dropout_139[0][0]                \n",
      "                                                                 dropout_140[0][0]                \n",
      "                                                                 dropout_141[0][0]                \n",
      "                                                                 dropout_142[0][0]                \n",
      "                                                                 dropout_143[0][0]                \n",
      "                                                                 dropout_144[0][0]                \n",
      "                                                                 dropout_145[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 311, 133)     532         concatenate_119[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 311, 133)     0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_144 (Conv1D)             (None, 311, 3)       1995        activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_146 (Dropout)           (None, 311, 3)       0           conv1d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_120 (Concatenate)   (None, 311, 136)     0           average_pooling1d_3[0][0]        \n",
      "                                                                 dropout_127[0][0]                \n",
      "                                                                 dropout_128[0][0]                \n",
      "                                                                 dropout_129[0][0]                \n",
      "                                                                 dropout_130[0][0]                \n",
      "                                                                 dropout_131[0][0]                \n",
      "                                                                 dropout_132[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "                                                                 dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "                                                                 dropout_137[0][0]                \n",
      "                                                                 dropout_138[0][0]                \n",
      "                                                                 dropout_139[0][0]                \n",
      "                                                                 dropout_140[0][0]                \n",
      "                                                                 dropout_141[0][0]                \n",
      "                                                                 dropout_142[0][0]                \n",
      "                                                                 dropout_143[0][0]                \n",
      "                                                                 dropout_144[0][0]                \n",
      "                                                                 dropout_145[0][0]                \n",
      "                                                                 dropout_146[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 311, 136)     544         concatenate_120[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 311, 136)     0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 42296)        0           activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           845920      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_147 (Dropout)           (None, 20)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            42          dropout_147[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 933,426\n",
      "Trainable params: 926,946\n",
      "Non-trainable params: 6,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 150s 120ms/step - loss: 0.9306 - acc: 0.8322 - val_loss: 0.7339 - val_acc: 0.6003\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:37,FP:109,FN:7,TP:131,Macc:0.601349962546,F1:0.693116553297\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 140s 112ms/step - loss: 0.3651 - acc: 0.8504 - val_loss: 0.6836 - val_acc: 0.5975\n",
      "6710/6710 [==============================] - 5s 718us/step\n",
      "TN:49,FP:97,FN:12,TP:126,Macc:0.624329909276,F1:0.698055704246\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.3176 - acc: 0.8730 - val_loss: 0.7229 - val_acc: 0.5550\n",
      "6710/6710 [==============================] - 5s 715us/step\n",
      "TN:27,FP:119,FN:2,TP:136,Macc:0.585219330369,F1:0.692106908051\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2941 - acc: 0.8833 - val_loss: 0.7676 - val_acc: 0.5423\n",
      "6710/6710 [==============================] - 5s 740us/step\n",
      "TN:23,FP:123,FN:0,TP:138,Macc:0.578767077498,F1:0.691724307873\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2814 - acc: 0.8878 - val_loss: 0.7752 - val_acc: 0.5511\n",
      "6710/6710 [==============================] - 5s 729us/step\n",
      "TN:28,FP:118,FN:2,TP:136,Macc:0.588643987645,F1:0.693872493737\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 140s 112ms/step - loss: 0.2710 - acc: 0.8918 - val_loss: 0.7058 - val_acc: 0.6124\n",
      "6710/6710 [==============================] - 5s 709us/step\n",
      "TN:45,FP:101,FN:5,TP:133,Macc:0.63599359699,F1:0.715048587475\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 140s 112ms/step - loss: 0.2649 - acc: 0.8929 - val_loss: 0.7391 - val_acc: 0.5869\n",
      "6710/6710 [==============================] - 5s 724us/step\n",
      "TN:36,FP:110,FN:2,TP:136,Macc:0.616041245855,F1:0.708328227846\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2603 - acc: 0.8934 - val_loss: 0.8217 - val_acc: 0.6726\n",
      "6710/6710 [==============================] - 5s 739us/step\n",
      "TN:117,FP:29,FN:79,TP:59,Macc:0.614453000221,F1:0.522118612238\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2565 - acc: 0.8958 - val_loss: 0.5726 - val_acc: 0.7228\n",
      "6710/6710 [==============================] - 5s 738us/step\n",
      "TN:85,FP:61,FN:35,TP:103,Macc:0.664284244529,F1:0.682113696417\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 139s 112ms/step - loss: 0.2529 - acc: 0.8971 - val_loss: 0.7148 - val_acc: 0.6103\n",
      "6710/6710 [==============================] - 5s 718us/step\n",
      "TN:41,FP:105,FN:7,TP:131,Macc:0.615048591651,F1:0.700529595822\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 139s 112ms/step - loss: 0.2532 - acc: 0.8969 - val_loss: 0.5579 - val_acc: 0.7212\n",
      "6710/6710 [==============================] - 5s 761us/step\n",
      "TN:68,FP:78,FN:13,TP:125,Macc:0.685775209407,F1:0.73313248249\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2477 - acc: 0.8989 - val_loss: 0.6116 - val_acc: 0.6824\n",
      "6710/6710 [==============================] - 5s 728us/step\n",
      "TN:68,FP:78,FN:18,TP:120,Macc:0.667659268822,F1:0.714280342939\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2454 - acc: 0.9000 - val_loss: 0.6445 - val_acc: 0.6700\n",
      "6710/6710 [==============================] - 5s 727us/step\n",
      "TN:54,FP:92,FN:4,TP:134,Macc:0.670438700593,F1:0.736258513259\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2434 - acc: 0.9004 - val_loss: 0.5711 - val_acc: 0.7118\n",
      "6710/6710 [==============================] - 5s 721us/step\n",
      "TN:72,FP:74,FN:26,TP:112,Macc:0.652372392991,F1:0.691352598502\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 139s 112ms/step - loss: 0.2382 - acc: 0.9036 - val_loss: 0.6117 - val_acc: 0.6923\n",
      "6710/6710 [==============================] - 5s 747us/step\n",
      "TN:96,FP:50,FN:52,TP:86,Macc:0.640361276579,F1:0.627731676216\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2367 - acc: 0.9035 - val_loss: 0.6078 - val_acc: 0.6930\n",
      "6710/6710 [==============================] - 5s 717us/step\n",
      "TN:67,FP:79,FN:19,TP:119,Macc:0.660611423429,F1:0.708327962377\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 139s 112ms/step - loss: 0.2362 - acc: 0.9031 - val_loss: 0.9392 - val_acc: 0.6499\n",
      "6710/6710 [==============================] - 5s 744us/step\n",
      "TN:141,FP:5,FN:99,TP:39,Macc:0.62418101251,F1:0.428567343957\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2338 - acc: 0.9047 - val_loss: 0.5985 - val_acc: 0.7033\n",
      "6710/6710 [==============================] - 5s 755us/step\n",
      "TN:78,FP:68,FN:35,TP:103,Macc:0.640311643596,F1:0.666661181976\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2329 - acc: 0.9041 - val_loss: 0.5571 - val_acc: 0.7176\n",
      "6710/6710 [==============================] - 5s 777us/step\n",
      "TN:111,FP:35,FN:57,TP:81,Macc:0.673615195137,F1:0.637789761656\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 139s 111ms/step - loss: 0.2317 - acc: 0.9061 - val_loss: 0.7154 - val_acc: 0.6931\n",
      "6710/6710 [==============================] - 5s 738us/step\n",
      "TN:112,FP:34,FN:57,TP:81,Macc:0.677039852414,F1:0.640310695355\n",
      "Loss: 0\n",
      "args (14.0, 2, 12.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_13 (Con (None, 2500, 1)      31          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_14 (Con (None, 2500, 1)      31          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_15 (Con (None, 2500, 1)      31          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_16 (Con (None, 2500, 1)      31          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_145 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_13[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_147 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_14[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_149 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_15[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_151 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_16[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 2496, 8)      32          conv1d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 2496, 8)      32          conv1d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 2496, 8)      32          conv1d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 2496, 8)      32          conv1d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 2496, 8)      0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 2496, 8)      0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 2496, 8)      0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 2496, 8)      0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_148 (Dropout)           (None, 2496, 8)      0           activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_150 (Dropout)           (None, 2496, 8)      0           activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_152 (Dropout)           (None, 2496, 8)      0           activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_154 (Dropout)           (None, 2496, 8)      0           activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 1248, 8)      0           dropout_148[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 1248, 8)      0           dropout_150[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 1248, 8)      0           dropout_152[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 1248, 8)      0           dropout_154[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_146 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_148 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_150 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_152 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 1244, 4)      16          conv1d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 1244, 4)      16          conv1d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 1244, 4)      16          conv1d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 1244, 4)      16          conv1d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 1244, 4)      0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 1244, 4)      0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 1244, 4)      0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 1244, 4)      0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_149 (Dropout)           (None, 1244, 4)      0           activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_151 (Dropout)           (None, 1244, 4)      0           activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_153 (Dropout)           (None, 1244, 4)      0           activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_155 (Dropout)           (None, 1244, 4)      0           activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 622, 4)       0           dropout_149[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 622, 4)       0           dropout_151[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 622, 4)       0           dropout_153[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 622, 4)       0           dropout_155[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_121 (Concatenate)   (None, 622, 16)      0           max_pooling1d_26[0][0]           \n",
      "                                                                 max_pooling1d_28[0][0]           \n",
      "                                                                 max_pooling1d_30[0][0]           \n",
      "                                                                 max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 622, 16)      64          concatenate_121[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 622, 16)      0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_153 (Conv1D)             (None, 622, 12)      960         activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_156 (Dropout)           (None, 622, 12)      0           conv1d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_122 (Concatenate)   (None, 622, 28)      0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 622, 28)      112         concatenate_122[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 622, 28)      0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_154 (Conv1D)             (None, 622, 12)      1680        activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_157 (Dropout)           (None, 622, 12)      0           conv1d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_123 (Concatenate)   (None, 622, 40)      0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 622, 40)      160         concatenate_123[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 622, 40)      0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_155 (Conv1D)             (None, 622, 12)      2400        activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_158 (Dropout)           (None, 622, 12)      0           conv1d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_124 (Concatenate)   (None, 622, 52)      0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 622, 52)      208         concatenate_124[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 622, 52)      0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_156 (Conv1D)             (None, 622, 12)      3120        activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_159 (Dropout)           (None, 622, 12)      0           conv1d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_125 (Concatenate)   (None, 622, 64)      0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 622, 64)      256         concatenate_125[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 622, 64)      0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_157 (Conv1D)             (None, 622, 12)      3840        activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_160 (Dropout)           (None, 622, 12)      0           conv1d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_126 (Concatenate)   (None, 622, 76)      0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 622, 76)      304         concatenate_126[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 622, 76)      0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_158 (Conv1D)             (None, 622, 12)      4560        activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_161 (Dropout)           (None, 622, 12)      0           conv1d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_127 (Concatenate)   (None, 622, 88)      0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "                                                                 dropout_161[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 622, 88)      352         concatenate_127[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 622, 88)      0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_159 (Conv1D)             (None, 622, 12)      5280        activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_162 (Dropout)           (None, 622, 12)      0           conv1d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_128 (Concatenate)   (None, 622, 100)     0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "                                                                 dropout_161[0][0]                \n",
      "                                                                 dropout_162[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 622, 100)     400         concatenate_128[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 622, 100)     0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_160 (Conv1D)             (None, 622, 12)      6000        activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_163 (Dropout)           (None, 622, 12)      0           conv1d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_129 (Concatenate)   (None, 622, 112)     0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "                                                                 dropout_161[0][0]                \n",
      "                                                                 dropout_162[0][0]                \n",
      "                                                                 dropout_163[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 622, 112)     448         concatenate_129[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 622, 112)     0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_161 (Conv1D)             (None, 622, 12)      6720        activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_164 (Dropout)           (None, 622, 12)      0           conv1d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_130 (Concatenate)   (None, 622, 124)     0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "                                                                 dropout_161[0][0]                \n",
      "                                                                 dropout_162[0][0]                \n",
      "                                                                 dropout_163[0][0]                \n",
      "                                                                 dropout_164[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 622, 124)     496         concatenate_130[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 622, 124)     0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_162 (Conv1D)             (None, 622, 12)      7440        activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_165 (Dropout)           (None, 622, 12)      0           conv1d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_131 (Concatenate)   (None, 622, 136)     0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "                                                                 dropout_161[0][0]                \n",
      "                                                                 dropout_162[0][0]                \n",
      "                                                                 dropout_163[0][0]                \n",
      "                                                                 dropout_164[0][0]                \n",
      "                                                                 dropout_165[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 622, 136)     544         concatenate_131[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 622, 136)     0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_163 (Conv1D)             (None, 622, 12)      8160        activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_166 (Dropout)           (None, 622, 12)      0           conv1d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_132 (Concatenate)   (None, 622, 148)     0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "                                                                 dropout_161[0][0]                \n",
      "                                                                 dropout_162[0][0]                \n",
      "                                                                 dropout_163[0][0]                \n",
      "                                                                 dropout_164[0][0]                \n",
      "                                                                 dropout_165[0][0]                \n",
      "                                                                 dropout_166[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 622, 148)     592         concatenate_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 622, 148)     0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_164 (Conv1D)             (None, 622, 12)      8880        activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_167 (Dropout)           (None, 622, 12)      0           conv1d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_133 (Concatenate)   (None, 622, 160)     0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "                                                                 dropout_161[0][0]                \n",
      "                                                                 dropout_162[0][0]                \n",
      "                                                                 dropout_163[0][0]                \n",
      "                                                                 dropout_164[0][0]                \n",
      "                                                                 dropout_165[0][0]                \n",
      "                                                                 dropout_166[0][0]                \n",
      "                                                                 dropout_167[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 622, 160)     640         concatenate_133[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 622, 160)     0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_165 (Conv1D)             (None, 622, 12)      9600        activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_168 (Dropout)           (None, 622, 12)      0           conv1d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_134 (Concatenate)   (None, 622, 172)     0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "                                                                 dropout_161[0][0]                \n",
      "                                                                 dropout_162[0][0]                \n",
      "                                                                 dropout_163[0][0]                \n",
      "                                                                 dropout_164[0][0]                \n",
      "                                                                 dropout_165[0][0]                \n",
      "                                                                 dropout_166[0][0]                \n",
      "                                                                 dropout_167[0][0]                \n",
      "                                                                 dropout_168[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 622, 172)     688         concatenate_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 622, 172)     0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_166 (Conv1D)             (None, 622, 12)      10320       activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_169 (Dropout)           (None, 622, 12)      0           conv1d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_135 (Concatenate)   (None, 622, 184)     0           concatenate_121[0][0]            \n",
      "                                                                 dropout_156[0][0]                \n",
      "                                                                 dropout_157[0][0]                \n",
      "                                                                 dropout_158[0][0]                \n",
      "                                                                 dropout_159[0][0]                \n",
      "                                                                 dropout_160[0][0]                \n",
      "                                                                 dropout_161[0][0]                \n",
      "                                                                 dropout_162[0][0]                \n",
      "                                                                 dropout_163[0][0]                \n",
      "                                                                 dropout_164[0][0]                \n",
      "                                                                 dropout_165[0][0]                \n",
      "                                                                 dropout_166[0][0]                \n",
      "                                                                 dropout_167[0][0]                \n",
      "                                                                 dropout_168[0][0]                \n",
      "                                                                 dropout_169[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 622, 184)     736         concatenate_135[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 622, 184)     0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_167 (Conv1D)             (None, 622, 184)     169280      activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_170 (Dropout)           (None, 622, 184)     0           conv1d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_4 (AveragePoo (None, 311, 184)     0           dropout_170[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 311, 184)     736         average_pooling1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 311, 184)     0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_168 (Conv1D)             (None, 311, 12)      11040       activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_171 (Dropout)           (None, 311, 12)      0           conv1d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_136 (Concatenate)   (None, 311, 196)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 311, 196)     784         concatenate_136[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 311, 196)     0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_169 (Conv1D)             (None, 311, 12)      11760       activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_172 (Dropout)           (None, 311, 12)      0           conv1d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_137 (Concatenate)   (None, 311, 208)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 311, 208)     832         concatenate_137[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 311, 208)     0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_170 (Conv1D)             (None, 311, 12)      12480       activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_173 (Dropout)           (None, 311, 12)      0           conv1d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_138 (Concatenate)   (None, 311, 220)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 311, 220)     880         concatenate_138[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 311, 220)     0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_171 (Conv1D)             (None, 311, 12)      13200       activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_174 (Dropout)           (None, 311, 12)      0           conv1d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_139 (Concatenate)   (None, 311, 232)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 311, 232)     928         concatenate_139[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 311, 232)     0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_172 (Conv1D)             (None, 311, 12)      13920       activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_175 (Dropout)           (None, 311, 12)      0           conv1d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_140 (Concatenate)   (None, 311, 244)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 311, 244)     976         concatenate_140[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 311, 244)     0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_173 (Conv1D)             (None, 311, 12)      14640       activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_176 (Dropout)           (None, 311, 12)      0           conv1d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_141 (Concatenate)   (None, 311, 256)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "                                                                 dropout_176[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 311, 256)     1024        concatenate_141[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 311, 256)     0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_174 (Conv1D)             (None, 311, 12)      15360       activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_177 (Dropout)           (None, 311, 12)      0           conv1d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_142 (Concatenate)   (None, 311, 268)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "                                                                 dropout_176[0][0]                \n",
      "                                                                 dropout_177[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 311, 268)     1072        concatenate_142[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 311, 268)     0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_175 (Conv1D)             (None, 311, 12)      16080       activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_178 (Dropout)           (None, 311, 12)      0           conv1d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_143 (Concatenate)   (None, 311, 280)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "                                                                 dropout_176[0][0]                \n",
      "                                                                 dropout_177[0][0]                \n",
      "                                                                 dropout_178[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 311, 280)     1120        concatenate_143[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 311, 280)     0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_176 (Conv1D)             (None, 311, 12)      16800       activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_179 (Dropout)           (None, 311, 12)      0           conv1d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_144 (Concatenate)   (None, 311, 292)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "                                                                 dropout_176[0][0]                \n",
      "                                                                 dropout_177[0][0]                \n",
      "                                                                 dropout_178[0][0]                \n",
      "                                                                 dropout_179[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 311, 292)     1168        concatenate_144[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 311, 292)     0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_177 (Conv1D)             (None, 311, 12)      17520       activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_180 (Dropout)           (None, 311, 12)      0           conv1d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_145 (Concatenate)   (None, 311, 304)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "                                                                 dropout_176[0][0]                \n",
      "                                                                 dropout_177[0][0]                \n",
      "                                                                 dropout_178[0][0]                \n",
      "                                                                 dropout_179[0][0]                \n",
      "                                                                 dropout_180[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 311, 304)     1216        concatenate_145[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 311, 304)     0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_178 (Conv1D)             (None, 311, 12)      18240       activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_181 (Dropout)           (None, 311, 12)      0           conv1d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_146 (Concatenate)   (None, 311, 316)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "                                                                 dropout_176[0][0]                \n",
      "                                                                 dropout_177[0][0]                \n",
      "                                                                 dropout_178[0][0]                \n",
      "                                                                 dropout_179[0][0]                \n",
      "                                                                 dropout_180[0][0]                \n",
      "                                                                 dropout_181[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 311, 316)     1264        concatenate_146[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 311, 316)     0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_179 (Conv1D)             (None, 311, 12)      18960       activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_182 (Dropout)           (None, 311, 12)      0           conv1d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_147 (Concatenate)   (None, 311, 328)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "                                                                 dropout_176[0][0]                \n",
      "                                                                 dropout_177[0][0]                \n",
      "                                                                 dropout_178[0][0]                \n",
      "                                                                 dropout_179[0][0]                \n",
      "                                                                 dropout_180[0][0]                \n",
      "                                                                 dropout_181[0][0]                \n",
      "                                                                 dropout_182[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 311, 328)     1312        concatenate_147[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 311, 328)     0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_180 (Conv1D)             (None, 311, 12)      19680       activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_183 (Dropout)           (None, 311, 12)      0           conv1d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_148 (Concatenate)   (None, 311, 340)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "                                                                 dropout_176[0][0]                \n",
      "                                                                 dropout_177[0][0]                \n",
      "                                                                 dropout_178[0][0]                \n",
      "                                                                 dropout_179[0][0]                \n",
      "                                                                 dropout_180[0][0]                \n",
      "                                                                 dropout_181[0][0]                \n",
      "                                                                 dropout_182[0][0]                \n",
      "                                                                 dropout_183[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 311, 340)     1360        concatenate_148[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 311, 340)     0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_181 (Conv1D)             (None, 311, 12)      20400       activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_184 (Dropout)           (None, 311, 12)      0           conv1d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_149 (Concatenate)   (None, 311, 352)     0           average_pooling1d_4[0][0]        \n",
      "                                                                 dropout_171[0][0]                \n",
      "                                                                 dropout_172[0][0]                \n",
      "                                                                 dropout_173[0][0]                \n",
      "                                                                 dropout_174[0][0]                \n",
      "                                                                 dropout_175[0][0]                \n",
      "                                                                 dropout_176[0][0]                \n",
      "                                                                 dropout_177[0][0]                \n",
      "                                                                 dropout_178[0][0]                \n",
      "                                                                 dropout_179[0][0]                \n",
      "                                                                 dropout_180[0][0]                \n",
      "                                                                 dropout_181[0][0]                \n",
      "                                                                 dropout_182[0][0]                \n",
      "                                                                 dropout_183[0][0]                \n",
      "                                                                 dropout_184[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 311, 352)     1408        concatenate_149[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 311, 352)     0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 109472)       0           activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 20)           2189440     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_185 (Dropout)           (None, 20)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            42          dropout_185[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,680,998\n",
      "Trainable params: 2,669,862\n",
      "Non-trainable params: 11,136\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 197s 158ms/step - loss: 3.6431 - acc: 0.8115 - val_loss: 7.2597 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 1.7834 - acc: 0.8100 - val_loss: 0.7096 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 187s 150ms/step - loss: 0.3274 - acc: 0.8261 - val_loss: 0.6238 - val_acc: 0.6519\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:68,FP:78,FN:26,TP:112,Macc:0.638673763886,F1:0.68292142174\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 188s 150ms/step - loss: 0.3085 - acc: 0.8397 - val_loss: 0.6143 - val_acc: 0.6548\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:77,FP:69,FN:26,TP:112,Macc:0.669495679372,F1:0.702188908916\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2997 - acc: 0.8552 - val_loss: 0.7110 - val_acc: 0.6201\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:73,FP:73,FN:32,TP:106,Macc:0.634057921566,F1:0.668764261724\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2860 - acc: 0.8695 - val_loss: 0.6835 - val_acc: 0.5706\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:32,FP:114,FN:12,TP:126,Macc:0.56611073558,F1:0.666661528384\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2771 - acc: 0.8759 - val_loss: 0.6582 - val_acc: 0.6064\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:43,FP:103,FN:10,TP:128,Macc:0.611028341853,F1:0.693761745707\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2740 - acc: 0.8800 - val_loss: 0.6003 - val_acc: 0.6519\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:82,FP:64,FN:36,TP:102,Macc:0.650387084584,F1:0.671047129719\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 189s 151ms/step - loss: 0.2668 - acc: 0.8823 - val_loss: 0.6395 - val_acc: 0.6505\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:80,FP:66,FN:34,TP:104,Macc:0.650784146265,F1:0.6753191865\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2637 - acc: 0.8850 - val_loss: 1.1174 - val_acc: 0.6306\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:100,FP:46,FN:60,TP:78,Macc:0.625074400748,F1:0.595414313086\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2617 - acc: 0.8861 - val_loss: 0.7123 - val_acc: 0.6270\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:75,FP:71,FN:32,TP:106,Macc:0.640907236118,F1:0.673010410364\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 189s 151ms/step - loss: 0.2591 - acc: 0.8869 - val_loss: 0.6523 - val_acc: 0.6344\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:52,FP:94,FN:13,TP:125,Macc:0.630980692988,F1:0.700274852066\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 189s 152ms/step - loss: 0.2565 - acc: 0.8894 - val_loss: 0.6587 - val_acc: 0.6434\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:58,FP:88,FN:17,TP:121,Macc:0.637035884177,F1:0.697401026142\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2539 - acc: 0.8907 - val_loss: 0.6940 - val_acc: 0.6122\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:40,FP:106,FN:5,TP:133,Macc:0.618870310609,F1:0.705565145391\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2539 - acc: 0.8916 - val_loss: 0.7261 - val_acc: 0.6063\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:50,FP:96,FN:15,TP:123,Macc:0.616885002201,F1:0.689070370964\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2515 - acc: 0.8907 - val_loss: 0.7271 - val_acc: 0.6004\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:34,FP:112,FN:2,TP:136,Macc:0.609191931303,F1:0.704658118966\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 189s 151ms/step - loss: 0.2511 - acc: 0.8908 - val_loss: 0.6599 - val_acc: 0.6367\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:56,FP:90,FN:14,TP:124,Macc:0.641056133976,F1:0.704540166942\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2509 - acc: 0.8910 - val_loss: 0.7443 - val_acc: 0.6505\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:75,FP:71,FN:29,TP:109,Macc:0.651776800469,F1:0.685529139752\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2491 - acc: 0.8930 - val_loss: 0.7646 - val_acc: 0.6171\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:55,FP:91,FN:20,TP:118,Macc:0.615892347997,F1:0.680109960957\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 188s 151ms/step - loss: 0.2486 - acc: 0.8932 - val_loss: 0.7502 - val_acc: 0.6419\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:78,FP:68,FN:30,TP:108,Macc:0.658427584181,F1:0.687892621569\n",
      "Loss: 0\n",
      "args (21.0, 3, 5.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_17 (Con (None, 2500, 1)      31          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_18 (Con (None, 2500, 1)      31          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_19 (Con (None, 2500, 1)      31          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_20 (Con (None, 2500, 1)      31          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_182 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_17[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_184 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_18[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_186 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_19[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_188 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_20[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 2496, 8)      32          conv1d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 2496, 8)      32          conv1d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 2496, 8)      32          conv1d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 2496, 8)      32          conv1d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 2496, 8)      0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 2496, 8)      0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 2496, 8)      0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 2496, 8)      0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_186 (Dropout)           (None, 2496, 8)      0           activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_188 (Dropout)           (None, 2496, 8)      0           activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_190 (Dropout)           (None, 2496, 8)      0           activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_192 (Dropout)           (None, 2496, 8)      0           activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 1248, 8)      0           dropout_186[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 1248, 8)      0           dropout_188[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1248, 8)      0           dropout_190[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 1248, 8)      0           dropout_192[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_183 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_185 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_187 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_189 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 1244, 4)      16          conv1d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 1244, 4)      16          conv1d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 1244, 4)      16          conv1d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 1244, 4)      16          conv1d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 1244, 4)      0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 1244, 4)      0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 1244, 4)      0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 1244, 4)      0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_187 (Dropout)           (None, 1244, 4)      0           activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_189 (Dropout)           (None, 1244, 4)      0           activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_191 (Dropout)           (None, 1244, 4)      0           activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_193 (Dropout)           (None, 1244, 4)      0           activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 622, 4)       0           dropout_187[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 622, 4)       0           dropout_189[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 622, 4)       0           dropout_191[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 622, 4)       0           dropout_193[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_150 (Concatenate)   (None, 622, 16)      0           max_pooling1d_34[0][0]           \n",
      "                                                                 max_pooling1d_36[0][0]           \n",
      "                                                                 max_pooling1d_38[0][0]           \n",
      "                                                                 max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 622, 16)      64          concatenate_150[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 622, 16)      0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_190 (Conv1D)             (None, 622, 5)       400         activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_194 (Dropout)           (None, 622, 5)       0           conv1d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_151 (Concatenate)   (None, 622, 21)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 622, 21)      84          concatenate_151[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 622, 21)      0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_191 (Conv1D)             (None, 622, 5)       525         activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_195 (Dropout)           (None, 622, 5)       0           conv1d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_152 (Concatenate)   (None, 622, 26)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 622, 26)      104         concatenate_152[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 622, 26)      0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_192 (Conv1D)             (None, 622, 5)       650         activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_196 (Dropout)           (None, 622, 5)       0           conv1d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_153 (Concatenate)   (None, 622, 31)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 622, 31)      124         concatenate_153[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 622, 31)      0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_193 (Conv1D)             (None, 622, 5)       775         activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_197 (Dropout)           (None, 622, 5)       0           conv1d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_154 (Concatenate)   (None, 622, 36)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 622, 36)      144         concatenate_154[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 622, 36)      0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_194 (Conv1D)             (None, 622, 5)       900         activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_198 (Dropout)           (None, 622, 5)       0           conv1d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_155 (Concatenate)   (None, 622, 41)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 622, 41)      164         concatenate_155[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 622, 41)      0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_195 (Conv1D)             (None, 622, 5)       1025        activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_199 (Dropout)           (None, 622, 5)       0           conv1d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_156 (Concatenate)   (None, 622, 46)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 622, 46)      184         concatenate_156[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 622, 46)      0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_196 (Conv1D)             (None, 622, 5)       1150        activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_200 (Dropout)           (None, 622, 5)       0           conv1d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_157 (Concatenate)   (None, 622, 51)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 622, 51)      204         concatenate_157[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 622, 51)      0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_197 (Conv1D)             (None, 622, 5)       1275        activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_201 (Dropout)           (None, 622, 5)       0           conv1d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_158 (Concatenate)   (None, 622, 56)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 622, 56)      224         concatenate_158[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 622, 56)      0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_198 (Conv1D)             (None, 622, 5)       1400        activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_202 (Dropout)           (None, 622, 5)       0           conv1d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_159 (Concatenate)   (None, 622, 61)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 622, 61)      244         concatenate_159[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 622, 61)      0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_199 (Conv1D)             (None, 622, 5)       1525        activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_203 (Dropout)           (None, 622, 5)       0           conv1d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_160 (Concatenate)   (None, 622, 66)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 622, 66)      264         concatenate_160[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 622, 66)      0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_200 (Conv1D)             (None, 622, 5)       1650        activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_204 (Dropout)           (None, 622, 5)       0           conv1d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_161 (Concatenate)   (None, 622, 71)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 622, 71)      284         concatenate_161[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 622, 71)      0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_201 (Conv1D)             (None, 622, 5)       1775        activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_205 (Dropout)           (None, 622, 5)       0           conv1d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_162 (Concatenate)   (None, 622, 76)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 622, 76)      304         concatenate_162[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 622, 76)      0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_202 (Conv1D)             (None, 622, 5)       1900        activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_206 (Dropout)           (None, 622, 5)       0           conv1d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_163 (Concatenate)   (None, 622, 81)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "                                                                 dropout_206[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 622, 81)      324         concatenate_163[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 622, 81)      0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_203 (Conv1D)             (None, 622, 5)       2025        activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_207 (Dropout)           (None, 622, 5)       0           conv1d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_164 (Concatenate)   (None, 622, 86)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "                                                                 dropout_206[0][0]                \n",
      "                                                                 dropout_207[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 622, 86)      344         concatenate_164[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 622, 86)      0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_204 (Conv1D)             (None, 622, 5)       2150        activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_208 (Dropout)           (None, 622, 5)       0           conv1d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_165 (Concatenate)   (None, 622, 91)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "                                                                 dropout_206[0][0]                \n",
      "                                                                 dropout_207[0][0]                \n",
      "                                                                 dropout_208[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 622, 91)      364         concatenate_165[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 622, 91)      0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_205 (Conv1D)             (None, 622, 5)       2275        activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_209 (Dropout)           (None, 622, 5)       0           conv1d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_166 (Concatenate)   (None, 622, 96)      0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "                                                                 dropout_206[0][0]                \n",
      "                                                                 dropout_207[0][0]                \n",
      "                                                                 dropout_208[0][0]                \n",
      "                                                                 dropout_209[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 622, 96)      384         concatenate_166[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 622, 96)      0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_206 (Conv1D)             (None, 622, 5)       2400        activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_210 (Dropout)           (None, 622, 5)       0           conv1d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_167 (Concatenate)   (None, 622, 101)     0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "                                                                 dropout_206[0][0]                \n",
      "                                                                 dropout_207[0][0]                \n",
      "                                                                 dropout_208[0][0]                \n",
      "                                                                 dropout_209[0][0]                \n",
      "                                                                 dropout_210[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 622, 101)     404         concatenate_167[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 622, 101)     0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_207 (Conv1D)             (None, 622, 5)       2525        activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_211 (Dropout)           (None, 622, 5)       0           conv1d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_168 (Concatenate)   (None, 622, 106)     0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "                                                                 dropout_206[0][0]                \n",
      "                                                                 dropout_207[0][0]                \n",
      "                                                                 dropout_208[0][0]                \n",
      "                                                                 dropout_209[0][0]                \n",
      "                                                                 dropout_210[0][0]                \n",
      "                                                                 dropout_211[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 622, 106)     424         concatenate_168[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 622, 106)     0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_208 (Conv1D)             (None, 622, 5)       2650        activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_212 (Dropout)           (None, 622, 5)       0           conv1d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_169 (Concatenate)   (None, 622, 111)     0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "                                                                 dropout_206[0][0]                \n",
      "                                                                 dropout_207[0][0]                \n",
      "                                                                 dropout_208[0][0]                \n",
      "                                                                 dropout_209[0][0]                \n",
      "                                                                 dropout_210[0][0]                \n",
      "                                                                 dropout_211[0][0]                \n",
      "                                                                 dropout_212[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 622, 111)     444         concatenate_169[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 622, 111)     0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_209 (Conv1D)             (None, 622, 5)       2775        activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_213 (Dropout)           (None, 622, 5)       0           conv1d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_170 (Concatenate)   (None, 622, 116)     0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "                                                                 dropout_206[0][0]                \n",
      "                                                                 dropout_207[0][0]                \n",
      "                                                                 dropout_208[0][0]                \n",
      "                                                                 dropout_209[0][0]                \n",
      "                                                                 dropout_210[0][0]                \n",
      "                                                                 dropout_211[0][0]                \n",
      "                                                                 dropout_212[0][0]                \n",
      "                                                                 dropout_213[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 622, 116)     464         concatenate_170[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 622, 116)     0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_210 (Conv1D)             (None, 622, 5)       2900        activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_214 (Dropout)           (None, 622, 5)       0           conv1d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_171 (Concatenate)   (None, 622, 121)     0           concatenate_150[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "                                                                 dropout_195[0][0]                \n",
      "                                                                 dropout_196[0][0]                \n",
      "                                                                 dropout_197[0][0]                \n",
      "                                                                 dropout_198[0][0]                \n",
      "                                                                 dropout_199[0][0]                \n",
      "                                                                 dropout_200[0][0]                \n",
      "                                                                 dropout_201[0][0]                \n",
      "                                                                 dropout_202[0][0]                \n",
      "                                                                 dropout_203[0][0]                \n",
      "                                                                 dropout_204[0][0]                \n",
      "                                                                 dropout_205[0][0]                \n",
      "                                                                 dropout_206[0][0]                \n",
      "                                                                 dropout_207[0][0]                \n",
      "                                                                 dropout_208[0][0]                \n",
      "                                                                 dropout_209[0][0]                \n",
      "                                                                 dropout_210[0][0]                \n",
      "                                                                 dropout_211[0][0]                \n",
      "                                                                 dropout_212[0][0]                \n",
      "                                                                 dropout_213[0][0]                \n",
      "                                                                 dropout_214[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 622, 121)     484         concatenate_171[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 622, 121)     0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_211 (Conv1D)             (None, 622, 121)     73205       activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_215 (Dropout)           (None, 622, 121)     0           conv1d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_5 (AveragePoo (None, 311, 121)     0           dropout_215[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 311, 121)     484         average_pooling1d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 311, 121)     0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_212 (Conv1D)             (None, 311, 5)       3025        activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_216 (Dropout)           (None, 311, 5)       0           conv1d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_172 (Concatenate)   (None, 311, 126)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 311, 126)     504         concatenate_172[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 311, 126)     0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_213 (Conv1D)             (None, 311, 5)       3150        activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_217 (Dropout)           (None, 311, 5)       0           conv1d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_173 (Concatenate)   (None, 311, 131)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 311, 131)     524         concatenate_173[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 311, 131)     0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_214 (Conv1D)             (None, 311, 5)       3275        activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_218 (Dropout)           (None, 311, 5)       0           conv1d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_174 (Concatenate)   (None, 311, 136)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 311, 136)     544         concatenate_174[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 311, 136)     0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_215 (Conv1D)             (None, 311, 5)       3400        activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_219 (Dropout)           (None, 311, 5)       0           conv1d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_175 (Concatenate)   (None, 311, 141)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 311, 141)     564         concatenate_175[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 311, 141)     0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_216 (Conv1D)             (None, 311, 5)       3525        activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_220 (Dropout)           (None, 311, 5)       0           conv1d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_176 (Concatenate)   (None, 311, 146)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 311, 146)     584         concatenate_176[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 311, 146)     0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_217 (Conv1D)             (None, 311, 5)       3650        activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_221 (Dropout)           (None, 311, 5)       0           conv1d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_177 (Concatenate)   (None, 311, 151)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 311, 151)     604         concatenate_177[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 311, 151)     0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_218 (Conv1D)             (None, 311, 5)       3775        activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_222 (Dropout)           (None, 311, 5)       0           conv1d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_178 (Concatenate)   (None, 311, 156)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 311, 156)     624         concatenate_178[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 311, 156)     0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_219 (Conv1D)             (None, 311, 5)       3900        activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_223 (Dropout)           (None, 311, 5)       0           conv1d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_179 (Concatenate)   (None, 311, 161)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 311, 161)     644         concatenate_179[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 311, 161)     0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_220 (Conv1D)             (None, 311, 5)       4025        activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_224 (Dropout)           (None, 311, 5)       0           conv1d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_180 (Concatenate)   (None, 311, 166)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 311, 166)     664         concatenate_180[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 311, 166)     0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_221 (Conv1D)             (None, 311, 5)       4150        activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_225 (Dropout)           (None, 311, 5)       0           conv1d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_181 (Concatenate)   (None, 311, 171)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 311, 171)     684         concatenate_181[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 311, 171)     0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_222 (Conv1D)             (None, 311, 5)       4275        activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_226 (Dropout)           (None, 311, 5)       0           conv1d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_182 (Concatenate)   (None, 311, 176)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 311, 176)     704         concatenate_182[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 311, 176)     0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_223 (Conv1D)             (None, 311, 5)       4400        activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_227 (Dropout)           (None, 311, 5)       0           conv1d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_183 (Concatenate)   (None, 311, 181)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 311, 181)     724         concatenate_183[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 311, 181)     0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_224 (Conv1D)             (None, 311, 5)       4525        activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_228 (Dropout)           (None, 311, 5)       0           conv1d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_184 (Concatenate)   (None, 311, 186)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "                                                                 dropout_228[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 311, 186)     744         concatenate_184[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 311, 186)     0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_225 (Conv1D)             (None, 311, 5)       4650        activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_229 (Dropout)           (None, 311, 5)       0           conv1d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_185 (Concatenate)   (None, 311, 191)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "                                                                 dropout_228[0][0]                \n",
      "                                                                 dropout_229[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 311, 191)     764         concatenate_185[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 311, 191)     0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_226 (Conv1D)             (None, 311, 5)       4775        activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_230 (Dropout)           (None, 311, 5)       0           conv1d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_186 (Concatenate)   (None, 311, 196)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "                                                                 dropout_228[0][0]                \n",
      "                                                                 dropout_229[0][0]                \n",
      "                                                                 dropout_230[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 311, 196)     784         concatenate_186[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 311, 196)     0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_227 (Conv1D)             (None, 311, 5)       4900        activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_231 (Dropout)           (None, 311, 5)       0           conv1d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_187 (Concatenate)   (None, 311, 201)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "                                                                 dropout_228[0][0]                \n",
      "                                                                 dropout_229[0][0]                \n",
      "                                                                 dropout_230[0][0]                \n",
      "                                                                 dropout_231[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 311, 201)     804         concatenate_187[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 311, 201)     0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_228 (Conv1D)             (None, 311, 5)       5025        activation_232[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_232 (Dropout)           (None, 311, 5)       0           conv1d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_188 (Concatenate)   (None, 311, 206)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "                                                                 dropout_228[0][0]                \n",
      "                                                                 dropout_229[0][0]                \n",
      "                                                                 dropout_230[0][0]                \n",
      "                                                                 dropout_231[0][0]                \n",
      "                                                                 dropout_232[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 311, 206)     824         concatenate_188[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 311, 206)     0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_229 (Conv1D)             (None, 311, 5)       5150        activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_233 (Dropout)           (None, 311, 5)       0           conv1d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_189 (Concatenate)   (None, 311, 211)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "                                                                 dropout_228[0][0]                \n",
      "                                                                 dropout_229[0][0]                \n",
      "                                                                 dropout_230[0][0]                \n",
      "                                                                 dropout_231[0][0]                \n",
      "                                                                 dropout_232[0][0]                \n",
      "                                                                 dropout_233[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 311, 211)     844         concatenate_189[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 311, 211)     0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_230 (Conv1D)             (None, 311, 5)       5275        activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_234 (Dropout)           (None, 311, 5)       0           conv1d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_190 (Concatenate)   (None, 311, 216)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "                                                                 dropout_228[0][0]                \n",
      "                                                                 dropout_229[0][0]                \n",
      "                                                                 dropout_230[0][0]                \n",
      "                                                                 dropout_231[0][0]                \n",
      "                                                                 dropout_232[0][0]                \n",
      "                                                                 dropout_233[0][0]                \n",
      "                                                                 dropout_234[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 311, 216)     864         concatenate_190[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 311, 216)     0           batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_231 (Conv1D)             (None, 311, 5)       5400        activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_235 (Dropout)           (None, 311, 5)       0           conv1d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_191 (Concatenate)   (None, 311, 221)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "                                                                 dropout_228[0][0]                \n",
      "                                                                 dropout_229[0][0]                \n",
      "                                                                 dropout_230[0][0]                \n",
      "                                                                 dropout_231[0][0]                \n",
      "                                                                 dropout_232[0][0]                \n",
      "                                                                 dropout_233[0][0]                \n",
      "                                                                 dropout_234[0][0]                \n",
      "                                                                 dropout_235[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 311, 221)     884         concatenate_191[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 311, 221)     0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_232 (Conv1D)             (None, 311, 5)       5525        activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_236 (Dropout)           (None, 311, 5)       0           conv1d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_192 (Concatenate)   (None, 311, 226)     0           average_pooling1d_5[0][0]        \n",
      "                                                                 dropout_216[0][0]                \n",
      "                                                                 dropout_217[0][0]                \n",
      "                                                                 dropout_218[0][0]                \n",
      "                                                                 dropout_219[0][0]                \n",
      "                                                                 dropout_220[0][0]                \n",
      "                                                                 dropout_221[0][0]                \n",
      "                                                                 dropout_222[0][0]                \n",
      "                                                                 dropout_223[0][0]                \n",
      "                                                                 dropout_224[0][0]                \n",
      "                                                                 dropout_225[0][0]                \n",
      "                                                                 dropout_226[0][0]                \n",
      "                                                                 dropout_227[0][0]                \n",
      "                                                                 dropout_228[0][0]                \n",
      "                                                                 dropout_229[0][0]                \n",
      "                                                                 dropout_230[0][0]                \n",
      "                                                                 dropout_231[0][0]                \n",
      "                                                                 dropout_232[0][0]                \n",
      "                                                                 dropout_233[0][0]                \n",
      "                                                                 dropout_234[0][0]                \n",
      "                                                                 dropout_235[0][0]                \n",
      "                                                                 dropout_236[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 311, 226)     904         concatenate_192[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 311, 226)     0           batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_233 (Conv1D)             (None, 311, 226)     255380      activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_237 (Dropout)           (None, 311, 226)     0           conv1d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_6 (AveragePoo (None, 155, 226)     0           dropout_237[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 155, 226)     904         average_pooling1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, 155, 226)     0           batch_normalization_238[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_234 (Conv1D)             (None, 155, 5)       5650        activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_238 (Dropout)           (None, 155, 5)       0           conv1d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_193 (Concatenate)   (None, 155, 231)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 155, 231)     924         concatenate_193[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, 155, 231)     0           batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_235 (Conv1D)             (None, 155, 5)       5775        activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_239 (Dropout)           (None, 155, 5)       0           conv1d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_194 (Concatenate)   (None, 155, 236)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 155, 236)     944         concatenate_194[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 155, 236)     0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_236 (Conv1D)             (None, 155, 5)       5900        activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_240 (Dropout)           (None, 155, 5)       0           conv1d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_195 (Concatenate)   (None, 155, 241)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 155, 241)     964         concatenate_195[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 155, 241)     0           batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_237 (Conv1D)             (None, 155, 5)       6025        activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_241 (Dropout)           (None, 155, 5)       0           conv1d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_196 (Concatenate)   (None, 155, 246)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 155, 246)     984         concatenate_196[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, 155, 246)     0           batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_238 (Conv1D)             (None, 155, 5)       6150        activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_242 (Dropout)           (None, 155, 5)       0           conv1d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_197 (Concatenate)   (None, 155, 251)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, 155, 251)     1004        concatenate_197[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 155, 251)     0           batch_normalization_243[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_239 (Conv1D)             (None, 155, 5)       6275        activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_243 (Dropout)           (None, 155, 5)       0           conv1d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_198 (Concatenate)   (None, 155, 256)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 155, 256)     1024        concatenate_198[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 155, 256)     0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_240 (Conv1D)             (None, 155, 5)       6400        activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_244 (Dropout)           (None, 155, 5)       0           conv1d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_199 (Concatenate)   (None, 155, 261)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, 155, 261)     1044        concatenate_199[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 155, 261)     0           batch_normalization_245[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_241 (Conv1D)             (None, 155, 5)       6525        activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_245 (Dropout)           (None, 155, 5)       0           conv1d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_200 (Concatenate)   (None, 155, 266)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_246 (BatchN (None, 155, 266)     1064        concatenate_200[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, 155, 266)     0           batch_normalization_246[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_242 (Conv1D)             (None, 155, 5)       6650        activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_246 (Dropout)           (None, 155, 5)       0           conv1d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_201 (Concatenate)   (None, 155, 271)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchN (None, 155, 271)     1084        concatenate_201[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, 155, 271)     0           batch_normalization_247[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_243 (Conv1D)             (None, 155, 5)       6775        activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_247 (Dropout)           (None, 155, 5)       0           conv1d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_202 (Concatenate)   (None, 155, 276)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, 155, 276)     1104        concatenate_202[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 155, 276)     0           batch_normalization_248[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_244 (Conv1D)             (None, 155, 5)       6900        activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_248 (Dropout)           (None, 155, 5)       0           conv1d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_203 (Concatenate)   (None, 155, 281)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_249 (BatchN (None, 155, 281)     1124        concatenate_203[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 155, 281)     0           batch_normalization_249[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_245 (Conv1D)             (None, 155, 5)       7025        activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_249 (Dropout)           (None, 155, 5)       0           conv1d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_204 (Concatenate)   (None, 155, 286)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchN (None, 155, 286)     1144        concatenate_204[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, 155, 286)     0           batch_normalization_250[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_246 (Conv1D)             (None, 155, 5)       7150        activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_250 (Dropout)           (None, 155, 5)       0           conv1d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_205 (Concatenate)   (None, 155, 291)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "                                                                 dropout_250[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchN (None, 155, 291)     1164        concatenate_205[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, 155, 291)     0           batch_normalization_251[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_247 (Conv1D)             (None, 155, 5)       7275        activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_251 (Dropout)           (None, 155, 5)       0           conv1d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_206 (Concatenate)   (None, 155, 296)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "                                                                 dropout_250[0][0]                \n",
      "                                                                 dropout_251[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, 155, 296)     1184        concatenate_206[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 155, 296)     0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_248 (Conv1D)             (None, 155, 5)       7400        activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_252 (Dropout)           (None, 155, 5)       0           conv1d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_207 (Concatenate)   (None, 155, 301)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "                                                                 dropout_250[0][0]                \n",
      "                                                                 dropout_251[0][0]                \n",
      "                                                                 dropout_252[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, 155, 301)     1204        concatenate_207[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 155, 301)     0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_249 (Conv1D)             (None, 155, 5)       7525        activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_253 (Dropout)           (None, 155, 5)       0           conv1d_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_208 (Concatenate)   (None, 155, 306)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "                                                                 dropout_250[0][0]                \n",
      "                                                                 dropout_251[0][0]                \n",
      "                                                                 dropout_252[0][0]                \n",
      "                                                                 dropout_253[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, 155, 306)     1224        concatenate_208[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, 155, 306)     0           batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_250 (Conv1D)             (None, 155, 5)       7650        activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_254 (Dropout)           (None, 155, 5)       0           conv1d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_209 (Concatenate)   (None, 155, 311)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "                                                                 dropout_250[0][0]                \n",
      "                                                                 dropout_251[0][0]                \n",
      "                                                                 dropout_252[0][0]                \n",
      "                                                                 dropout_253[0][0]                \n",
      "                                                                 dropout_254[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchN (None, 155, 311)     1244        concatenate_209[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, 155, 311)     0           batch_normalization_255[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_251 (Conv1D)             (None, 155, 5)       7775        activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_255 (Dropout)           (None, 155, 5)       0           conv1d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_210 (Concatenate)   (None, 155, 316)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "                                                                 dropout_250[0][0]                \n",
      "                                                                 dropout_251[0][0]                \n",
      "                                                                 dropout_252[0][0]                \n",
      "                                                                 dropout_253[0][0]                \n",
      "                                                                 dropout_254[0][0]                \n",
      "                                                                 dropout_255[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, 155, 316)     1264        concatenate_210[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, 155, 316)     0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_252 (Conv1D)             (None, 155, 5)       7900        activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_256 (Dropout)           (None, 155, 5)       0           conv1d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_211 (Concatenate)   (None, 155, 321)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "                                                                 dropout_250[0][0]                \n",
      "                                                                 dropout_251[0][0]                \n",
      "                                                                 dropout_252[0][0]                \n",
      "                                                                 dropout_253[0][0]                \n",
      "                                                                 dropout_254[0][0]                \n",
      "                                                                 dropout_255[0][0]                \n",
      "                                                                 dropout_256[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_257 (BatchN (None, 155, 321)     1284        concatenate_211[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, 155, 321)     0           batch_normalization_257[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_253 (Conv1D)             (None, 155, 5)       8025        activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_257 (Dropout)           (None, 155, 5)       0           conv1d_253[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_212 (Concatenate)   (None, 155, 326)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "                                                                 dropout_250[0][0]                \n",
      "                                                                 dropout_251[0][0]                \n",
      "                                                                 dropout_252[0][0]                \n",
      "                                                                 dropout_253[0][0]                \n",
      "                                                                 dropout_254[0][0]                \n",
      "                                                                 dropout_255[0][0]                \n",
      "                                                                 dropout_256[0][0]                \n",
      "                                                                 dropout_257[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_258 (BatchN (None, 155, 326)     1304        concatenate_212[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, 155, 326)     0           batch_normalization_258[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_254 (Conv1D)             (None, 155, 5)       8150        activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_258 (Dropout)           (None, 155, 5)       0           conv1d_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_213 (Concatenate)   (None, 155, 331)     0           average_pooling1d_6[0][0]        \n",
      "                                                                 dropout_238[0][0]                \n",
      "                                                                 dropout_239[0][0]                \n",
      "                                                                 dropout_240[0][0]                \n",
      "                                                                 dropout_241[0][0]                \n",
      "                                                                 dropout_242[0][0]                \n",
      "                                                                 dropout_243[0][0]                \n",
      "                                                                 dropout_244[0][0]                \n",
      "                                                                 dropout_245[0][0]                \n",
      "                                                                 dropout_246[0][0]                \n",
      "                                                                 dropout_247[0][0]                \n",
      "                                                                 dropout_248[0][0]                \n",
      "                                                                 dropout_249[0][0]                \n",
      "                                                                 dropout_250[0][0]                \n",
      "                                                                 dropout_251[0][0]                \n",
      "                                                                 dropout_252[0][0]                \n",
      "                                                                 dropout_253[0][0]                \n",
      "                                                                 dropout_254[0][0]                \n",
      "                                                                 dropout_255[0][0]                \n",
      "                                                                 dropout_256[0][0]                \n",
      "                                                                 dropout_257[0][0]                \n",
      "                                                                 dropout_258[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_259 (BatchN (None, 155, 331)     1324        concatenate_213[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, 155, 331)     0           batch_normalization_259[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 51305)        0           activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 20)           1026100     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_259 (Dropout)           (None, 20)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2)            42          dropout_259[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,670,972\n",
      "Trainable params: 1,647,974\n",
      "Non-trainable params: 22,998\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 289s 232ms/step - loss: 1.2549 - acc: 0.8202 - val_loss: 0.7885 - val_acc: 0.5709\n",
      "6710/6710 [==============================] - 15s 2ms/step\n",
      "TN:30,FP:116,FN:4,TP:134,Macc:0.588246925964,F1:0.690716568641\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 270s 217ms/step - loss: 0.3860 - acc: 0.8425 - val_loss: 0.6605 - val_acc: 0.6431\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:60,FP:86,FN:16,TP:122,Macc:0.647508386847,F1:0.705196992455\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 271s 217ms/step - loss: 0.3369 - acc: 0.8498 - val_loss: 0.6832 - val_acc: 0.5686\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:37,FP:109,FN:6,TP:132,Macc:0.604973150663,F1:0.696564786665\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 272s 218ms/step - loss: 0.3085 - acc: 0.8723 - val_loss: 0.6874 - val_acc: 0.5933\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:36,FP:110,FN:1,TP:137,Macc:0.619664433972,F1:0.71168321191\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 272s 218ms/step - loss: 0.2940 - acc: 0.8805 - val_loss: 0.8093 - val_acc: 0.6672\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:119,FP:27,FN:68,TP:70,Macc:0.66115738406,F1:0.595739292542\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 273s 219ms/step - loss: 0.2829 - acc: 0.8852 - val_loss: 0.8075 - val_acc: 0.6301\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:142,FP:4,FN:118,TP:20,Macc:0.558765095564,F1:0.24691077034\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 273s 219ms/step - loss: 0.2758 - acc: 0.8891 - val_loss: 0.8137 - val_acc: 0.6247\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:139,FP:7,FN:113,TP:25,Macc:0.56660706432,F1:0.294114247374\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 273s 219ms/step - loss: 0.2724 - acc: 0.8899 - val_loss: 0.6546 - val_acc: 0.6692\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:112,FP:34,FN:72,TP:66,Macc:0.622692030659,F1:0.554616437733\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 273s 219ms/step - loss: 0.2660 - acc: 0.8931 - val_loss: 0.6176 - val_acc: 0.6924\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:67,FP:79,FN:12,TP:126,Macc:0.685973740248,F1:0.734688540323\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 273s 219ms/step - loss: 0.2623 - acc: 0.8940 - val_loss: 0.8522 - val_acc: 0.6443\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:123,FP:23,FN:75,TP:63,Macc:0.649493696346,F1:0.562494741199\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 273s 219ms/step - loss: 0.2595 - acc: 0.8958 - val_loss: 0.5705 - val_acc: 0.7072\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:116,FP:30,FN:65,TP:73,Macc:0.661752976582,F1:0.605803689378\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 273s 219ms/step - loss: 0.2537 - acc: 0.8983 - val_loss: 1.0817 - val_acc: 0.5912\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:145,FP:1,FN:121,TP:17,Macc:0.558169503041,F1:0.217946441672\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 273s 219ms/step - loss: 0.2513 - acc: 0.8993 - val_loss: 1.4179 - val_acc: 0.5940\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:139,FP:7,FN:114,TP:24,Macc:0.562983876203,F1:0.28402033644\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 273s 219ms/step - loss: 0.2469 - acc: 0.9014 - val_loss: 0.8079 - val_acc: 0.6548\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:131,FP:15,FN:91,TP:47,Macc:0.618919944684,F1:0.469995242547\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 277s 222ms/step - loss: 0.2462 - acc: 0.9021 - val_loss: 0.5926 - val_acc: 0.7216\n",
      "6710/6710 [==============================] - 10s 2ms/step\n",
      "TN:118,FP:28,FN:54,TP:84,Macc:0.708457360422,F1:0.671994500396\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 275s 220ms/step - loss: 0.2421 - acc: 0.9028 - val_loss: 0.7048 - val_acc: 0.6382\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:49,FP:97,FN:6,TP:132,Macc:0.646069037978,F1:0.719340844115\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 274s 219ms/step - loss: 0.2403 - acc: 0.9047 - val_loss: 0.7738 - val_acc: 0.6663\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:128,FP:18,FN:94,TP:44,Macc:0.597776408504,F1:0.43999524585\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 277s 222ms/step - loss: 0.2386 - acc: 0.9047 - val_loss: 0.9547 - val_acc: 0.6335\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:140,FP:6,FN:107,TP:31,Macc:0.591770850298,F1:0.354282001801\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 277s 222ms/step - loss: 0.2376 - acc: 0.9061 - val_loss: 0.7494 - val_acc: 0.6380\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:131,FP:15,FN:101,TP:37,Macc:0.582688063514,F1:0.389469265977\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 353s 283ms/step - loss: 0.2384 - acc: 0.9042 - val_loss: 0.6280 - val_acc: 0.6893\n",
      "6710/6710 [==============================] - 10s 1ms/step\n",
      "TN:69,FP:77,FN:17,TP:121,Macc:0.674707114216,F1:0.720232723502\n",
      "Loss: 0\n",
      "args (6.0, 3, 8.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_21 (Con (None, 2500, 1)      31          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_22 (Con (None, 2500, 1)      31          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_23 (Con (None, 2500, 1)      31          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_24 (Con (None, 2500, 1)      31          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_255 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_21[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_257 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_22[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_259 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_23[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_261 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_24[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_260 (BatchN (None, 2496, 8)      32          conv1d_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_262 (BatchN (None, 2496, 8)      32          conv1d_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_264 (BatchN (None, 2496, 8)      32          conv1d_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_266 (BatchN (None, 2496, 8)      32          conv1d_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, 2496, 8)      0           batch_normalization_260[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, 2496, 8)      0           batch_normalization_262[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, 2496, 8)      0           batch_normalization_264[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_266 (Activation)     (None, 2496, 8)      0           batch_normalization_266[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_260 (Dropout)           (None, 2496, 8)      0           activation_260[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_262 (Dropout)           (None, 2496, 8)      0           activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_264 (Dropout)           (None, 2496, 8)      0           activation_264[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_266 (Dropout)           (None, 2496, 8)      0           activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1248, 8)      0           dropout_260[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 1248, 8)      0           dropout_262[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 1248, 8)      0           dropout_264[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 1248, 8)      0           dropout_266[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_256 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_258 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_260 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_262 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_261 (BatchN (None, 1244, 4)      16          conv1d_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_263 (BatchN (None, 1244, 4)      16          conv1d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_265 (BatchN (None, 1244, 4)      16          conv1d_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_267 (BatchN (None, 1244, 4)      16          conv1d_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, 1244, 4)      0           batch_normalization_261[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, 1244, 4)      0           batch_normalization_263[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, 1244, 4)      0           batch_normalization_265[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_267 (Activation)     (None, 1244, 4)      0           batch_normalization_267[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_261 (Dropout)           (None, 1244, 4)      0           activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_263 (Dropout)           (None, 1244, 4)      0           activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_265 (Dropout)           (None, 1244, 4)      0           activation_265[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_267 (Dropout)           (None, 1244, 4)      0           activation_267[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 622, 4)       0           dropout_261[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 622, 4)       0           dropout_263[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 622, 4)       0           dropout_265[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 622, 4)       0           dropout_267[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_214 (Concatenate)   (None, 622, 16)      0           max_pooling1d_42[0][0]           \n",
      "                                                                 max_pooling1d_44[0][0]           \n",
      "                                                                 max_pooling1d_46[0][0]           \n",
      "                                                                 max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_268 (BatchN (None, 622, 16)      64          concatenate_214[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, 622, 16)      0           batch_normalization_268[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_263 (Conv1D)             (None, 622, 8)       640         activation_268[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_268 (Dropout)           (None, 622, 8)       0           conv1d_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_215 (Concatenate)   (None, 622, 24)      0           concatenate_214[0][0]            \n",
      "                                                                 dropout_268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_269 (BatchN (None, 622, 24)      96          concatenate_215[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, 622, 24)      0           batch_normalization_269[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_264 (Conv1D)             (None, 622, 8)       960         activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_269 (Dropout)           (None, 622, 8)       0           conv1d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_216 (Concatenate)   (None, 622, 32)      0           concatenate_214[0][0]            \n",
      "                                                                 dropout_268[0][0]                \n",
      "                                                                 dropout_269[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_270 (BatchN (None, 622, 32)      128         concatenate_216[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_270 (Activation)     (None, 622, 32)      0           batch_normalization_270[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_265 (Conv1D)             (None, 622, 8)       1280        activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_270 (Dropout)           (None, 622, 8)       0           conv1d_265[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_217 (Concatenate)   (None, 622, 40)      0           concatenate_214[0][0]            \n",
      "                                                                 dropout_268[0][0]                \n",
      "                                                                 dropout_269[0][0]                \n",
      "                                                                 dropout_270[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_271 (BatchN (None, 622, 40)      160         concatenate_217[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_271 (Activation)     (None, 622, 40)      0           batch_normalization_271[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_266 (Conv1D)             (None, 622, 8)       1600        activation_271[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_271 (Dropout)           (None, 622, 8)       0           conv1d_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_218 (Concatenate)   (None, 622, 48)      0           concatenate_214[0][0]            \n",
      "                                                                 dropout_268[0][0]                \n",
      "                                                                 dropout_269[0][0]                \n",
      "                                                                 dropout_270[0][0]                \n",
      "                                                                 dropout_271[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_272 (BatchN (None, 622, 48)      192         concatenate_218[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, 622, 48)      0           batch_normalization_272[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_267 (Conv1D)             (None, 622, 8)       1920        activation_272[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_272 (Dropout)           (None, 622, 8)       0           conv1d_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_219 (Concatenate)   (None, 622, 56)      0           concatenate_214[0][0]            \n",
      "                                                                 dropout_268[0][0]                \n",
      "                                                                 dropout_269[0][0]                \n",
      "                                                                 dropout_270[0][0]                \n",
      "                                                                 dropout_271[0][0]                \n",
      "                                                                 dropout_272[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_273 (BatchN (None, 622, 56)      224         concatenate_219[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, 622, 56)      0           batch_normalization_273[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_268 (Conv1D)             (None, 622, 8)       2240        activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_273 (Dropout)           (None, 622, 8)       0           conv1d_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_220 (Concatenate)   (None, 622, 64)      0           concatenate_214[0][0]            \n",
      "                                                                 dropout_268[0][0]                \n",
      "                                                                 dropout_269[0][0]                \n",
      "                                                                 dropout_270[0][0]                \n",
      "                                                                 dropout_271[0][0]                \n",
      "                                                                 dropout_272[0][0]                \n",
      "                                                                 dropout_273[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_274 (BatchN (None, 622, 64)      256         concatenate_220[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_274 (Activation)     (None, 622, 64)      0           batch_normalization_274[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_269 (Conv1D)             (None, 622, 64)      20480       activation_274[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_274 (Dropout)           (None, 622, 64)      0           conv1d_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_7 (AveragePoo (None, 311, 64)      0           dropout_274[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_275 (BatchN (None, 311, 64)      256         average_pooling1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_275 (Activation)     (None, 311, 64)      0           batch_normalization_275[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_270 (Conv1D)             (None, 311, 8)       2560        activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_275 (Dropout)           (None, 311, 8)       0           conv1d_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_221 (Concatenate)   (None, 311, 72)      0           average_pooling1d_7[0][0]        \n",
      "                                                                 dropout_275[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchN (None, 311, 72)      288         concatenate_221[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, 311, 72)      0           batch_normalization_276[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_271 (Conv1D)             (None, 311, 8)       2880        activation_276[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_276 (Dropout)           (None, 311, 8)       0           conv1d_271[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_222 (Concatenate)   (None, 311, 80)      0           average_pooling1d_7[0][0]        \n",
      "                                                                 dropout_275[0][0]                \n",
      "                                                                 dropout_276[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_277 (BatchN (None, 311, 80)      320         concatenate_222[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, 311, 80)      0           batch_normalization_277[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_272 (Conv1D)             (None, 311, 8)       3200        activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_277 (Dropout)           (None, 311, 8)       0           conv1d_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_223 (Concatenate)   (None, 311, 88)      0           average_pooling1d_7[0][0]        \n",
      "                                                                 dropout_275[0][0]                \n",
      "                                                                 dropout_276[0][0]                \n",
      "                                                                 dropout_277[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_278 (BatchN (None, 311, 88)      352         concatenate_223[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_278 (Activation)     (None, 311, 88)      0           batch_normalization_278[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_273 (Conv1D)             (None, 311, 8)       3520        activation_278[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_278 (Dropout)           (None, 311, 8)       0           conv1d_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_224 (Concatenate)   (None, 311, 96)      0           average_pooling1d_7[0][0]        \n",
      "                                                                 dropout_275[0][0]                \n",
      "                                                                 dropout_276[0][0]                \n",
      "                                                                 dropout_277[0][0]                \n",
      "                                                                 dropout_278[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_279 (BatchN (None, 311, 96)      384         concatenate_224[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_279 (Activation)     (None, 311, 96)      0           batch_normalization_279[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_274 (Conv1D)             (None, 311, 8)       3840        activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_279 (Dropout)           (None, 311, 8)       0           conv1d_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_225 (Concatenate)   (None, 311, 104)     0           average_pooling1d_7[0][0]        \n",
      "                                                                 dropout_275[0][0]                \n",
      "                                                                 dropout_276[0][0]                \n",
      "                                                                 dropout_277[0][0]                \n",
      "                                                                 dropout_278[0][0]                \n",
      "                                                                 dropout_279[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchN (None, 311, 104)     416         concatenate_225[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, 311, 104)     0           batch_normalization_280[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_275 (Conv1D)             (None, 311, 8)       4160        activation_280[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_280 (Dropout)           (None, 311, 8)       0           conv1d_275[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_226 (Concatenate)   (None, 311, 112)     0           average_pooling1d_7[0][0]        \n",
      "                                                                 dropout_275[0][0]                \n",
      "                                                                 dropout_276[0][0]                \n",
      "                                                                 dropout_277[0][0]                \n",
      "                                                                 dropout_278[0][0]                \n",
      "                                                                 dropout_279[0][0]                \n",
      "                                                                 dropout_280[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchN (None, 311, 112)     448         concatenate_226[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, 311, 112)     0           batch_normalization_281[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_276 (Conv1D)             (None, 311, 112)     62720       activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_281 (Dropout)           (None, 311, 112)     0           conv1d_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_8 (AveragePoo (None, 155, 112)     0           dropout_281[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 155, 112)     448         average_pooling1d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, 155, 112)     0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_277 (Conv1D)             (None, 155, 8)       4480        activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_282 (Dropout)           (None, 155, 8)       0           conv1d_277[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_227 (Concatenate)   (None, 155, 120)     0           average_pooling1d_8[0][0]        \n",
      "                                                                 dropout_282[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 155, 120)     480         concatenate_227[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, 155, 120)     0           batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_278 (Conv1D)             (None, 155, 8)       4800        activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_283 (Dropout)           (None, 155, 8)       0           conv1d_278[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_228 (Concatenate)   (None, 155, 128)     0           average_pooling1d_8[0][0]        \n",
      "                                                                 dropout_282[0][0]                \n",
      "                                                                 dropout_283[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 155, 128)     512         concatenate_228[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, 155, 128)     0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_279 (Conv1D)             (None, 155, 8)       5120        activation_284[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_284 (Dropout)           (None, 155, 8)       0           conv1d_279[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_229 (Concatenate)   (None, 155, 136)     0           average_pooling1d_8[0][0]        \n",
      "                                                                 dropout_282[0][0]                \n",
      "                                                                 dropout_283[0][0]                \n",
      "                                                                 dropout_284[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 155, 136)     544         concatenate_229[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, 155, 136)     0           batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_280 (Conv1D)             (None, 155, 8)       5440        activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_285 (Dropout)           (None, 155, 8)       0           conv1d_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_230 (Concatenate)   (None, 155, 144)     0           average_pooling1d_8[0][0]        \n",
      "                                                                 dropout_282[0][0]                \n",
      "                                                                 dropout_283[0][0]                \n",
      "                                                                 dropout_284[0][0]                \n",
      "                                                                 dropout_285[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 155, 144)     576         concatenate_230[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_286 (Activation)     (None, 155, 144)     0           batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_281 (Conv1D)             (None, 155, 8)       5760        activation_286[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_286 (Dropout)           (None, 155, 8)       0           conv1d_281[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_231 (Concatenate)   (None, 155, 152)     0           average_pooling1d_8[0][0]        \n",
      "                                                                 dropout_282[0][0]                \n",
      "                                                                 dropout_283[0][0]                \n",
      "                                                                 dropout_284[0][0]                \n",
      "                                                                 dropout_285[0][0]                \n",
      "                                                                 dropout_286[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 155, 152)     608         concatenate_231[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_287 (Activation)     (None, 155, 152)     0           batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_282 (Conv1D)             (None, 155, 8)       6080        activation_287[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_287 (Dropout)           (None, 155, 8)       0           conv1d_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_232 (Concatenate)   (None, 155, 160)     0           average_pooling1d_8[0][0]        \n",
      "                                                                 dropout_282[0][0]                \n",
      "                                                                 dropout_283[0][0]                \n",
      "                                                                 dropout_284[0][0]                \n",
      "                                                                 dropout_285[0][0]                \n",
      "                                                                 dropout_286[0][0]                \n",
      "                                                                 dropout_287[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 155, 160)     640         concatenate_232[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, 155, 160)     0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 24800)        0           activation_288[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 20)           496000      flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_288 (Dropout)           (None, 20)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2)            42          dropout_288[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 648,230\n",
      "Trainable params: 644,438\n",
      "Non-trainable params: 3,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 80s 64ms/step - loss: 0.9109 - acc: 0.8255 - val_loss: 0.7393 - val_acc: 0.6526\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:66,FP:80,FN:19,TP:119,Macc:0.657186766153,F1:0.706226088145\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 75s 60ms/step - loss: 0.3719 - acc: 0.8506 - val_loss: 0.6641 - val_acc: 0.6320\n",
      "6710/6710 [==============================] - 3s 444us/step\n",
      "TN:52,FP:94,FN:10,TP:128,Macc:0.641850257339,F1:0.711105867137\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 74s 59ms/step - loss: 0.3249 - acc: 0.8696 - val_loss: 0.7271 - val_acc: 0.5550\n",
      "6710/6710 [==============================] - 3s 435us/step\n",
      "TN:28,FP:118,FN:2,TP:136,Macc:0.588643987645,F1:0.693872493737\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 76s 61ms/step - loss: 0.3026 - acc: 0.8797 - val_loss: 0.7210 - val_acc: 0.5943\n",
      "6710/6710 [==============================] - 3s 427us/step\n",
      "TN:54,FP:92,FN:25,TP:113,Macc:0.594351750136,F1:0.658886795918\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 75s 60ms/step - loss: 0.2918 - acc: 0.8830 - val_loss: 0.7501 - val_acc: 0.5638\n",
      "6710/6710 [==============================] - 3s 426us/step\n",
      "TN:30,FP:116,FN:3,TP:135,Macc:0.591870114081,F1:0.694082328491\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 75s 60ms/step - loss: 0.2790 - acc: 0.8880 - val_loss: 0.6609 - val_acc: 0.6592\n",
      "6710/6710 [==============================] - 3s 438us/step\n",
      "TN:71,FP:75,FN:34,TP:104,Macc:0.619962230779,F1:0.656145966071\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 74s 59ms/step - loss: 0.2739 - acc: 0.8893 - val_loss: 0.7576 - val_acc: 0.7259\n",
      "6710/6710 [==============================] - 3s 415us/step\n",
      "TN:121,FP:25,FN:71,TP:67,Macc:0.657137134262,F1:0.582603359972\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 75s 60ms/step - loss: 0.2693 - acc: 0.8916 - val_loss: 0.7053 - val_acc: 0.6414\n",
      "6710/6710 [==============================] - 3s 449us/step\n",
      "TN:61,FP:85,FN:19,TP:119,Macc:0.640063479772,F1:0.695901092855\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 74s 59ms/step - loss: 0.2615 - acc: 0.8937 - val_loss: 0.6935 - val_acc: 0.6680\n",
      "6710/6710 [==============================] - 3s 421us/step\n",
      "TN:69,FP:77,FN:23,TP:115,Macc:0.652967985514,F1:0.696964297819\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 75s 60ms/step - loss: 0.2527 - acc: 0.8978 - val_loss: 1.9985 - val_acc: 0.5933\n",
      "6710/6710 [==============================] - 3s 421us/step\n",
      "TN:146,FP:0,FN:129,TP:9,Macc:0.532608655382,F1:0.122447696805\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 74s 60ms/step - loss: 0.2498 - acc: 0.8996 - val_loss: 0.6772 - val_acc: 0.6632\n",
      "6710/6710 [==============================] - 3s 417us/step\n",
      "TN:53,FP:93,FN:2,TP:136,Macc:0.674260419551,F1:0.74113920793\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 74s 59ms/step - loss: 0.2457 - acc: 0.9008 - val_loss: 0.6801 - val_acc: 0.6714\n",
      "6710/6710 [==============================] - 3s 423us/step\n",
      "TN:56,FP:90,FN:6,TP:132,Macc:0.670041638912,F1:0.733328088\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 74s 59ms/step - loss: 0.2411 - acc: 0.9030 - val_loss: 0.6819 - val_acc: 0.7073\n",
      "6710/6710 [==============================] - 3s 411us/step\n",
      "TN:118,FP:28,FN:66,TP:72,Macc:0.664979103018,F1:0.605036601135\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 74s 59ms/step - loss: 0.2377 - acc: 0.9039 - val_loss: 2.1126 - val_acc: 0.5934\n",
      "6710/6710 [==============================] - 3s 437us/step\n",
      "TN:145,FP:1,FN:122,TP:16,Macc:0.554546314924,F1:0.20644943536\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 73s 59ms/step - loss: 0.2349 - acc: 0.9067 - val_loss: 0.7014 - val_acc: 0.6703\n",
      "6710/6710 [==============================] - 3s 437us/step\n",
      "TN:57,FP:89,FN:7,TP:131,Macc:0.669843108071,F1:0.731838319035\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 75s 60ms/step - loss: 0.2329 - acc: 0.9060 - val_loss: 0.6302 - val_acc: 0.7116\n",
      "6710/6710 [==============================] - 3s 430us/step\n",
      "TN:78,FP:68,FN:24,TP:114,Macc:0.680166712883,F1:0.712494555041\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 75s 60ms/step - loss: 0.2303 - acc: 0.9068 - val_loss: 0.7247 - val_acc: 0.6973\n",
      "6710/6710 [==============================] - 3s 407us/step\n",
      "TN:90,FP:56,FN:48,TP:90,Macc:0.63430608539,F1:0.633797272216\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 74s 59ms/step - loss: 0.2272 - acc: 0.9076 - val_loss: 0.7367 - val_acc: 0.6909\n",
      "6710/6710 [==============================] - 3s 440us/step\n",
      "TN:95,FP:51,FN:48,TP:90,Macc:0.651429371771,F1:0.645155740132\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 74s 59ms/step - loss: 0.2240 - acc: 0.9107 - val_loss: 0.6729 - val_acc: 0.7076\n",
      "6710/6710 [==============================] - 3s 419us/step\n",
      "TN:74,FP:72,FN:25,TP:113,Macc:0.662844895661,F1:0.699684971315\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 73s 59ms/step - loss: 0.2231 - acc: 0.9103 - val_loss: 0.7546 - val_acc: 0.6917\n",
      "6710/6710 [==============================] - 3s 413us/step\n",
      "TN:89,FP:57,FN:45,TP:93,Macc:0.641750992464,F1:0.645827793594\n",
      "Loss: 0\n",
      "args (14.0, 2, 1.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_25 (Con (None, 2500, 1)      31          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_26 (Con (None, 2500, 1)      31          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_27 (Con (None, 2500, 1)      31          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_28 (Con (None, 2500, 1)      31          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_283 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_25[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_285 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_26[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_287 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_27[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_289 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_28[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchN (None, 2496, 8)      32          conv1d_283[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchN (None, 2496, 8)      32          conv1d_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 2496, 8)      32          conv1d_287[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, 2496, 8)      32          conv1d_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, 2496, 8)      0           batch_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_291 (Activation)     (None, 2496, 8)      0           batch_normalization_291[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, 2496, 8)      0           batch_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_295 (Activation)     (None, 2496, 8)      0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_289 (Dropout)           (None, 2496, 8)      0           activation_289[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_291 (Dropout)           (None, 2496, 8)      0           activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_293 (Dropout)           (None, 2496, 8)      0           activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_295 (Dropout)           (None, 2496, 8)      0           activation_295[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 1248, 8)      0           dropout_289[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 1248, 8)      0           dropout_291[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 1248, 8)      0           dropout_293[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 1248, 8)      0           dropout_295[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_284 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_286 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_288 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_290 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchN (None, 1244, 4)      16          conv1d_284[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 1244, 4)      16          conv1d_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, 1244, 4)      16          conv1d_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, 1244, 4)      16          conv1d_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_290 (Activation)     (None, 1244, 4)      0           batch_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, 1244, 4)      0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, 1244, 4)      0           batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, 1244, 4)      0           batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_290 (Dropout)           (None, 1244, 4)      0           activation_290[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_292 (Dropout)           (None, 1244, 4)      0           activation_292[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_294 (Dropout)           (None, 1244, 4)      0           activation_294[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_296 (Dropout)           (None, 1244, 4)      0           activation_296[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 622, 4)       0           dropout_290[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 622, 4)       0           dropout_292[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 622, 4)       0           dropout_294[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 622, 4)       0           dropout_296[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_233 (Concatenate)   (None, 622, 16)      0           max_pooling1d_50[0][0]           \n",
      "                                                                 max_pooling1d_52[0][0]           \n",
      "                                                                 max_pooling1d_54[0][0]           \n",
      "                                                                 max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_297 (BatchN (None, 622, 16)      64          concatenate_233[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, 622, 16)      0           batch_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_291 (Conv1D)             (None, 622, 1)       80          activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_297 (Dropout)           (None, 622, 1)       0           conv1d_291[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_234 (Concatenate)   (None, 622, 17)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, 622, 17)      68          concatenate_234[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_298 (Activation)     (None, 622, 17)      0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_292 (Conv1D)             (None, 622, 1)       85          activation_298[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_298 (Dropout)           (None, 622, 1)       0           conv1d_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_235 (Concatenate)   (None, 622, 18)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_299 (BatchN (None, 622, 18)      72          concatenate_235[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 622, 18)      0           batch_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_293 (Conv1D)             (None, 622, 1)       90          activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_299 (Dropout)           (None, 622, 1)       0           conv1d_293[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_236 (Concatenate)   (None, 622, 19)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, 622, 19)      76          concatenate_236[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 622, 19)      0           batch_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_294 (Conv1D)             (None, 622, 1)       95          activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_300 (Dropout)           (None, 622, 1)       0           conv1d_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_237 (Concatenate)   (None, 622, 20)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_301 (BatchN (None, 622, 20)      80          concatenate_237[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 622, 20)      0           batch_normalization_301[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_295 (Conv1D)             (None, 622, 1)       100         activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_301 (Dropout)           (None, 622, 1)       0           conv1d_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_238 (Concatenate)   (None, 622, 21)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_302 (BatchN (None, 622, 21)      84          concatenate_238[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 622, 21)      0           batch_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_296 (Conv1D)             (None, 622, 1)       105         activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_302 (Dropout)           (None, 622, 1)       0           conv1d_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_239 (Concatenate)   (None, 622, 22)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "                                                                 dropout_302[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_303 (BatchN (None, 622, 22)      88          concatenate_239[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 622, 22)      0           batch_normalization_303[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_297 (Conv1D)             (None, 622, 1)       110         activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_303 (Dropout)           (None, 622, 1)       0           conv1d_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_240 (Concatenate)   (None, 622, 23)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "                                                                 dropout_302[0][0]                \n",
      "                                                                 dropout_303[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_304 (BatchN (None, 622, 23)      92          concatenate_240[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 622, 23)      0           batch_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_298 (Conv1D)             (None, 622, 1)       115         activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_304 (Dropout)           (None, 622, 1)       0           conv1d_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_241 (Concatenate)   (None, 622, 24)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "                                                                 dropout_302[0][0]                \n",
      "                                                                 dropout_303[0][0]                \n",
      "                                                                 dropout_304[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, 622, 24)      96          concatenate_241[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 622, 24)      0           batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_299 (Conv1D)             (None, 622, 1)       120         activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_305 (Dropout)           (None, 622, 1)       0           conv1d_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_242 (Concatenate)   (None, 622, 25)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "                                                                 dropout_302[0][0]                \n",
      "                                                                 dropout_303[0][0]                \n",
      "                                                                 dropout_304[0][0]                \n",
      "                                                                 dropout_305[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 622, 25)      100         concatenate_242[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 622, 25)      0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_300 (Conv1D)             (None, 622, 1)       125         activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_306 (Dropout)           (None, 622, 1)       0           conv1d_300[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_243 (Concatenate)   (None, 622, 26)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "                                                                 dropout_302[0][0]                \n",
      "                                                                 dropout_303[0][0]                \n",
      "                                                                 dropout_304[0][0]                \n",
      "                                                                 dropout_305[0][0]                \n",
      "                                                                 dropout_306[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 622, 26)      104         concatenate_243[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 622, 26)      0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_301 (Conv1D)             (None, 622, 1)       130         activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_307 (Dropout)           (None, 622, 1)       0           conv1d_301[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_244 (Concatenate)   (None, 622, 27)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "                                                                 dropout_302[0][0]                \n",
      "                                                                 dropout_303[0][0]                \n",
      "                                                                 dropout_304[0][0]                \n",
      "                                                                 dropout_305[0][0]                \n",
      "                                                                 dropout_306[0][0]                \n",
      "                                                                 dropout_307[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 622, 27)      108         concatenate_244[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 622, 27)      0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_302 (Conv1D)             (None, 622, 1)       135         activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_308 (Dropout)           (None, 622, 1)       0           conv1d_302[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_245 (Concatenate)   (None, 622, 28)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "                                                                 dropout_302[0][0]                \n",
      "                                                                 dropout_303[0][0]                \n",
      "                                                                 dropout_304[0][0]                \n",
      "                                                                 dropout_305[0][0]                \n",
      "                                                                 dropout_306[0][0]                \n",
      "                                                                 dropout_307[0][0]                \n",
      "                                                                 dropout_308[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 622, 28)      112         concatenate_245[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 622, 28)      0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_303 (Conv1D)             (None, 622, 1)       140         activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_309 (Dropout)           (None, 622, 1)       0           conv1d_303[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_246 (Concatenate)   (None, 622, 29)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "                                                                 dropout_302[0][0]                \n",
      "                                                                 dropout_303[0][0]                \n",
      "                                                                 dropout_304[0][0]                \n",
      "                                                                 dropout_305[0][0]                \n",
      "                                                                 dropout_306[0][0]                \n",
      "                                                                 dropout_307[0][0]                \n",
      "                                                                 dropout_308[0][0]                \n",
      "                                                                 dropout_309[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 622, 29)      116         concatenate_246[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 622, 29)      0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_304 (Conv1D)             (None, 622, 1)       145         activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_310 (Dropout)           (None, 622, 1)       0           conv1d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_247 (Concatenate)   (None, 622, 30)      0           concatenate_233[0][0]            \n",
      "                                                                 dropout_297[0][0]                \n",
      "                                                                 dropout_298[0][0]                \n",
      "                                                                 dropout_299[0][0]                \n",
      "                                                                 dropout_300[0][0]                \n",
      "                                                                 dropout_301[0][0]                \n",
      "                                                                 dropout_302[0][0]                \n",
      "                                                                 dropout_303[0][0]                \n",
      "                                                                 dropout_304[0][0]                \n",
      "                                                                 dropout_305[0][0]                \n",
      "                                                                 dropout_306[0][0]                \n",
      "                                                                 dropout_307[0][0]                \n",
      "                                                                 dropout_308[0][0]                \n",
      "                                                                 dropout_309[0][0]                \n",
      "                                                                 dropout_310[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 622, 30)      120         concatenate_247[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 622, 30)      0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_305 (Conv1D)             (None, 622, 30)      4500        activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_311 (Dropout)           (None, 622, 30)      0           conv1d_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_9 (AveragePoo (None, 311, 30)      0           dropout_311[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 311, 30)      120         average_pooling1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 311, 30)      0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_306 (Conv1D)             (None, 311, 1)       150         activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_312 (Dropout)           (None, 311, 1)       0           conv1d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_248 (Concatenate)   (None, 311, 31)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 311, 31)      124         concatenate_248[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 311, 31)      0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_307 (Conv1D)             (None, 311, 1)       155         activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_313 (Dropout)           (None, 311, 1)       0           conv1d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_249 (Concatenate)   (None, 311, 32)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 311, 32)      128         concatenate_249[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 311, 32)      0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_308 (Conv1D)             (None, 311, 1)       160         activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_314 (Dropout)           (None, 311, 1)       0           conv1d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_250 (Concatenate)   (None, 311, 33)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 311, 33)      132         concatenate_250[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 311, 33)      0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_309 (Conv1D)             (None, 311, 1)       165         activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_315 (Dropout)           (None, 311, 1)       0           conv1d_309[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_251 (Concatenate)   (None, 311, 34)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 311, 34)      136         concatenate_251[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 311, 34)      0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_310 (Conv1D)             (None, 311, 1)       170         activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_316 (Dropout)           (None, 311, 1)       0           conv1d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_252 (Concatenate)   (None, 311, 35)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 311, 35)      140         concatenate_252[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 311, 35)      0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_311 (Conv1D)             (None, 311, 1)       175         activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_317 (Dropout)           (None, 311, 1)       0           conv1d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_253 (Concatenate)   (None, 311, 36)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "                                                                 dropout_317[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 311, 36)      144         concatenate_253[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 311, 36)      0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_312 (Conv1D)             (None, 311, 1)       180         activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_318 (Dropout)           (None, 311, 1)       0           conv1d_312[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_254 (Concatenate)   (None, 311, 37)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "                                                                 dropout_317[0][0]                \n",
      "                                                                 dropout_318[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 311, 37)      148         concatenate_254[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 311, 37)      0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_313 (Conv1D)             (None, 311, 1)       185         activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_319 (Dropout)           (None, 311, 1)       0           conv1d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_255 (Concatenate)   (None, 311, 38)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "                                                                 dropout_317[0][0]                \n",
      "                                                                 dropout_318[0][0]                \n",
      "                                                                 dropout_319[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 311, 38)      152         concatenate_255[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 311, 38)      0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_314 (Conv1D)             (None, 311, 1)       190         activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_320 (Dropout)           (None, 311, 1)       0           conv1d_314[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_256 (Concatenate)   (None, 311, 39)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "                                                                 dropout_317[0][0]                \n",
      "                                                                 dropout_318[0][0]                \n",
      "                                                                 dropout_319[0][0]                \n",
      "                                                                 dropout_320[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 311, 39)      156         concatenate_256[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 311, 39)      0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_315 (Conv1D)             (None, 311, 1)       195         activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_321 (Dropout)           (None, 311, 1)       0           conv1d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_257 (Concatenate)   (None, 311, 40)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "                                                                 dropout_317[0][0]                \n",
      "                                                                 dropout_318[0][0]                \n",
      "                                                                 dropout_319[0][0]                \n",
      "                                                                 dropout_320[0][0]                \n",
      "                                                                 dropout_321[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 311, 40)      160         concatenate_257[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 311, 40)      0           batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_316 (Conv1D)             (None, 311, 1)       200         activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_322 (Dropout)           (None, 311, 1)       0           conv1d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_258 (Concatenate)   (None, 311, 41)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "                                                                 dropout_317[0][0]                \n",
      "                                                                 dropout_318[0][0]                \n",
      "                                                                 dropout_319[0][0]                \n",
      "                                                                 dropout_320[0][0]                \n",
      "                                                                 dropout_321[0][0]                \n",
      "                                                                 dropout_322[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 311, 41)      164         concatenate_258[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 311, 41)      0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_317 (Conv1D)             (None, 311, 1)       205         activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_323 (Dropout)           (None, 311, 1)       0           conv1d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_259 (Concatenate)   (None, 311, 42)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "                                                                 dropout_317[0][0]                \n",
      "                                                                 dropout_318[0][0]                \n",
      "                                                                 dropout_319[0][0]                \n",
      "                                                                 dropout_320[0][0]                \n",
      "                                                                 dropout_321[0][0]                \n",
      "                                                                 dropout_322[0][0]                \n",
      "                                                                 dropout_323[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 311, 42)      168         concatenate_259[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 311, 42)      0           batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_318 (Conv1D)             (None, 311, 1)       210         activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_324 (Dropout)           (None, 311, 1)       0           conv1d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_260 (Concatenate)   (None, 311, 43)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "                                                                 dropout_317[0][0]                \n",
      "                                                                 dropout_318[0][0]                \n",
      "                                                                 dropout_319[0][0]                \n",
      "                                                                 dropout_320[0][0]                \n",
      "                                                                 dropout_321[0][0]                \n",
      "                                                                 dropout_322[0][0]                \n",
      "                                                                 dropout_323[0][0]                \n",
      "                                                                 dropout_324[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 311, 43)      172         concatenate_260[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 311, 43)      0           batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_319 (Conv1D)             (None, 311, 1)       215         activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_325 (Dropout)           (None, 311, 1)       0           conv1d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_261 (Concatenate)   (None, 311, 44)      0           average_pooling1d_9[0][0]        \n",
      "                                                                 dropout_312[0][0]                \n",
      "                                                                 dropout_313[0][0]                \n",
      "                                                                 dropout_314[0][0]                \n",
      "                                                                 dropout_315[0][0]                \n",
      "                                                                 dropout_316[0][0]                \n",
      "                                                                 dropout_317[0][0]                \n",
      "                                                                 dropout_318[0][0]                \n",
      "                                                                 dropout_319[0][0]                \n",
      "                                                                 dropout_320[0][0]                \n",
      "                                                                 dropout_321[0][0]                \n",
      "                                                                 dropout_322[0][0]                \n",
      "                                                                 dropout_323[0][0]                \n",
      "                                                                 dropout_324[0][0]                \n",
      "                                                                 dropout_325[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_326 (BatchN (None, 311, 44)      176         concatenate_261[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 311, 44)      0           batch_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 13684)        0           activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 20)           273680      flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_326 (Dropout)           (None, 20)           0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 2)            42          dropout_326[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 287,068\n",
      "Trainable params: 285,172\n",
      "Non-trainable params: 1,896\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 87s 70ms/step - loss: 0.8234 - acc: 0.8403 - val_loss: 1.2641 - val_acc: 0.5693\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:140,FP:6,FN:126,TP:12,Macc:0.522930276075,F1:0.15384388662\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 77s 62ms/step - loss: 0.3458 - acc: 0.8569 - val_loss: 0.6433 - val_acc: 0.6431\n",
      "6710/6710 [==============================] - 3s 397us/step\n",
      "TN:57,FP:89,FN:20,TP:118,Macc:0.62274166255,F1:0.684052647434\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 80s 64ms/step - loss: 0.3108 - acc: 0.8744 - val_loss: 0.6181 - val_acc: 0.6663\n",
      "6710/6710 [==============================] - 3s 413us/step\n",
      "TN:57,FP:89,FN:10,TP:128,Macc:0.65897354372,F1:0.721121488282\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 79s 63ms/step - loss: 0.2963 - acc: 0.8772 - val_loss: 0.6023 - val_acc: 0.6897\n",
      "6710/6710 [==============================] - 3s 399us/step\n",
      "TN:66,FP:80,FN:13,TP:125,Macc:0.678925894855,F1:0.728857636907\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 78s 63ms/step - loss: 0.2850 - acc: 0.8813 - val_loss: 0.6187 - val_acc: 0.7089\n",
      "6710/6710 [==============================] - 3s 397us/step\n",
      "TN:89,FP:57,FN:42,TP:96,Macc:0.652620556815,F1:0.659788279211\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 79s 64ms/step - loss: 0.2765 - acc: 0.8847 - val_loss: 0.6076 - val_acc: 0.6911\n",
      "6710/6710 [==============================] - 3s 388us/step\n",
      "TN:72,FP:74,FN:28,TP:110,Macc:0.645126016757,F1:0.683224379272\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 79s 64ms/step - loss: 0.2722 - acc: 0.8860 - val_loss: 0.5894 - val_acc: 0.6996\n",
      "6710/6710 [==============================] - 3s 418us/step\n",
      "TN:76,FP:70,FN:29,TP:109,Macc:0.655201457745,F1:0.687691705204\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 79s 64ms/step - loss: 0.2685 - acc: 0.8877 - val_loss: 0.5774 - val_acc: 0.7094\n",
      "6710/6710 [==============================] - 3s 396us/step\n",
      "TN:76,FP:70,FN:26,TP:112,Macc:0.666071022096,F1:0.699994555901\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 79s 63ms/step - loss: 0.2629 - acc: 0.8901 - val_loss: 0.5811 - val_acc: 0.7080\n",
      "6710/6710 [==============================] - 3s 401us/step\n",
      "TN:77,FP:69,FN:26,TP:112,Macc:0.669495679372,F1:0.702188908916\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 79s 63ms/step - loss: 0.2577 - acc: 0.8921 - val_loss: 0.5925 - val_acc: 0.6988\n",
      "6710/6710 [==============================] - 3s 395us/step\n",
      "TN:72,FP:74,FN:26,TP:112,Macc:0.652372392991,F1:0.691352598502\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 79s 63ms/step - loss: 0.2580 - acc: 0.8916 - val_loss: 0.6057 - val_acc: 0.6963\n",
      "6710/6710 [==============================] - 3s 410us/step\n",
      "TN:73,FP:73,FN:24,TP:114,Macc:0.663043426502,F1:0.701533039113\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 78s 63ms/step - loss: 0.2533 - acc: 0.8941 - val_loss: 0.6276 - val_acc: 0.6852\n",
      "6710/6710 [==============================] - 3s 391us/step\n",
      "TN:62,FP:84,FN:15,TP:123,Macc:0.657980889516,F1:0.713038152831\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 79s 63ms/step - loss: 0.2512 - acc: 0.8951 - val_loss: 0.5797 - val_acc: 0.7168\n",
      "6710/6710 [==============================] - 3s 386us/step\n",
      "TN:82,FP:64,FN:30,TP:108,Macc:0.672126213286,F1:0.696768710303\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 78s 63ms/step - loss: 0.2514 - acc: 0.8954 - val_loss: 0.6195 - val_acc: 0.6890\n",
      "6710/6710 [==============================] - 3s 393us/step\n",
      "TN:62,FP:84,FN:13,TP:125,Macc:0.66522726575,F1:0.720455779723\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 77s 62ms/step - loss: 0.2473 - acc: 0.8962 - val_loss: 0.5955 - val_acc: 0.7170\n",
      "6710/6710 [==============================] - 3s 400us/step\n",
      "TN:80,FP:66,FN:32,TP:106,Macc:0.658030522499,F1:0.683865485413\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 77s 62ms/step - loss: 0.2453 - acc: 0.8972 - val_loss: 0.6012 - val_acc: 0.6994\n",
      "6710/6710 [==============================] - 3s 395us/step\n",
      "TN:66,FP:80,FN:16,TP:122,Macc:0.668056330504,F1:0.717641707306\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 77s 62ms/step - loss: 0.2460 - acc: 0.8972 - val_loss: 0.5734 - val_acc: 0.7158\n",
      "6710/6710 [==============================] - 3s 408us/step\n",
      "TN:73,FP:73,FN:16,TP:122,Macc:0.692028931437,F1:0.73272734551\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 77s 62ms/step - loss: 0.2440 - acc: 0.8972 - val_loss: 0.5888 - val_acc: 0.7201\n",
      "6710/6710 [==============================] - 3s 387us/step\n",
      "TN:77,FP:69,FN:27,TP:111,Macc:0.665872491255,F1:0.698107755233\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 77s 62ms/step - loss: 0.2413 - acc: 0.8988 - val_loss: 0.5994 - val_acc: 0.7095\n",
      "6710/6710 [==============================] - 3s 398us/step\n",
      "TN:82,FP:64,FN:29,TP:109,Macc:0.675749401403,F1:0.70095915034\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 77s 62ms/step - loss: 0.2426 - acc: 0.8970 - val_loss: 0.5918 - val_acc: 0.7073\n",
      "6710/6710 [==============================] - 3s 392us/step\n",
      "TN:79,FP:67,FN:26,TP:112,Macc:0.676344993925,F1:0.706619148685\n",
      "Loss: 0\n",
      "args (5.0, 1, 15.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_29 (Con (None, 2500, 1)      31          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_30 (Con (None, 2500, 1)      31          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_31 (Con (None, 2500, 1)      31          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_32 (Con (None, 2500, 1)      31          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_320 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_29[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_322 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_30[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_324 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_31[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_326 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_32[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 2496, 8)      32          conv1d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 2496, 8)      32          conv1d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 2496, 8)      32          conv1d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 2496, 8)      32          conv1d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 2496, 8)      0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 2496, 8)      0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 2496, 8)      0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 2496, 8)      0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_327 (Dropout)           (None, 2496, 8)      0           activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_329 (Dropout)           (None, 2496, 8)      0           activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_331 (Dropout)           (None, 2496, 8)      0           activation_331[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_333 (Dropout)           (None, 2496, 8)      0           activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 1248, 8)      0           dropout_327[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1248, 8)      0           dropout_329[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 1248, 8)      0           dropout_331[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 1248, 8)      0           dropout_333[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_321 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_323 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_325 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_327 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 1244, 4)      16          conv1d_321[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 1244, 4)      16          conv1d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 1244, 4)      16          conv1d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 1244, 4)      16          conv1d_327[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 1244, 4)      0           batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 1244, 4)      0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 1244, 4)      0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 1244, 4)      0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_328 (Dropout)           (None, 1244, 4)      0           activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_330 (Dropout)           (None, 1244, 4)      0           activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_332 (Dropout)           (None, 1244, 4)      0           activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_334 (Dropout)           (None, 1244, 4)      0           activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 622, 4)       0           dropout_328[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 622, 4)       0           dropout_330[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 622, 4)       0           dropout_332[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 622, 4)       0           dropout_334[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_262 (Concatenate)   (None, 622, 16)      0           max_pooling1d_58[0][0]           \n",
      "                                                                 max_pooling1d_60[0][0]           \n",
      "                                                                 max_pooling1d_62[0][0]           \n",
      "                                                                 max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 622, 16)      64          concatenate_262[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 622, 16)      0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_328 (Conv1D)             (None, 622, 15)      1200        activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_335 (Dropout)           (None, 622, 15)      0           conv1d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_263 (Concatenate)   (None, 622, 31)      0           concatenate_262[0][0]            \n",
      "                                                                 dropout_335[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 622, 31)      124         concatenate_263[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 622, 31)      0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_329 (Conv1D)             (None, 622, 15)      2325        activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_336 (Dropout)           (None, 622, 15)      0           conv1d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_264 (Concatenate)   (None, 622, 46)      0           concatenate_262[0][0]            \n",
      "                                                                 dropout_335[0][0]                \n",
      "                                                                 dropout_336[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 622, 46)      184         concatenate_264[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 622, 46)      0           batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_330 (Conv1D)             (None, 622, 15)      3450        activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_337 (Dropout)           (None, 622, 15)      0           conv1d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_265 (Concatenate)   (None, 622, 61)      0           concatenate_262[0][0]            \n",
      "                                                                 dropout_335[0][0]                \n",
      "                                                                 dropout_336[0][0]                \n",
      "                                                                 dropout_337[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_338 (BatchN (None, 622, 61)      244         concatenate_265[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 622, 61)      0           batch_normalization_338[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_331 (Conv1D)             (None, 622, 15)      4575        activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_338 (Dropout)           (None, 622, 15)      0           conv1d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_266 (Concatenate)   (None, 622, 76)      0           concatenate_262[0][0]            \n",
      "                                                                 dropout_335[0][0]                \n",
      "                                                                 dropout_336[0][0]                \n",
      "                                                                 dropout_337[0][0]                \n",
      "                                                                 dropout_338[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 622, 76)      304         concatenate_266[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 622, 76)      0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_332 (Conv1D)             (None, 622, 15)      5700        activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_339 (Dropout)           (None, 622, 15)      0           conv1d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_267 (Concatenate)   (None, 622, 91)      0           concatenate_262[0][0]            \n",
      "                                                                 dropout_335[0][0]                \n",
      "                                                                 dropout_336[0][0]                \n",
      "                                                                 dropout_337[0][0]                \n",
      "                                                                 dropout_338[0][0]                \n",
      "                                                                 dropout_339[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 622, 91)      364         concatenate_267[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 622, 91)      0           batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 56602)        0           activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 20)           1132040     flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_340 (Dropout)           (None, 20)           0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            42          dropout_340[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,151,732\n",
      "Trainable params: 1,150,994\n",
      "Non-trainable params: 738\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 50s 40ms/step - loss: 1.0806 - acc: 0.8218 - val_loss: 0.7234 - val_acc: 0.6510\n",
      "6710/6710 [==============================] - 6s 930us/step\n",
      "TN:110,FP:36,FN:74,TP:64,Macc:0.608596339872,F1:0.537809716599\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 57s 46ms/step - loss: 0.3231 - acc: 0.8576 - val_loss: 0.6089 - val_acc: 0.6893\n",
      "6710/6710 [==============================] - 2s 277us/step\n",
      "TN:85,FP:61,FN:38,TP:100,Macc:0.653414680178,F1:0.668890804443\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.3064 - acc: 0.8636 - val_loss: 0.6046 - val_acc: 0.6715\n",
      "6710/6710 [==============================] - 2s 309us/step\n",
      "TN:81,FP:65,FN:39,TP:99,Macc:0.636092862956,F1:0.655623632123\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2964 - acc: 0.8686 - val_loss: 0.6170 - val_acc: 0.6772\n",
      "6710/6710 [==============================] - 2s 304us/step\n",
      "TN:90,FP:56,FN:46,TP:92,Macc:0.641552461623,F1:0.643351100639\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2927 - acc: 0.8700 - val_loss: 0.5992 - val_acc: 0.6821\n",
      "6710/6710 [==============================] - 2s 311us/step\n",
      "TN:80,FP:66,FN:34,TP:104,Macc:0.650784146265,F1:0.6753191865\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2857 - acc: 0.8733 - val_loss: 0.5677 - val_acc: 0.7176\n",
      "6710/6710 [==============================] - 2s 272us/step\n",
      "TN:78,FP:68,FN:27,TP:111,Macc:0.669297148532,F1:0.700310000858\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2837 - acc: 0.8750 - val_loss: 0.5943 - val_acc: 0.6917\n",
      "6710/6710 [==============================] - 2s 286us/step\n",
      "TN:71,FP:75,FN:27,TP:111,Macc:0.645324547598,F1:0.685179759416\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2806 - acc: 0.8767 - val_loss: 0.5799 - val_acc: 0.7004\n",
      "6710/6710 [==============================] - 2s 283us/step\n",
      "TN:67,FP:79,FN:17,TP:121,Macc:0.667857799663,F1:0.715970969858\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2734 - acc: 0.8799 - val_loss: 0.6529 - val_acc: 0.6362\n",
      "6710/6710 [==============================] - 2s 298us/step\n",
      "TN:86,FP:60,FN:47,TP:91,Macc:0.624230644402,F1:0.629752248704\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2699 - acc: 0.8817 - val_loss: 0.6629 - val_acc: 0.6311\n",
      "6710/6710 [==============================] - 2s 300us/step\n",
      "TN:87,FP:59,FN:39,TP:99,Macc:0.656640806614,F1:0.668913394356\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2686 - acc: 0.8839 - val_loss: 0.6514 - val_acc: 0.6380\n",
      "6710/6710 [==============================] - 2s 285us/step\n",
      "TN:66,FP:80,FN:29,TP:109,Macc:0.620954884983,F1:0.666661255643\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2616 - acc: 0.8880 - val_loss: 0.6121 - val_acc: 0.6773\n",
      "6710/6710 [==============================] - 2s 294us/step\n",
      "TN:85,FP:61,FN:37,TP:101,Macc:0.657037868295,F1:0.6733278192\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2582 - acc: 0.8901 - val_loss: 0.6214 - val_acc: 0.6712\n",
      "6710/6710 [==============================] - 2s 277us/step\n",
      "TN:58,FP:88,FN:19,TP:119,Macc:0.629789507943,F1:0.689849748513\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2544 - acc: 0.8918 - val_loss: 0.6483 - val_acc: 0.6590\n",
      "6710/6710 [==============================] - 2s 307us/step\n",
      "TN:54,FP:92,FN:10,TP:128,Macc:0.648699571891,F1:0.715078543529\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2532 - acc: 0.8920 - val_loss: 0.6003 - val_acc: 0.6891\n",
      "6710/6710 [==============================] - 2s 291us/step\n",
      "TN:65,FP:81,FN:20,TP:118,Macc:0.65013892076,F1:0.700291370432\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2509 - acc: 0.8919 - val_loss: 0.6122 - val_acc: 0.6903\n",
      "6710/6710 [==============================] - 2s 295us/step\n",
      "TN:78,FP:68,FN:41,TP:97,Macc:0.618572514894,F1:0.640258523634\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2484 - acc: 0.8937 - val_loss: 0.6218 - val_acc: 0.6826\n",
      "6710/6710 [==============================] - 2s 274us/step\n",
      "TN:66,FP:80,FN:26,TP:112,Macc:0.631824449334,F1:0.67878248085\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2484 - acc: 0.8931 - val_loss: 0.6337 - val_acc: 0.6863\n",
      "6710/6710 [==============================] - 2s 290us/step\n",
      "TN:74,FP:72,FN:35,TP:103,Macc:0.626613014491,F1:0.658141495497\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2462 - acc: 0.8948 - val_loss: 0.6529 - val_acc: 0.6745\n",
      "6710/6710 [==============================] - 2s 279us/step\n",
      "TN:57,FP:89,FN:16,TP:122,Macc:0.637234415018,F1:0.699135097748\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 46s 37ms/step - loss: 0.2463 - acc: 0.8955 - val_loss: 0.6401 - val_acc: 0.6805\n",
      "6710/6710 [==============================] - 2s 274us/step\n",
      "TN:59,FP:87,FN:15,TP:123,Macc:0.647706917687,F1:0.706891242508\n",
      "Loss: 0\n",
      "args (15.0, 2, 5.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_33 (Con (None, 2500, 1)      31          input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_34 (Con (None, 2500, 1)      31          input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_35 (Con (None, 2500, 1)      31          input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_36 (Con (None, 2500, 1)      31          input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_333 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_33[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_335 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_34[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_337 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_35[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_339 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_36[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_341 (BatchN (None, 2496, 8)      32          conv1d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 2496, 8)      32          conv1d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_345 (BatchN (None, 2496, 8)      32          conv1d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 2496, 8)      32          conv1d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 2496, 8)      0           batch_normalization_341[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 2496, 8)      0           batch_normalization_343[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 2496, 8)      0           batch_normalization_345[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 2496, 8)      0           batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_341 (Dropout)           (None, 2496, 8)      0           activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_343 (Dropout)           (None, 2496, 8)      0           activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_345 (Dropout)           (None, 2496, 8)      0           activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_347 (Dropout)           (None, 2496, 8)      0           activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 1248, 8)      0           dropout_341[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling1D) (None, 1248, 8)      0           dropout_343[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling1D) (None, 1248, 8)      0           dropout_345[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling1D) (None, 1248, 8)      0           dropout_347[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_334 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_336 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_67[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_338 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_69[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_340 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_71[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 1244, 4)      16          conv1d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 1244, 4)      16          conv1d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 1244, 4)      16          conv1d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_348 (BatchN (None, 1244, 4)      16          conv1d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 1244, 4)      0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 1244, 4)      0           batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 1244, 4)      0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 1244, 4)      0           batch_normalization_348[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_342 (Dropout)           (None, 1244, 4)      0           activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_344 (Dropout)           (None, 1244, 4)      0           activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_346 (Dropout)           (None, 1244, 4)      0           activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_348 (Dropout)           (None, 1244, 4)      0           activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling1D) (None, 622, 4)       0           dropout_342[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling1D) (None, 622, 4)       0           dropout_344[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling1D) (None, 622, 4)       0           dropout_346[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling1D) (None, 622, 4)       0           dropout_348[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_268 (Concatenate)   (None, 622, 16)      0           max_pooling1d_66[0][0]           \n",
      "                                                                 max_pooling1d_68[0][0]           \n",
      "                                                                 max_pooling1d_70[0][0]           \n",
      "                                                                 max_pooling1d_72[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 622, 16)      64          concatenate_268[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 622, 16)      0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_341 (Conv1D)             (None, 622, 5)       400         activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_349 (Dropout)           (None, 622, 5)       0           conv1d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_269 (Concatenate)   (None, 622, 21)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 622, 21)      84          concatenate_269[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 622, 21)      0           batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_342 (Conv1D)             (None, 622, 5)       525         activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_350 (Dropout)           (None, 622, 5)       0           conv1d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_270 (Concatenate)   (None, 622, 26)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_351 (BatchN (None, 622, 26)      104         concatenate_270[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_351 (Activation)     (None, 622, 26)      0           batch_normalization_351[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_343 (Conv1D)             (None, 622, 5)       650         activation_351[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_351 (Dropout)           (None, 622, 5)       0           conv1d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_271 (Concatenate)   (None, 622, 31)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_352 (BatchN (None, 622, 31)      124         concatenate_271[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_352 (Activation)     (None, 622, 31)      0           batch_normalization_352[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_344 (Conv1D)             (None, 622, 5)       775         activation_352[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_352 (Dropout)           (None, 622, 5)       0           conv1d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_272 (Concatenate)   (None, 622, 36)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_353 (BatchN (None, 622, 36)      144         concatenate_272[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_353 (Activation)     (None, 622, 36)      0           batch_normalization_353[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_345 (Conv1D)             (None, 622, 5)       900         activation_353[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_353 (Dropout)           (None, 622, 5)       0           conv1d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_273 (Concatenate)   (None, 622, 41)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_354 (BatchN (None, 622, 41)      164         concatenate_273[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_354 (Activation)     (None, 622, 41)      0           batch_normalization_354[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_346 (Conv1D)             (None, 622, 5)       1025        activation_354[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_354 (Dropout)           (None, 622, 5)       0           conv1d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_274 (Concatenate)   (None, 622, 46)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_355 (BatchN (None, 622, 46)      184         concatenate_274[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_355 (Activation)     (None, 622, 46)      0           batch_normalization_355[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_347 (Conv1D)             (None, 622, 5)       1150        activation_355[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_355 (Dropout)           (None, 622, 5)       0           conv1d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_275 (Concatenate)   (None, 622, 51)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "                                                                 dropout_355[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_356 (BatchN (None, 622, 51)      204         concatenate_275[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_356 (Activation)     (None, 622, 51)      0           batch_normalization_356[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_348 (Conv1D)             (None, 622, 5)       1275        activation_356[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_356 (Dropout)           (None, 622, 5)       0           conv1d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_276 (Concatenate)   (None, 622, 56)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "                                                                 dropout_355[0][0]                \n",
      "                                                                 dropout_356[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_357 (BatchN (None, 622, 56)      224         concatenate_276[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_357 (Activation)     (None, 622, 56)      0           batch_normalization_357[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_349 (Conv1D)             (None, 622, 5)       1400        activation_357[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_357 (Dropout)           (None, 622, 5)       0           conv1d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_277 (Concatenate)   (None, 622, 61)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "                                                                 dropout_355[0][0]                \n",
      "                                                                 dropout_356[0][0]                \n",
      "                                                                 dropout_357[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_358 (BatchN (None, 622, 61)      244         concatenate_277[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_358 (Activation)     (None, 622, 61)      0           batch_normalization_358[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_350 (Conv1D)             (None, 622, 5)       1525        activation_358[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_358 (Dropout)           (None, 622, 5)       0           conv1d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_278 (Concatenate)   (None, 622, 66)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "                                                                 dropout_355[0][0]                \n",
      "                                                                 dropout_356[0][0]                \n",
      "                                                                 dropout_357[0][0]                \n",
      "                                                                 dropout_358[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_359 (BatchN (None, 622, 66)      264         concatenate_278[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_359 (Activation)     (None, 622, 66)      0           batch_normalization_359[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_351 (Conv1D)             (None, 622, 5)       1650        activation_359[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_359 (Dropout)           (None, 622, 5)       0           conv1d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_279 (Concatenate)   (None, 622, 71)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "                                                                 dropout_355[0][0]                \n",
      "                                                                 dropout_356[0][0]                \n",
      "                                                                 dropout_357[0][0]                \n",
      "                                                                 dropout_358[0][0]                \n",
      "                                                                 dropout_359[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_360 (BatchN (None, 622, 71)      284         concatenate_279[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_360 (Activation)     (None, 622, 71)      0           batch_normalization_360[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_352 (Conv1D)             (None, 622, 5)       1775        activation_360[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_360 (Dropout)           (None, 622, 5)       0           conv1d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_280 (Concatenate)   (None, 622, 76)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "                                                                 dropout_355[0][0]                \n",
      "                                                                 dropout_356[0][0]                \n",
      "                                                                 dropout_357[0][0]                \n",
      "                                                                 dropout_358[0][0]                \n",
      "                                                                 dropout_359[0][0]                \n",
      "                                                                 dropout_360[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_361 (BatchN (None, 622, 76)      304         concatenate_280[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_361 (Activation)     (None, 622, 76)      0           batch_normalization_361[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_353 (Conv1D)             (None, 622, 5)       1900        activation_361[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_361 (Dropout)           (None, 622, 5)       0           conv1d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_281 (Concatenate)   (None, 622, 81)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "                                                                 dropout_355[0][0]                \n",
      "                                                                 dropout_356[0][0]                \n",
      "                                                                 dropout_357[0][0]                \n",
      "                                                                 dropout_358[0][0]                \n",
      "                                                                 dropout_359[0][0]                \n",
      "                                                                 dropout_360[0][0]                \n",
      "                                                                 dropout_361[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_362 (BatchN (None, 622, 81)      324         concatenate_281[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_362 (Activation)     (None, 622, 81)      0           batch_normalization_362[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_354 (Conv1D)             (None, 622, 5)       2025        activation_362[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_362 (Dropout)           (None, 622, 5)       0           conv1d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_282 (Concatenate)   (None, 622, 86)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "                                                                 dropout_355[0][0]                \n",
      "                                                                 dropout_356[0][0]                \n",
      "                                                                 dropout_357[0][0]                \n",
      "                                                                 dropout_358[0][0]                \n",
      "                                                                 dropout_359[0][0]                \n",
      "                                                                 dropout_360[0][0]                \n",
      "                                                                 dropout_361[0][0]                \n",
      "                                                                 dropout_362[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_363 (BatchN (None, 622, 86)      344         concatenate_282[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_363 (Activation)     (None, 622, 86)      0           batch_normalization_363[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_355 (Conv1D)             (None, 622, 5)       2150        activation_363[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_363 (Dropout)           (None, 622, 5)       0           conv1d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_283 (Concatenate)   (None, 622, 91)      0           concatenate_268[0][0]            \n",
      "                                                                 dropout_349[0][0]                \n",
      "                                                                 dropout_350[0][0]                \n",
      "                                                                 dropout_351[0][0]                \n",
      "                                                                 dropout_352[0][0]                \n",
      "                                                                 dropout_353[0][0]                \n",
      "                                                                 dropout_354[0][0]                \n",
      "                                                                 dropout_355[0][0]                \n",
      "                                                                 dropout_356[0][0]                \n",
      "                                                                 dropout_357[0][0]                \n",
      "                                                                 dropout_358[0][0]                \n",
      "                                                                 dropout_359[0][0]                \n",
      "                                                                 dropout_360[0][0]                \n",
      "                                                                 dropout_361[0][0]                \n",
      "                                                                 dropout_362[0][0]                \n",
      "                                                                 dropout_363[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_364 (BatchN (None, 622, 91)      364         concatenate_283[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_364 (Activation)     (None, 622, 91)      0           batch_normalization_364[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_356 (Conv1D)             (None, 622, 91)      41405       activation_364[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_364 (Dropout)           (None, 622, 91)      0           conv1d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_10 (AveragePo (None, 311, 91)      0           dropout_364[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_365 (BatchN (None, 311, 91)      364         average_pooling1d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_365 (Activation)     (None, 311, 91)      0           batch_normalization_365[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_357 (Conv1D)             (None, 311, 5)       2275        activation_365[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_365 (Dropout)           (None, 311, 5)       0           conv1d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_284 (Concatenate)   (None, 311, 96)      0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_366 (BatchN (None, 311, 96)      384         concatenate_284[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_366 (Activation)     (None, 311, 96)      0           batch_normalization_366[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_358 (Conv1D)             (None, 311, 5)       2400        activation_366[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_366 (Dropout)           (None, 311, 5)       0           conv1d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_285 (Concatenate)   (None, 311, 101)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_367 (BatchN (None, 311, 101)     404         concatenate_285[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_367 (Activation)     (None, 311, 101)     0           batch_normalization_367[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_359 (Conv1D)             (None, 311, 5)       2525        activation_367[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_367 (Dropout)           (None, 311, 5)       0           conv1d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_286 (Concatenate)   (None, 311, 106)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_368 (BatchN (None, 311, 106)     424         concatenate_286[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_368 (Activation)     (None, 311, 106)     0           batch_normalization_368[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_360 (Conv1D)             (None, 311, 5)       2650        activation_368[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_368 (Dropout)           (None, 311, 5)       0           conv1d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_287 (Concatenate)   (None, 311, 111)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_369 (BatchN (None, 311, 111)     444         concatenate_287[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_369 (Activation)     (None, 311, 111)     0           batch_normalization_369[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_361 (Conv1D)             (None, 311, 5)       2775        activation_369[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_369 (Dropout)           (None, 311, 5)       0           conv1d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_288 (Concatenate)   (None, 311, 116)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_370 (BatchN (None, 311, 116)     464         concatenate_288[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_370 (Activation)     (None, 311, 116)     0           batch_normalization_370[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_362 (Conv1D)             (None, 311, 5)       2900        activation_370[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_370 (Dropout)           (None, 311, 5)       0           conv1d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_289 (Concatenate)   (None, 311, 121)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_371 (BatchN (None, 311, 121)     484         concatenate_289[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_371 (Activation)     (None, 311, 121)     0           batch_normalization_371[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_363 (Conv1D)             (None, 311, 5)       3025        activation_371[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_371 (Dropout)           (None, 311, 5)       0           conv1d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_290 (Concatenate)   (None, 311, 126)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "                                                                 dropout_371[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_372 (BatchN (None, 311, 126)     504         concatenate_290[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_372 (Activation)     (None, 311, 126)     0           batch_normalization_372[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_364 (Conv1D)             (None, 311, 5)       3150        activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_372 (Dropout)           (None, 311, 5)       0           conv1d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_291 (Concatenate)   (None, 311, 131)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "                                                                 dropout_371[0][0]                \n",
      "                                                                 dropout_372[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_373 (BatchN (None, 311, 131)     524         concatenate_291[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_373 (Activation)     (None, 311, 131)     0           batch_normalization_373[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_365 (Conv1D)             (None, 311, 5)       3275        activation_373[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_373 (Dropout)           (None, 311, 5)       0           conv1d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_292 (Concatenate)   (None, 311, 136)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "                                                                 dropout_371[0][0]                \n",
      "                                                                 dropout_372[0][0]                \n",
      "                                                                 dropout_373[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_374 (BatchN (None, 311, 136)     544         concatenate_292[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_374 (Activation)     (None, 311, 136)     0           batch_normalization_374[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_366 (Conv1D)             (None, 311, 5)       3400        activation_374[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_374 (Dropout)           (None, 311, 5)       0           conv1d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_293 (Concatenate)   (None, 311, 141)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "                                                                 dropout_371[0][0]                \n",
      "                                                                 dropout_372[0][0]                \n",
      "                                                                 dropout_373[0][0]                \n",
      "                                                                 dropout_374[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_375 (BatchN (None, 311, 141)     564         concatenate_293[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_375 (Activation)     (None, 311, 141)     0           batch_normalization_375[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_367 (Conv1D)             (None, 311, 5)       3525        activation_375[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_375 (Dropout)           (None, 311, 5)       0           conv1d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_294 (Concatenate)   (None, 311, 146)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "                                                                 dropout_371[0][0]                \n",
      "                                                                 dropout_372[0][0]                \n",
      "                                                                 dropout_373[0][0]                \n",
      "                                                                 dropout_374[0][0]                \n",
      "                                                                 dropout_375[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_376 (BatchN (None, 311, 146)     584         concatenate_294[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_376 (Activation)     (None, 311, 146)     0           batch_normalization_376[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_368 (Conv1D)             (None, 311, 5)       3650        activation_376[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_376 (Dropout)           (None, 311, 5)       0           conv1d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_295 (Concatenate)   (None, 311, 151)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "                                                                 dropout_371[0][0]                \n",
      "                                                                 dropout_372[0][0]                \n",
      "                                                                 dropout_373[0][0]                \n",
      "                                                                 dropout_374[0][0]                \n",
      "                                                                 dropout_375[0][0]                \n",
      "                                                                 dropout_376[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 311, 151)     604         concatenate_295[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 311, 151)     0           batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_369 (Conv1D)             (None, 311, 5)       3775        activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_377 (Dropout)           (None, 311, 5)       0           conv1d_369[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_296 (Concatenate)   (None, 311, 156)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "                                                                 dropout_371[0][0]                \n",
      "                                                                 dropout_372[0][0]                \n",
      "                                                                 dropout_373[0][0]                \n",
      "                                                                 dropout_374[0][0]                \n",
      "                                                                 dropout_375[0][0]                \n",
      "                                                                 dropout_376[0][0]                \n",
      "                                                                 dropout_377[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 311, 156)     624         concatenate_296[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 311, 156)     0           batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_370 (Conv1D)             (None, 311, 5)       3900        activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_378 (Dropout)           (None, 311, 5)       0           conv1d_370[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_297 (Concatenate)   (None, 311, 161)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "                                                                 dropout_371[0][0]                \n",
      "                                                                 dropout_372[0][0]                \n",
      "                                                                 dropout_373[0][0]                \n",
      "                                                                 dropout_374[0][0]                \n",
      "                                                                 dropout_375[0][0]                \n",
      "                                                                 dropout_376[0][0]                \n",
      "                                                                 dropout_377[0][0]                \n",
      "                                                                 dropout_378[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 311, 161)     644         concatenate_297[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 311, 161)     0           batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_371 (Conv1D)             (None, 311, 5)       4025        activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_379 (Dropout)           (None, 311, 5)       0           conv1d_371[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_298 (Concatenate)   (None, 311, 166)     0           average_pooling1d_10[0][0]       \n",
      "                                                                 dropout_365[0][0]                \n",
      "                                                                 dropout_366[0][0]                \n",
      "                                                                 dropout_367[0][0]                \n",
      "                                                                 dropout_368[0][0]                \n",
      "                                                                 dropout_369[0][0]                \n",
      "                                                                 dropout_370[0][0]                \n",
      "                                                                 dropout_371[0][0]                \n",
      "                                                                 dropout_372[0][0]                \n",
      "                                                                 dropout_373[0][0]                \n",
      "                                                                 dropout_374[0][0]                \n",
      "                                                                 dropout_375[0][0]                \n",
      "                                                                 dropout_376[0][0]                \n",
      "                                                                 dropout_377[0][0]                \n",
      "                                                                 dropout_378[0][0]                \n",
      "                                                                 dropout_379[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 311, 166)     664         concatenate_298[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 311, 166)     0           batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 51626)        0           activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 20)           1032520     flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_380 (Dropout)           (None, 20)           0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 2)            42          dropout_380[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,153,106\n",
      "Trainable params: 1,147,186\n",
      "Non-trainable params: 5,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 142s 113ms/step - loss: 0.9324 - acc: 0.8255 - val_loss: 0.6566 - val_acc: 0.7183\n",
      "6710/6710 [==============================] - 10s 2ms/step\n",
      "TN:71,FP:75,FN:16,TP:122,Macc:0.685179616885,F1:0.728352831761\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 132s 106ms/step - loss: 0.3617 - acc: 0.8417 - val_loss: 0.6516 - val_acc: 0.6587\n",
      "6710/6710 [==============================] - 5s 796us/step\n",
      "TN:72,FP:74,FN:34,TP:104,Macc:0.623386888055,F1:0.658222390447\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.3287 - acc: 0.8465 - val_loss: 0.6789 - val_acc: 0.6858\n",
      "6710/6710 [==============================] - 5s 765us/step\n",
      "TN:94,FP:52,FN:39,TP:99,Macc:0.680613407547,F1:0.685115566285\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 130s 104ms/step - loss: 0.3130 - acc: 0.8531 - val_loss: 0.6670 - val_acc: 0.6773\n",
      "6710/6710 [==============================] - 5s 767us/step\n",
      "TN:92,FP:54,FN:45,TP:93,Macc:0.652024964293,F1:0.6526260341\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 130s 104ms/step - loss: 0.3005 - acc: 0.8723 - val_loss: 0.5906 - val_acc: 0.6770\n",
      "6710/6710 [==============================] - 5s 759us/step\n",
      "TN:61,FP:85,FN:17,TP:121,Macc:0.647309856006,F1:0.703483042056\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.2856 - acc: 0.8813 - val_loss: 0.5806 - val_acc: 0.7006\n",
      "6710/6710 [==============================] - 5s 776us/step\n",
      "TN:76,FP:70,FN:29,TP:109,Macc:0.655201457745,F1:0.687691705204\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.2789 - acc: 0.8848 - val_loss: 0.6470 - val_acc: 0.6896\n",
      "6710/6710 [==============================] - 5s 750us/step\n",
      "TN:95,FP:51,FN:49,TP:89,Macc:0.647806183654,F1:0.640282219446\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 129s 103ms/step - loss: 0.2742 - acc: 0.8858 - val_loss: 0.6231 - val_acc: 0.6958\n",
      "6710/6710 [==============================] - 5s 750us/step\n",
      "TN:93,FP:53,FN:46,TP:92,Macc:0.651826433452,F1:0.650171131313\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.2705 - acc: 0.8880 - val_loss: 0.6292 - val_acc: 0.6639\n",
      "6710/6710 [==============================] - 5s 744us/step\n",
      "TN:59,FP:87,FN:12,TP:126,Macc:0.658576482038,F1:0.717943424101\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.2671 - acc: 0.8884 - val_loss: 0.6105 - val_acc: 0.6815\n",
      "6710/6710 [==============================] - 5s 774us/step\n",
      "TN:56,FP:90,FN:8,TP:130,Macc:0.662795262678,F1:0.726251727199\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 130s 104ms/step - loss: 0.2661 - acc: 0.8892 - val_loss: 0.5954 - val_acc: 0.7098\n",
      "6710/6710 [==============================] - 5s 753us/step\n",
      "TN:97,FP:49,FN:49,TP:89,Macc:0.654655498206,F1:0.644921984872\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 130s 104ms/step - loss: 0.2598 - acc: 0.8924 - val_loss: 0.5788 - val_acc: 0.7122\n",
      "6710/6710 [==============================] - 5s 745us/step\n",
      "TN:69,FP:77,FN:15,TP:123,Macc:0.68195349045,F1:0.727805288614\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.2588 - acc: 0.8928 - val_loss: 0.5796 - val_acc: 0.7142\n",
      "6710/6710 [==============================] - 5s 754us/step\n",
      "TN:74,FP:72,FN:24,TP:114,Macc:0.666468083778,F1:0.703698276676\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.2581 - acc: 0.8924 - val_loss: 1.1772 - val_acc: 0.6489\n",
      "6710/6710 [==============================] - 5s 750us/step\n",
      "TN:115,FP:31,FN:71,TP:67,Macc:0.636589190604,F1:0.56779121529\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.2545 - acc: 0.8943 - val_loss: 0.6175 - val_acc: 0.7113\n",
      "6710/6710 [==============================] - 5s 750us/step\n",
      "TN:84,FP:62,FN:32,TP:106,Macc:0.671729151604,F1:0.692804960614\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.2540 - acc: 0.8942 - val_loss: 0.6025 - val_acc: 0.7057\n",
      "6710/6710 [==============================] - 5s 783us/step\n",
      "TN:78,FP:68,FN:31,TP:107,Macc:0.654804396064,F1:0.68370059913\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 130s 104ms/step - loss: 0.2496 - acc: 0.8970 - val_loss: 0.6696 - val_acc: 0.6933\n",
      "6710/6710 [==============================] - 5s 772us/step\n",
      "TN:100,FP:46,FN:47,TP:91,Macc:0.672175846269,F1:0.661812628991\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 129s 103ms/step - loss: 0.2492 - acc: 0.8971 - val_loss: 0.6302 - val_acc: 0.6866\n",
      "6710/6710 [==============================] - 5s 779us/step\n",
      "TN:80,FP:66,FN:30,TP:108,Macc:0.665276898733,F1:0.692302216759\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 129s 103ms/step - loss: 0.2487 - acc: 0.8969 - val_loss: 0.8758 - val_acc: 0.6538\n",
      "6710/6710 [==============================] - 5s 752us/step\n",
      "TN:121,FP:25,FN:74,TP:64,Macc:0.646267569911,F1:0.563871353655\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 129s 104ms/step - loss: 0.2469 - acc: 0.8982 - val_loss: 0.5966 - val_acc: 0.7006\n",
      "6710/6710 [==============================] - 5s 763us/step\n",
      "TN:82,FP:64,FN:30,TP:108,Macc:0.672126213286,F1:0.696768710303\n",
      "Loss: 0\n",
      "args (18.0, 2, 18.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_37 (Con (None, 2500, 1)      31          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_38 (Con (None, 2500, 1)      31          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_39 (Con (None, 2500, 1)      31          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_40 (Con (None, 2500, 1)      31          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_372 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_37[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_374 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_38[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_376 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_39[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_378 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_40[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 2496, 8)      32          conv1d_372[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 2496, 8)      32          conv1d_374[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_385 (BatchN (None, 2496, 8)      32          conv1d_376[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_387 (BatchN (None, 2496, 8)      32          conv1d_378[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 2496, 8)      0           batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 2496, 8)      0           batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 2496, 8)      0           batch_normalization_385[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 2496, 8)      0           batch_normalization_387[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_381 (Dropout)           (None, 2496, 8)      0           activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_383 (Dropout)           (None, 2496, 8)      0           activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_385 (Dropout)           (None, 2496, 8)      0           activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_387 (Dropout)           (None, 2496, 8)      0           activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling1D) (None, 1248, 8)      0           dropout_381[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling1D) (None, 1248, 8)      0           dropout_383[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling1D) (None, 1248, 8)      0           dropout_385[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling1D) (None, 1248, 8)      0           dropout_387[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_373 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_73[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_375 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_75[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_377 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_77[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_379 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_79[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 1244, 4)      16          conv1d_373[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 1244, 4)      16          conv1d_375[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_386 (BatchN (None, 1244, 4)      16          conv1d_377[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_388 (BatchN (None, 1244, 4)      16          conv1d_379[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 1244, 4)      0           batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 1244, 4)      0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 1244, 4)      0           batch_normalization_386[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 1244, 4)      0           batch_normalization_388[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_382 (Dropout)           (None, 1244, 4)      0           activation_382[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_384 (Dropout)           (None, 1244, 4)      0           activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_386 (Dropout)           (None, 1244, 4)      0           activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_388 (Dropout)           (None, 1244, 4)      0           activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling1D) (None, 622, 4)       0           dropout_382[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling1D) (None, 622, 4)       0           dropout_384[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling1D) (None, 622, 4)       0           dropout_386[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling1D) (None, 622, 4)       0           dropout_388[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_299 (Concatenate)   (None, 622, 16)      0           max_pooling1d_74[0][0]           \n",
      "                                                                 max_pooling1d_76[0][0]           \n",
      "                                                                 max_pooling1d_78[0][0]           \n",
      "                                                                 max_pooling1d_80[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_389 (BatchN (None, 622, 16)      64          concatenate_299[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 622, 16)      0           batch_normalization_389[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_380 (Conv1D)             (None, 622, 18)      1440        activation_389[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_389 (Dropout)           (None, 622, 18)      0           conv1d_380[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_300 (Concatenate)   (None, 622, 34)      0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 622, 34)      136         concatenate_300[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 622, 34)      0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_381 (Conv1D)             (None, 622, 18)      3060        activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_390 (Dropout)           (None, 622, 18)      0           conv1d_381[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_301 (Concatenate)   (None, 622, 52)      0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 622, 52)      208         concatenate_301[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 622, 52)      0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_382 (Conv1D)             (None, 622, 18)      4680        activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_391 (Dropout)           (None, 622, 18)      0           conv1d_382[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_302 (Concatenate)   (None, 622, 70)      0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 622, 70)      280         concatenate_302[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 622, 70)      0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_383 (Conv1D)             (None, 622, 18)      6300        activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_392 (Dropout)           (None, 622, 18)      0           conv1d_383[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_303 (Concatenate)   (None, 622, 88)      0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 622, 88)      352         concatenate_303[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 622, 88)      0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_384 (Conv1D)             (None, 622, 18)      7920        activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_393 (Dropout)           (None, 622, 18)      0           conv1d_384[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_304 (Concatenate)   (None, 622, 106)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 622, 106)     424         concatenate_304[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 622, 106)     0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_385 (Conv1D)             (None, 622, 18)      9540        activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_394 (Dropout)           (None, 622, 18)      0           conv1d_385[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_305 (Concatenate)   (None, 622, 124)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 622, 124)     496         concatenate_305[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 622, 124)     0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_386 (Conv1D)             (None, 622, 18)      11160       activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_395 (Dropout)           (None, 622, 18)      0           conv1d_386[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_306 (Concatenate)   (None, 622, 142)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 622, 142)     568         concatenate_306[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 622, 142)     0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_387 (Conv1D)             (None, 622, 18)      12780       activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_396 (Dropout)           (None, 622, 18)      0           conv1d_387[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_307 (Concatenate)   (None, 622, 160)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 622, 160)     640         concatenate_307[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 622, 160)     0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_388 (Conv1D)             (None, 622, 18)      14400       activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_397 (Dropout)           (None, 622, 18)      0           conv1d_388[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_308 (Concatenate)   (None, 622, 178)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 622, 178)     712         concatenate_308[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 622, 178)     0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_389 (Conv1D)             (None, 622, 18)      16020       activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_398 (Dropout)           (None, 622, 18)      0           conv1d_389[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_309 (Concatenate)   (None, 622, 196)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "                                                                 dropout_398[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 622, 196)     784         concatenate_309[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 622, 196)     0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_390 (Conv1D)             (None, 622, 18)      17640       activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_399 (Dropout)           (None, 622, 18)      0           conv1d_390[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_310 (Concatenate)   (None, 622, 214)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "                                                                 dropout_398[0][0]                \n",
      "                                                                 dropout_399[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 622, 214)     856         concatenate_310[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 622, 214)     0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_391 (Conv1D)             (None, 622, 18)      19260       activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_400 (Dropout)           (None, 622, 18)      0           conv1d_391[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_311 (Concatenate)   (None, 622, 232)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "                                                                 dropout_398[0][0]                \n",
      "                                                                 dropout_399[0][0]                \n",
      "                                                                 dropout_400[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 622, 232)     928         concatenate_311[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 622, 232)     0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_392 (Conv1D)             (None, 622, 18)      20880       activation_401[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_401 (Dropout)           (None, 622, 18)      0           conv1d_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_312 (Concatenate)   (None, 622, 250)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "                                                                 dropout_398[0][0]                \n",
      "                                                                 dropout_399[0][0]                \n",
      "                                                                 dropout_400[0][0]                \n",
      "                                                                 dropout_401[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 622, 250)     1000        concatenate_312[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 622, 250)     0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_393 (Conv1D)             (None, 622, 18)      22500       activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_402 (Dropout)           (None, 622, 18)      0           conv1d_393[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_313 (Concatenate)   (None, 622, 268)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "                                                                 dropout_398[0][0]                \n",
      "                                                                 dropout_399[0][0]                \n",
      "                                                                 dropout_400[0][0]                \n",
      "                                                                 dropout_401[0][0]                \n",
      "                                                                 dropout_402[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 622, 268)     1072        concatenate_313[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 622, 268)     0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_394 (Conv1D)             (None, 622, 18)      24120       activation_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_403 (Dropout)           (None, 622, 18)      0           conv1d_394[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_314 (Concatenate)   (None, 622, 286)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "                                                                 dropout_398[0][0]                \n",
      "                                                                 dropout_399[0][0]                \n",
      "                                                                 dropout_400[0][0]                \n",
      "                                                                 dropout_401[0][0]                \n",
      "                                                                 dropout_402[0][0]                \n",
      "                                                                 dropout_403[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 622, 286)     1144        concatenate_314[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 622, 286)     0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_395 (Conv1D)             (None, 622, 18)      25740       activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_404 (Dropout)           (None, 622, 18)      0           conv1d_395[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_315 (Concatenate)   (None, 622, 304)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "                                                                 dropout_398[0][0]                \n",
      "                                                                 dropout_399[0][0]                \n",
      "                                                                 dropout_400[0][0]                \n",
      "                                                                 dropout_401[0][0]                \n",
      "                                                                 dropout_402[0][0]                \n",
      "                                                                 dropout_403[0][0]                \n",
      "                                                                 dropout_404[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 622, 304)     1216        concatenate_315[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 622, 304)     0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_396 (Conv1D)             (None, 622, 18)      27360       activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_405 (Dropout)           (None, 622, 18)      0           conv1d_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_316 (Concatenate)   (None, 622, 322)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "                                                                 dropout_398[0][0]                \n",
      "                                                                 dropout_399[0][0]                \n",
      "                                                                 dropout_400[0][0]                \n",
      "                                                                 dropout_401[0][0]                \n",
      "                                                                 dropout_402[0][0]                \n",
      "                                                                 dropout_403[0][0]                \n",
      "                                                                 dropout_404[0][0]                \n",
      "                                                                 dropout_405[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 622, 322)     1288        concatenate_316[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 622, 322)     0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_397 (Conv1D)             (None, 622, 18)      28980       activation_406[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_406 (Dropout)           (None, 622, 18)      0           conv1d_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_317 (Concatenate)   (None, 622, 340)     0           concatenate_299[0][0]            \n",
      "                                                                 dropout_389[0][0]                \n",
      "                                                                 dropout_390[0][0]                \n",
      "                                                                 dropout_391[0][0]                \n",
      "                                                                 dropout_392[0][0]                \n",
      "                                                                 dropout_393[0][0]                \n",
      "                                                                 dropout_394[0][0]                \n",
      "                                                                 dropout_395[0][0]                \n",
      "                                                                 dropout_396[0][0]                \n",
      "                                                                 dropout_397[0][0]                \n",
      "                                                                 dropout_398[0][0]                \n",
      "                                                                 dropout_399[0][0]                \n",
      "                                                                 dropout_400[0][0]                \n",
      "                                                                 dropout_401[0][0]                \n",
      "                                                                 dropout_402[0][0]                \n",
      "                                                                 dropout_403[0][0]                \n",
      "                                                                 dropout_404[0][0]                \n",
      "                                                                 dropout_405[0][0]                \n",
      "                                                                 dropout_406[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_407 (BatchN (None, 622, 340)     1360        concatenate_317[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 622, 340)     0           batch_normalization_407[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_398 (Conv1D)             (None, 622, 340)     578000      activation_407[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_407 (Dropout)           (None, 622, 340)     0           conv1d_398[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_11 (AveragePo (None, 311, 340)     0           dropout_407[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_408 (BatchN (None, 311, 340)     1360        average_pooling1d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 311, 340)     0           batch_normalization_408[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_399 (Conv1D)             (None, 311, 18)      30600       activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_408 (Dropout)           (None, 311, 18)      0           conv1d_399[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_318 (Concatenate)   (None, 311, 358)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_409 (BatchN (None, 311, 358)     1432        concatenate_318[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 311, 358)     0           batch_normalization_409[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_400 (Conv1D)             (None, 311, 18)      32220       activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_409 (Dropout)           (None, 311, 18)      0           conv1d_400[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_319 (Concatenate)   (None, 311, 376)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_410 (BatchN (None, 311, 376)     1504        concatenate_319[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 311, 376)     0           batch_normalization_410[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_401 (Conv1D)             (None, 311, 18)      33840       activation_410[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_410 (Dropout)           (None, 311, 18)      0           conv1d_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_320 (Concatenate)   (None, 311, 394)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_411 (BatchN (None, 311, 394)     1576        concatenate_320[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 311, 394)     0           batch_normalization_411[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_402 (Conv1D)             (None, 311, 18)      35460       activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_411 (Dropout)           (None, 311, 18)      0           conv1d_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_321 (Concatenate)   (None, 311, 412)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_412 (BatchN (None, 311, 412)     1648        concatenate_321[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 311, 412)     0           batch_normalization_412[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_403 (Conv1D)             (None, 311, 18)      37080       activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_412 (Dropout)           (None, 311, 18)      0           conv1d_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_322 (Concatenate)   (None, 311, 430)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_413 (BatchN (None, 311, 430)     1720        concatenate_322[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 311, 430)     0           batch_normalization_413[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_404 (Conv1D)             (None, 311, 18)      38700       activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_413 (Dropout)           (None, 311, 18)      0           conv1d_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_323 (Concatenate)   (None, 311, 448)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_414 (BatchN (None, 311, 448)     1792        concatenate_323[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 311, 448)     0           batch_normalization_414[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_405 (Conv1D)             (None, 311, 18)      40320       activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_414 (Dropout)           (None, 311, 18)      0           conv1d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_324 (Concatenate)   (None, 311, 466)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_415 (BatchN (None, 311, 466)     1864        concatenate_324[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 311, 466)     0           batch_normalization_415[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_406 (Conv1D)             (None, 311, 18)      41940       activation_415[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_415 (Dropout)           (None, 311, 18)      0           conv1d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_325 (Concatenate)   (None, 311, 484)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_416 (BatchN (None, 311, 484)     1936        concatenate_325[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 311, 484)     0           batch_normalization_416[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_407 (Conv1D)             (None, 311, 18)      43560       activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_416 (Dropout)           (None, 311, 18)      0           conv1d_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_326 (Concatenate)   (None, 311, 502)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_417 (BatchN (None, 311, 502)     2008        concatenate_326[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_417 (Activation)     (None, 311, 502)     0           batch_normalization_417[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_408 (Conv1D)             (None, 311, 18)      45180       activation_417[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_417 (Dropout)           (None, 311, 18)      0           conv1d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_327 (Concatenate)   (None, 311, 520)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "                                                                 dropout_417[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_418 (BatchN (None, 311, 520)     2080        concatenate_327[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_418 (Activation)     (None, 311, 520)     0           batch_normalization_418[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_409 (Conv1D)             (None, 311, 18)      46800       activation_418[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_418 (Dropout)           (None, 311, 18)      0           conv1d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_328 (Concatenate)   (None, 311, 538)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "                                                                 dropout_417[0][0]                \n",
      "                                                                 dropout_418[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_419 (BatchN (None, 311, 538)     2152        concatenate_328[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_419 (Activation)     (None, 311, 538)     0           batch_normalization_419[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_410 (Conv1D)             (None, 311, 18)      48420       activation_419[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_419 (Dropout)           (None, 311, 18)      0           conv1d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_329 (Concatenate)   (None, 311, 556)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "                                                                 dropout_417[0][0]                \n",
      "                                                                 dropout_418[0][0]                \n",
      "                                                                 dropout_419[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_420 (BatchN (None, 311, 556)     2224        concatenate_329[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_420 (Activation)     (None, 311, 556)     0           batch_normalization_420[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_411 (Conv1D)             (None, 311, 18)      50040       activation_420[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_420 (Dropout)           (None, 311, 18)      0           conv1d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_330 (Concatenate)   (None, 311, 574)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "                                                                 dropout_417[0][0]                \n",
      "                                                                 dropout_418[0][0]                \n",
      "                                                                 dropout_419[0][0]                \n",
      "                                                                 dropout_420[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_421 (BatchN (None, 311, 574)     2296        concatenate_330[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_421 (Activation)     (None, 311, 574)     0           batch_normalization_421[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_412 (Conv1D)             (None, 311, 18)      51660       activation_421[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_421 (Dropout)           (None, 311, 18)      0           conv1d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_331 (Concatenate)   (None, 311, 592)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "                                                                 dropout_417[0][0]                \n",
      "                                                                 dropout_418[0][0]                \n",
      "                                                                 dropout_419[0][0]                \n",
      "                                                                 dropout_420[0][0]                \n",
      "                                                                 dropout_421[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_422 (BatchN (None, 311, 592)     2368        concatenate_331[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_422 (Activation)     (None, 311, 592)     0           batch_normalization_422[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_413 (Conv1D)             (None, 311, 18)      53280       activation_422[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_422 (Dropout)           (None, 311, 18)      0           conv1d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_332 (Concatenate)   (None, 311, 610)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "                                                                 dropout_417[0][0]                \n",
      "                                                                 dropout_418[0][0]                \n",
      "                                                                 dropout_419[0][0]                \n",
      "                                                                 dropout_420[0][0]                \n",
      "                                                                 dropout_421[0][0]                \n",
      "                                                                 dropout_422[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_423 (BatchN (None, 311, 610)     2440        concatenate_332[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_423 (Activation)     (None, 311, 610)     0           batch_normalization_423[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_414 (Conv1D)             (None, 311, 18)      54900       activation_423[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_423 (Dropout)           (None, 311, 18)      0           conv1d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_333 (Concatenate)   (None, 311, 628)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "                                                                 dropout_417[0][0]                \n",
      "                                                                 dropout_418[0][0]                \n",
      "                                                                 dropout_419[0][0]                \n",
      "                                                                 dropout_420[0][0]                \n",
      "                                                                 dropout_421[0][0]                \n",
      "                                                                 dropout_422[0][0]                \n",
      "                                                                 dropout_423[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_424 (BatchN (None, 311, 628)     2512        concatenate_333[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_424 (Activation)     (None, 311, 628)     0           batch_normalization_424[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_415 (Conv1D)             (None, 311, 18)      56520       activation_424[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_424 (Dropout)           (None, 311, 18)      0           conv1d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_334 (Concatenate)   (None, 311, 646)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "                                                                 dropout_417[0][0]                \n",
      "                                                                 dropout_418[0][0]                \n",
      "                                                                 dropout_419[0][0]                \n",
      "                                                                 dropout_420[0][0]                \n",
      "                                                                 dropout_421[0][0]                \n",
      "                                                                 dropout_422[0][0]                \n",
      "                                                                 dropout_423[0][0]                \n",
      "                                                                 dropout_424[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_425 (BatchN (None, 311, 646)     2584        concatenate_334[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_425 (Activation)     (None, 311, 646)     0           batch_normalization_425[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_416 (Conv1D)             (None, 311, 18)      58140       activation_425[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_425 (Dropout)           (None, 311, 18)      0           conv1d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_335 (Concatenate)   (None, 311, 664)     0           average_pooling1d_11[0][0]       \n",
      "                                                                 dropout_408[0][0]                \n",
      "                                                                 dropout_409[0][0]                \n",
      "                                                                 dropout_410[0][0]                \n",
      "                                                                 dropout_411[0][0]                \n",
      "                                                                 dropout_412[0][0]                \n",
      "                                                                 dropout_413[0][0]                \n",
      "                                                                 dropout_414[0][0]                \n",
      "                                                                 dropout_415[0][0]                \n",
      "                                                                 dropout_416[0][0]                \n",
      "                                                                 dropout_417[0][0]                \n",
      "                                                                 dropout_418[0][0]                \n",
      "                                                                 dropout_419[0][0]                \n",
      "                                                                 dropout_420[0][0]                \n",
      "                                                                 dropout_421[0][0]                \n",
      "                                                                 dropout_422[0][0]                \n",
      "                                                                 dropout_423[0][0]                \n",
      "                                                                 dropout_424[0][0]                \n",
      "                                                                 dropout_425[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_426 (BatchN (None, 311, 664)     2656        concatenate_335[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_426 (Activation)     (None, 311, 664)     0           batch_normalization_426[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 206504)       0           activation_426[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 20)           4130080     flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_426 (Dropout)           (None, 20)           0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 2)            42          dropout_426[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 5,833,358\n",
      "Trainable params: 5,807,422\n",
      "Non-trainable params: 25,936\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 406s 326ms/step - loss: 3.8532 - acc: 0.8114 - val_loss: 7.2601 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 21s 3ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 394s 316ms/step - loss: 1.0782 - acc: 0.8214 - val_loss: 0.8692 - val_acc: 0.5335\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:37,FP:109,FN:28,TP:110,Macc:0.525263012089,F1:0.616241243804\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 395s 316ms/step - loss: 0.2989 - acc: 0.8633 - val_loss: 0.6723 - val_acc: 0.6800\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:59,FP:87,FN:8,TP:130,Macc:0.673069234506,F1:0.732389093217\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 394s 316ms/step - loss: 0.2781 - acc: 0.8736 - val_loss: 0.7568 - val_acc: 0.6401\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:45,FP:101,FN:5,TP:133,Macc:0.63599359699,F1:0.715048587475\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 395s 317ms/step - loss: 0.2652 - acc: 0.8780 - val_loss: 0.8388 - val_acc: 0.5981\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:38,FP:108,FN:3,TP:135,Macc:0.61926737229,F1:0.708656294165\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 395s 317ms/step - loss: 0.2587 - acc: 0.8820 - val_loss: 0.7347 - val_acc: 0.6221\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:43,FP:103,FN:5,TP:133,Macc:0.629144282438,F1:0.711224782358\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 395s 317ms/step - loss: 0.2526 - acc: 0.8846 - val_loss: 0.6088 - val_acc: 0.6955\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:63,FP:83,FN:7,TP:131,Macc:0.690391051728,F1:0.744312891727\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 396s 317ms/step - loss: 0.2521 - acc: 0.8843 - val_loss: 0.6059 - val_acc: 0.6911\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:72,FP:74,FN:25,TP:113,Macc:0.655995581108,F1:0.695379193376\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 396s 317ms/step - loss: 0.2471 - acc: 0.8869 - val_loss: 0.9788 - val_acc: 0.6119\n",
      "6710/6710 [==============================] - 15s 2ms/step\n",
      "TN:40,FP:106,FN:3,TP:135,Macc:0.626116686843,F1:0.71239592031\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 398s 319ms/step - loss: 0.2459 - acc: 0.8886 - val_loss: 0.5406 - val_acc: 0.7219\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:69,FP:77,FN:10,TP:128,Macc:0.700069431034,F1:0.746350347155\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 396s 318ms/step - loss: 0.2422 - acc: 0.8925 - val_loss: 0.5368 - val_acc: 0.7207\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:76,FP:70,FN:11,TP:127,Macc:0.720418843851,F1:0.758203576068\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 397s 318ms/step - loss: 0.2400 - acc: 0.8943 - val_loss: 0.5868 - val_acc: 0.7130\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:86,FP:60,FN:29,TP:109,Macc:0.689448030507,F1:0.710092225105\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 397s 318ms/step - loss: 0.2356 - acc: 0.8972 - val_loss: 0.5960 - val_acc: 0.7234\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:73,FP:73,FN:13,TP:125,Macc:0.702898495788,F1:0.744042245751\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 397s 318ms/step - loss: 0.2345 - acc: 0.8963 - val_loss: 0.8302 - val_acc: 0.6300\n",
      "6710/6710 [==============================] - 15s 2ms/step\n",
      "TN:40,FP:106,FN:2,TP:136,Macc:0.62973987496,F1:0.715784344247\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 396s 318ms/step - loss: 0.2303 - acc: 0.8991 - val_loss: 0.8439 - val_acc: 0.6748\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:52,FP:94,FN:6,TP:132,Macc:0.656343009807,F1:0.725269502935\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 396s 318ms/step - loss: 0.2278 - acc: 0.9016 - val_loss: 0.5831 - val_acc: 0.7264\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:77,FP:69,FN:22,TP:116,Macc:0.68398843184,F1:0.718260821442\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 396s 317ms/step - loss: 0.2283 - acc: 0.8996 - val_loss: 0.7299 - val_acc: 0.6949\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:55,FP:91,FN:1,TP:137,Macc:0.68473292222,F1:0.748628667389\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 396s 318ms/step - loss: 0.2255 - acc: 0.9021 - val_loss: 0.7481 - val_acc: 0.7030\n",
      "6710/6710 [==============================] - 15s 2ms/step\n",
      "TN:59,FP:87,FN:4,TP:134,Macc:0.687561986974,F1:0.746512854126\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 396s 317ms/step - loss: 0.2270 - acc: 0.9017 - val_loss: 0.6604 - val_acc: 0.6851\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:55,FP:91,FN:3,TP:135,Macc:0.677486545986,F1:0.741753018422\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 395s 317ms/step - loss: 0.2236 - acc: 0.9031 - val_loss: 0.7088 - val_acc: 0.6872\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:54,FP:92,FN:5,TP:133,Macc:0.666815512476,F1:0.732777140699\n",
      "Loss: 0\n",
      "args (23.0, 1, 12.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_41 (Con (None, 2500, 1)      31          input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_42 (Con (None, 2500, 1)      31          input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_43 (Con (None, 2500, 1)      31          input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_44 (Con (None, 2500, 1)      31          input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_417 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_41[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_419 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_42[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_421 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_43[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_423 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_44[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_427 (BatchN (None, 2496, 8)      32          conv1d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_429 (BatchN (None, 2496, 8)      32          conv1d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_431 (BatchN (None, 2496, 8)      32          conv1d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_433 (BatchN (None, 2496, 8)      32          conv1d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_427 (Activation)     (None, 2496, 8)      0           batch_normalization_427[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_429 (Activation)     (None, 2496, 8)      0           batch_normalization_429[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_431 (Activation)     (None, 2496, 8)      0           batch_normalization_431[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_433 (Activation)     (None, 2496, 8)      0           batch_normalization_433[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_427 (Dropout)           (None, 2496, 8)      0           activation_427[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_429 (Dropout)           (None, 2496, 8)      0           activation_429[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_431 (Dropout)           (None, 2496, 8)      0           activation_431[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_433 (Dropout)           (None, 2496, 8)      0           activation_433[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling1D) (None, 1248, 8)      0           dropout_427[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling1D) (None, 1248, 8)      0           dropout_429[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling1D) (None, 1248, 8)      0           dropout_431[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling1D) (None, 1248, 8)      0           dropout_433[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_418 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_81[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_420 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_83[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_422 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_85[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_424 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_87[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_428 (BatchN (None, 1244, 4)      16          conv1d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_430 (BatchN (None, 1244, 4)      16          conv1d_420[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_432 (BatchN (None, 1244, 4)      16          conv1d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_434 (BatchN (None, 1244, 4)      16          conv1d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_428 (Activation)     (None, 1244, 4)      0           batch_normalization_428[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_430 (Activation)     (None, 1244, 4)      0           batch_normalization_430[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_432 (Activation)     (None, 1244, 4)      0           batch_normalization_432[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_434 (Activation)     (None, 1244, 4)      0           batch_normalization_434[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_428 (Dropout)           (None, 1244, 4)      0           activation_428[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_430 (Dropout)           (None, 1244, 4)      0           activation_430[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_432 (Dropout)           (None, 1244, 4)      0           activation_432[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_434 (Dropout)           (None, 1244, 4)      0           activation_434[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling1D) (None, 622, 4)       0           dropout_428[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling1D) (None, 622, 4)       0           dropout_430[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling1D) (None, 622, 4)       0           dropout_432[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling1D) (None, 622, 4)       0           dropout_434[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_336 (Concatenate)   (None, 622, 16)      0           max_pooling1d_82[0][0]           \n",
      "                                                                 max_pooling1d_84[0][0]           \n",
      "                                                                 max_pooling1d_86[0][0]           \n",
      "                                                                 max_pooling1d_88[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_435 (BatchN (None, 622, 16)      64          concatenate_336[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_435 (Activation)     (None, 622, 16)      0           batch_normalization_435[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_425 (Conv1D)             (None, 622, 12)      960         activation_435[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_435 (Dropout)           (None, 622, 12)      0           conv1d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_337 (Concatenate)   (None, 622, 28)      0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_436 (BatchN (None, 622, 28)      112         concatenate_337[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_436 (Activation)     (None, 622, 28)      0           batch_normalization_436[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_426 (Conv1D)             (None, 622, 12)      1680        activation_436[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_436 (Dropout)           (None, 622, 12)      0           conv1d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_338 (Concatenate)   (None, 622, 40)      0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_437 (BatchN (None, 622, 40)      160         concatenate_338[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_437 (Activation)     (None, 622, 40)      0           batch_normalization_437[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_427 (Conv1D)             (None, 622, 12)      2400        activation_437[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_437 (Dropout)           (None, 622, 12)      0           conv1d_427[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_339 (Concatenate)   (None, 622, 52)      0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_438 (BatchN (None, 622, 52)      208         concatenate_339[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_438 (Activation)     (None, 622, 52)      0           batch_normalization_438[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_428 (Conv1D)             (None, 622, 12)      3120        activation_438[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_438 (Dropout)           (None, 622, 12)      0           conv1d_428[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_340 (Concatenate)   (None, 622, 64)      0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_439 (BatchN (None, 622, 64)      256         concatenate_340[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_439 (Activation)     (None, 622, 64)      0           batch_normalization_439[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_429 (Conv1D)             (None, 622, 12)      3840        activation_439[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_439 (Dropout)           (None, 622, 12)      0           conv1d_429[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_341 (Concatenate)   (None, 622, 76)      0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_440 (BatchN (None, 622, 76)      304         concatenate_341[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_440 (Activation)     (None, 622, 76)      0           batch_normalization_440[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_430 (Conv1D)             (None, 622, 12)      4560        activation_440[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_440 (Dropout)           (None, 622, 12)      0           conv1d_430[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_342 (Concatenate)   (None, 622, 88)      0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_441 (BatchN (None, 622, 88)      352         concatenate_342[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_441 (Activation)     (None, 622, 88)      0           batch_normalization_441[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_431 (Conv1D)             (None, 622, 12)      5280        activation_441[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_441 (Dropout)           (None, 622, 12)      0           conv1d_431[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_343 (Concatenate)   (None, 622, 100)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_442 (BatchN (None, 622, 100)     400         concatenate_343[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_442 (Activation)     (None, 622, 100)     0           batch_normalization_442[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_432 (Conv1D)             (None, 622, 12)      6000        activation_442[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_442 (Dropout)           (None, 622, 12)      0           conv1d_432[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_344 (Concatenate)   (None, 622, 112)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_443 (BatchN (None, 622, 112)     448         concatenate_344[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_443 (Activation)     (None, 622, 112)     0           batch_normalization_443[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_433 (Conv1D)             (None, 622, 12)      6720        activation_443[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_443 (Dropout)           (None, 622, 12)      0           conv1d_433[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_345 (Concatenate)   (None, 622, 124)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_444 (BatchN (None, 622, 124)     496         concatenate_345[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_444 (Activation)     (None, 622, 124)     0           batch_normalization_444[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_434 (Conv1D)             (None, 622, 12)      7440        activation_444[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_444 (Dropout)           (None, 622, 12)      0           conv1d_434[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_346 (Concatenate)   (None, 622, 136)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_445 (BatchN (None, 622, 136)     544         concatenate_346[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_445 (Activation)     (None, 622, 136)     0           batch_normalization_445[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_435 (Conv1D)             (None, 622, 12)      8160        activation_445[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_445 (Dropout)           (None, 622, 12)      0           conv1d_435[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_347 (Concatenate)   (None, 622, 148)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_446 (BatchN (None, 622, 148)     592         concatenate_347[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_446 (Activation)     (None, 622, 148)     0           batch_normalization_446[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_436 (Conv1D)             (None, 622, 12)      8880        activation_446[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_446 (Dropout)           (None, 622, 12)      0           conv1d_436[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_348 (Concatenate)   (None, 622, 160)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_447 (BatchN (None, 622, 160)     640         concatenate_348[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_447 (Activation)     (None, 622, 160)     0           batch_normalization_447[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_437 (Conv1D)             (None, 622, 12)      9600        activation_447[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_447 (Dropout)           (None, 622, 12)      0           conv1d_437[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_349 (Concatenate)   (None, 622, 172)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_448 (BatchN (None, 622, 172)     688         concatenate_349[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 622, 172)     0           batch_normalization_448[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_438 (Conv1D)             (None, 622, 12)      10320       activation_448[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_448 (Dropout)           (None, 622, 12)      0           conv1d_438[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_350 (Concatenate)   (None, 622, 184)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_449 (BatchN (None, 622, 184)     736         concatenate_350[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 622, 184)     0           batch_normalization_449[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_439 (Conv1D)             (None, 622, 12)      11040       activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_449 (Dropout)           (None, 622, 12)      0           conv1d_439[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_351 (Concatenate)   (None, 622, 196)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "                                                                 dropout_449[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_450 (BatchN (None, 622, 196)     784         concatenate_351[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_450 (Activation)     (None, 622, 196)     0           batch_normalization_450[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_440 (Conv1D)             (None, 622, 12)      11760       activation_450[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_450 (Dropout)           (None, 622, 12)      0           conv1d_440[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_352 (Concatenate)   (None, 622, 208)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "                                                                 dropout_449[0][0]                \n",
      "                                                                 dropout_450[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_451 (BatchN (None, 622, 208)     832         concatenate_352[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_451 (Activation)     (None, 622, 208)     0           batch_normalization_451[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_441 (Conv1D)             (None, 622, 12)      12480       activation_451[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_451 (Dropout)           (None, 622, 12)      0           conv1d_441[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_353 (Concatenate)   (None, 622, 220)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "                                                                 dropout_449[0][0]                \n",
      "                                                                 dropout_450[0][0]                \n",
      "                                                                 dropout_451[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_452 (BatchN (None, 622, 220)     880         concatenate_353[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_452 (Activation)     (None, 622, 220)     0           batch_normalization_452[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_442 (Conv1D)             (None, 622, 12)      13200       activation_452[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_452 (Dropout)           (None, 622, 12)      0           conv1d_442[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_354 (Concatenate)   (None, 622, 232)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "                                                                 dropout_449[0][0]                \n",
      "                                                                 dropout_450[0][0]                \n",
      "                                                                 dropout_451[0][0]                \n",
      "                                                                 dropout_452[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_453 (BatchN (None, 622, 232)     928         concatenate_354[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_453 (Activation)     (None, 622, 232)     0           batch_normalization_453[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_443 (Conv1D)             (None, 622, 12)      13920       activation_453[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_453 (Dropout)           (None, 622, 12)      0           conv1d_443[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_355 (Concatenate)   (None, 622, 244)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "                                                                 dropout_449[0][0]                \n",
      "                                                                 dropout_450[0][0]                \n",
      "                                                                 dropout_451[0][0]                \n",
      "                                                                 dropout_452[0][0]                \n",
      "                                                                 dropout_453[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_454 (BatchN (None, 622, 244)     976         concatenate_355[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_454 (Activation)     (None, 622, 244)     0           batch_normalization_454[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_444 (Conv1D)             (None, 622, 12)      14640       activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_454 (Dropout)           (None, 622, 12)      0           conv1d_444[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_356 (Concatenate)   (None, 622, 256)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "                                                                 dropout_449[0][0]                \n",
      "                                                                 dropout_450[0][0]                \n",
      "                                                                 dropout_451[0][0]                \n",
      "                                                                 dropout_452[0][0]                \n",
      "                                                                 dropout_453[0][0]                \n",
      "                                                                 dropout_454[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_455 (BatchN (None, 622, 256)     1024        concatenate_356[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_455 (Activation)     (None, 622, 256)     0           batch_normalization_455[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_445 (Conv1D)             (None, 622, 12)      15360       activation_455[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_455 (Dropout)           (None, 622, 12)      0           conv1d_445[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_357 (Concatenate)   (None, 622, 268)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "                                                                 dropout_449[0][0]                \n",
      "                                                                 dropout_450[0][0]                \n",
      "                                                                 dropout_451[0][0]                \n",
      "                                                                 dropout_452[0][0]                \n",
      "                                                                 dropout_453[0][0]                \n",
      "                                                                 dropout_454[0][0]                \n",
      "                                                                 dropout_455[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_456 (BatchN (None, 622, 268)     1072        concatenate_357[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_456 (Activation)     (None, 622, 268)     0           batch_normalization_456[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_446 (Conv1D)             (None, 622, 12)      16080       activation_456[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_456 (Dropout)           (None, 622, 12)      0           conv1d_446[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_358 (Concatenate)   (None, 622, 280)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "                                                                 dropout_449[0][0]                \n",
      "                                                                 dropout_450[0][0]                \n",
      "                                                                 dropout_451[0][0]                \n",
      "                                                                 dropout_452[0][0]                \n",
      "                                                                 dropout_453[0][0]                \n",
      "                                                                 dropout_454[0][0]                \n",
      "                                                                 dropout_455[0][0]                \n",
      "                                                                 dropout_456[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_457 (BatchN (None, 622, 280)     1120        concatenate_358[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_457 (Activation)     (None, 622, 280)     0           batch_normalization_457[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_447 (Conv1D)             (None, 622, 12)      16800       activation_457[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_457 (Dropout)           (None, 622, 12)      0           conv1d_447[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_359 (Concatenate)   (None, 622, 292)     0           concatenate_336[0][0]            \n",
      "                                                                 dropout_435[0][0]                \n",
      "                                                                 dropout_436[0][0]                \n",
      "                                                                 dropout_437[0][0]                \n",
      "                                                                 dropout_438[0][0]                \n",
      "                                                                 dropout_439[0][0]                \n",
      "                                                                 dropout_440[0][0]                \n",
      "                                                                 dropout_441[0][0]                \n",
      "                                                                 dropout_442[0][0]                \n",
      "                                                                 dropout_443[0][0]                \n",
      "                                                                 dropout_444[0][0]                \n",
      "                                                                 dropout_445[0][0]                \n",
      "                                                                 dropout_446[0][0]                \n",
      "                                                                 dropout_447[0][0]                \n",
      "                                                                 dropout_448[0][0]                \n",
      "                                                                 dropout_449[0][0]                \n",
      "                                                                 dropout_450[0][0]                \n",
      "                                                                 dropout_451[0][0]                \n",
      "                                                                 dropout_452[0][0]                \n",
      "                                                                 dropout_453[0][0]                \n",
      "                                                                 dropout_454[0][0]                \n",
      "                                                                 dropout_455[0][0]                \n",
      "                                                                 dropout_456[0][0]                \n",
      "                                                                 dropout_457[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_458 (BatchN (None, 622, 292)     1168        concatenate_359[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_458 (Activation)     (None, 622, 292)     0           batch_normalization_458[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 181624)       0           activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 20)           3632480     flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_458 (Dropout)           (None, 20)           0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 2)            42          dropout_458[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,852,662\n",
      "Trainable params: 3,845,174\n",
      "Non-trainable params: 7,488\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 190s 152ms/step - loss: 3.5621 - acc: 0.8112 - val_loss: 7.2473 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 15s 2ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 181s 145ms/step - loss: 1.1469 - acc: 0.8106 - val_loss: 0.7355 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 181s 145ms/step - loss: 0.3205 - acc: 0.8335 - val_loss: 0.7434 - val_acc: 0.6757\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:114,FP:32,FN:71,TP:67,Macc:0.633164533328,F1:0.565395440382\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 181s 145ms/step - loss: 0.3055 - acc: 0.8429 - val_loss: 0.7121 - val_acc: 0.6882\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:96,FP:50,FN:55,TP:83,Macc:0.629491712228,F1:0.612540577656\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 181s 146ms/step - loss: 0.2956 - acc: 0.8551 - val_loss: 0.6235 - val_acc: 0.6930\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:105,FP:41,FN:55,TP:83,Macc:0.660313627714,F1:0.633582248809\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 181s 145ms/step - loss: 0.2814 - acc: 0.8750 - val_loss: 0.5527 - val_acc: 0.7076\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:81,FP:65,FN:35,TP:103,Macc:0.650585615424,F1:0.673197118887\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2751 - acc: 0.8803 - val_loss: 0.6620 - val_acc: 0.6878\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:107,FP:39,FN:57,TP:81,Macc:0.659916566032,F1:0.627901450021\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2697 - acc: 0.8818 - val_loss: 0.6835 - val_acc: 0.6863\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:104,FP:42,FN:58,TP:80,Macc:0.646019406087,F1:0.615379084191\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2662 - acc: 0.8848 - val_loss: 0.6033 - val_acc: 0.6943\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:96,FP:50,FN:45,TP:93,Macc:0.665723593398,F1:0.661916158149\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 181s 146ms/step - loss: 0.2625 - acc: 0.8844 - val_loss: 0.6255 - val_acc: 0.6958\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:89,FP:57,FN:39,TP:99,Macc:0.663490121166,F1:0.67346385802\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2612 - acc: 0.8854 - val_loss: 0.6197 - val_acc: 0.7000\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:94,FP:52,FN:43,TP:95,Macc:0.666120655079,F1:0.666661120735\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2562 - acc: 0.8888 - val_loss: 0.7453 - val_acc: 0.6896\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:97,FP:49,FN:52,TP:86,Macc:0.643785933855,F1:0.630031079977\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2566 - acc: 0.8869 - val_loss: 0.7204 - val_acc: 0.6887\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:92,FP:54,FN:51,TP:87,Macc:0.630285835591,F1:0.623650365486\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2542 - acc: 0.8878 - val_loss: 0.6537 - val_acc: 0.6978\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:91,FP:55,FN:50,TP:88,Macc:0.630484366432,F1:0.626328972326\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2516 - acc: 0.8908 - val_loss: 0.6732 - val_acc: 0.6888\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:94,FP:52,FN:54,TP:84,Macc:0.626265585792,F1:0.613133137244\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 181s 146ms/step - loss: 0.2504 - acc: 0.8896 - val_loss: 0.6013 - val_acc: 0.7098\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:84,FP:62,FN:38,TP:100,Macc:0.649990022902,F1:0.666661153023\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2478 - acc: 0.8915 - val_loss: 0.5905 - val_acc: 0.7073\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:86,FP:60,FN:34,TP:104,Macc:0.671332089922,F1:0.68873621249\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2466 - acc: 0.8939 - val_loss: 0.6208 - val_acc: 0.7109\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:85,FP:61,FN:36,TP:102,Macc:0.660661056412,F1:0.677735352237\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 181s 145ms/step - loss: 0.2469 - acc: 0.8922 - val_loss: 0.7264 - val_acc: 0.6815\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:107,FP:39,FN:66,TP:72,Macc:0.627307872979,F1:0.578307766635\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 182s 146ms/step - loss: 0.2455 - acc: 0.8938 - val_loss: 0.7486 - val_acc: 0.6812\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:105,FP:41,FN:65,TP:73,Macc:0.624081746544,F1:0.579359578723\n",
      "Loss: 0\n",
      "args (24.0, 1, 14.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_45 (Con (None, 2500, 1)      31          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_46 (Con (None, 2500, 1)      31          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_47 (Con (None, 2500, 1)      31          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_48 (Con (None, 2500, 1)      31          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_448 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_45[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_450 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_46[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_452 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_47[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_454 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_48[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_459 (BatchN (None, 2496, 8)      32          conv1d_448[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_461 (BatchN (None, 2496, 8)      32          conv1d_450[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_463 (BatchN (None, 2496, 8)      32          conv1d_452[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_465 (BatchN (None, 2496, 8)      32          conv1d_454[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_459 (Activation)     (None, 2496, 8)      0           batch_normalization_459[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_461 (Activation)     (None, 2496, 8)      0           batch_normalization_461[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_463 (Activation)     (None, 2496, 8)      0           batch_normalization_463[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_465 (Activation)     (None, 2496, 8)      0           batch_normalization_465[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_459 (Dropout)           (None, 2496, 8)      0           activation_459[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_461 (Dropout)           (None, 2496, 8)      0           activation_461[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_463 (Dropout)           (None, 2496, 8)      0           activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_465 (Dropout)           (None, 2496, 8)      0           activation_465[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling1D) (None, 1248, 8)      0           dropout_459[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling1D) (None, 1248, 8)      0           dropout_461[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling1D) (None, 1248, 8)      0           dropout_463[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling1D) (None, 1248, 8)      0           dropout_465[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_449 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_89[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_451 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_91[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_453 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_93[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_455 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_95[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_460 (BatchN (None, 1244, 4)      16          conv1d_449[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_462 (BatchN (None, 1244, 4)      16          conv1d_451[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_464 (BatchN (None, 1244, 4)      16          conv1d_453[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_466 (BatchN (None, 1244, 4)      16          conv1d_455[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_460 (Activation)     (None, 1244, 4)      0           batch_normalization_460[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_462 (Activation)     (None, 1244, 4)      0           batch_normalization_462[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_464 (Activation)     (None, 1244, 4)      0           batch_normalization_464[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_466 (Activation)     (None, 1244, 4)      0           batch_normalization_466[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_460 (Dropout)           (None, 1244, 4)      0           activation_460[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_462 (Dropout)           (None, 1244, 4)      0           activation_462[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_464 (Dropout)           (None, 1244, 4)      0           activation_464[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_466 (Dropout)           (None, 1244, 4)      0           activation_466[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling1D) (None, 622, 4)       0           dropout_460[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling1D) (None, 622, 4)       0           dropout_462[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling1D) (None, 622, 4)       0           dropout_464[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling1D) (None, 622, 4)       0           dropout_466[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_360 (Concatenate)   (None, 622, 16)      0           max_pooling1d_90[0][0]           \n",
      "                                                                 max_pooling1d_92[0][0]           \n",
      "                                                                 max_pooling1d_94[0][0]           \n",
      "                                                                 max_pooling1d_96[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_467 (BatchN (None, 622, 16)      64          concatenate_360[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_467 (Activation)     (None, 622, 16)      0           batch_normalization_467[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_456 (Conv1D)             (None, 622, 14)      1120        activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_467 (Dropout)           (None, 622, 14)      0           conv1d_456[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_361 (Concatenate)   (None, 622, 30)      0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_468 (BatchN (None, 622, 30)      120         concatenate_361[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_468 (Activation)     (None, 622, 30)      0           batch_normalization_468[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_457 (Conv1D)             (None, 622, 14)      2100        activation_468[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_468 (Dropout)           (None, 622, 14)      0           conv1d_457[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_362 (Concatenate)   (None, 622, 44)      0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_469 (BatchN (None, 622, 44)      176         concatenate_362[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_469 (Activation)     (None, 622, 44)      0           batch_normalization_469[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_458 (Conv1D)             (None, 622, 14)      3080        activation_469[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_469 (Dropout)           (None, 622, 14)      0           conv1d_458[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_363 (Concatenate)   (None, 622, 58)      0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_470 (BatchN (None, 622, 58)      232         concatenate_363[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_470 (Activation)     (None, 622, 58)      0           batch_normalization_470[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_459 (Conv1D)             (None, 622, 14)      4060        activation_470[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_470 (Dropout)           (None, 622, 14)      0           conv1d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_364 (Concatenate)   (None, 622, 72)      0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_471 (BatchN (None, 622, 72)      288         concatenate_364[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_471 (Activation)     (None, 622, 72)      0           batch_normalization_471[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_460 (Conv1D)             (None, 622, 14)      5040        activation_471[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_471 (Dropout)           (None, 622, 14)      0           conv1d_460[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_365 (Concatenate)   (None, 622, 86)      0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_472 (BatchN (None, 622, 86)      344         concatenate_365[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_472 (Activation)     (None, 622, 86)      0           batch_normalization_472[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_461 (Conv1D)             (None, 622, 14)      6020        activation_472[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_472 (Dropout)           (None, 622, 14)      0           conv1d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_366 (Concatenate)   (None, 622, 100)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_473 (BatchN (None, 622, 100)     400         concatenate_366[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_473 (Activation)     (None, 622, 100)     0           batch_normalization_473[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_462 (Conv1D)             (None, 622, 14)      7000        activation_473[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_473 (Dropout)           (None, 622, 14)      0           conv1d_462[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_367 (Concatenate)   (None, 622, 114)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_474 (BatchN (None, 622, 114)     456         concatenate_367[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_474 (Activation)     (None, 622, 114)     0           batch_normalization_474[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_463 (Conv1D)             (None, 622, 14)      7980        activation_474[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_474 (Dropout)           (None, 622, 14)      0           conv1d_463[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_368 (Concatenate)   (None, 622, 128)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_475 (BatchN (None, 622, 128)     512         concatenate_368[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_475 (Activation)     (None, 622, 128)     0           batch_normalization_475[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_464 (Conv1D)             (None, 622, 14)      8960        activation_475[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_475 (Dropout)           (None, 622, 14)      0           conv1d_464[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_369 (Concatenate)   (None, 622, 142)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_476 (BatchN (None, 622, 142)     568         concatenate_369[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_476 (Activation)     (None, 622, 142)     0           batch_normalization_476[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_465 (Conv1D)             (None, 622, 14)      9940        activation_476[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_476 (Dropout)           (None, 622, 14)      0           conv1d_465[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_370 (Concatenate)   (None, 622, 156)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_477 (BatchN (None, 622, 156)     624         concatenate_370[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_477 (Activation)     (None, 622, 156)     0           batch_normalization_477[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_466 (Conv1D)             (None, 622, 14)      10920       activation_477[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_477 (Dropout)           (None, 622, 14)      0           conv1d_466[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_371 (Concatenate)   (None, 622, 170)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_478 (BatchN (None, 622, 170)     680         concatenate_371[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_478 (Activation)     (None, 622, 170)     0           batch_normalization_478[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_467 (Conv1D)             (None, 622, 14)      11900       activation_478[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_478 (Dropout)           (None, 622, 14)      0           conv1d_467[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_372 (Concatenate)   (None, 622, 184)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_479 (BatchN (None, 622, 184)     736         concatenate_372[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_479 (Activation)     (None, 622, 184)     0           batch_normalization_479[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_468 (Conv1D)             (None, 622, 14)      12880       activation_479[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_479 (Dropout)           (None, 622, 14)      0           conv1d_468[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_373 (Concatenate)   (None, 622, 198)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_480 (BatchN (None, 622, 198)     792         concatenate_373[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_480 (Activation)     (None, 622, 198)     0           batch_normalization_480[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_469 (Conv1D)             (None, 622, 14)      13860       activation_480[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_480 (Dropout)           (None, 622, 14)      0           conv1d_469[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_374 (Concatenate)   (None, 622, 212)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_481 (BatchN (None, 622, 212)     848         concatenate_374[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_481 (Activation)     (None, 622, 212)     0           batch_normalization_481[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_470 (Conv1D)             (None, 622, 14)      14840       activation_481[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_481 (Dropout)           (None, 622, 14)      0           conv1d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_375 (Concatenate)   (None, 622, 226)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_482 (BatchN (None, 622, 226)     904         concatenate_375[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_482 (Activation)     (None, 622, 226)     0           batch_normalization_482[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_471 (Conv1D)             (None, 622, 14)      15820       activation_482[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_482 (Dropout)           (None, 622, 14)      0           conv1d_471[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_376 (Concatenate)   (None, 622, 240)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "                                                                 dropout_482[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_483 (BatchN (None, 622, 240)     960         concatenate_376[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_483 (Activation)     (None, 622, 240)     0           batch_normalization_483[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_472 (Conv1D)             (None, 622, 14)      16800       activation_483[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_483 (Dropout)           (None, 622, 14)      0           conv1d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_377 (Concatenate)   (None, 622, 254)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "                                                                 dropout_482[0][0]                \n",
      "                                                                 dropout_483[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_484 (BatchN (None, 622, 254)     1016        concatenate_377[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_484 (Activation)     (None, 622, 254)     0           batch_normalization_484[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_473 (Conv1D)             (None, 622, 14)      17780       activation_484[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_484 (Dropout)           (None, 622, 14)      0           conv1d_473[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_378 (Concatenate)   (None, 622, 268)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "                                                                 dropout_482[0][0]                \n",
      "                                                                 dropout_483[0][0]                \n",
      "                                                                 dropout_484[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_485 (BatchN (None, 622, 268)     1072        concatenate_378[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_485 (Activation)     (None, 622, 268)     0           batch_normalization_485[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_474 (Conv1D)             (None, 622, 14)      18760       activation_485[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_485 (Dropout)           (None, 622, 14)      0           conv1d_474[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_379 (Concatenate)   (None, 622, 282)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "                                                                 dropout_482[0][0]                \n",
      "                                                                 dropout_483[0][0]                \n",
      "                                                                 dropout_484[0][0]                \n",
      "                                                                 dropout_485[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_486 (BatchN (None, 622, 282)     1128        concatenate_379[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_486 (Activation)     (None, 622, 282)     0           batch_normalization_486[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_475 (Conv1D)             (None, 622, 14)      19740       activation_486[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_486 (Dropout)           (None, 622, 14)      0           conv1d_475[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_380 (Concatenate)   (None, 622, 296)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "                                                                 dropout_482[0][0]                \n",
      "                                                                 dropout_483[0][0]                \n",
      "                                                                 dropout_484[0][0]                \n",
      "                                                                 dropout_485[0][0]                \n",
      "                                                                 dropout_486[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_487 (BatchN (None, 622, 296)     1184        concatenate_380[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_487 (Activation)     (None, 622, 296)     0           batch_normalization_487[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_476 (Conv1D)             (None, 622, 14)      20720       activation_487[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_487 (Dropout)           (None, 622, 14)      0           conv1d_476[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_381 (Concatenate)   (None, 622, 310)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "                                                                 dropout_482[0][0]                \n",
      "                                                                 dropout_483[0][0]                \n",
      "                                                                 dropout_484[0][0]                \n",
      "                                                                 dropout_485[0][0]                \n",
      "                                                                 dropout_486[0][0]                \n",
      "                                                                 dropout_487[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_488 (BatchN (None, 622, 310)     1240        concatenate_381[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_488 (Activation)     (None, 622, 310)     0           batch_normalization_488[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_477 (Conv1D)             (None, 622, 14)      21700       activation_488[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_488 (Dropout)           (None, 622, 14)      0           conv1d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_382 (Concatenate)   (None, 622, 324)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "                                                                 dropout_482[0][0]                \n",
      "                                                                 dropout_483[0][0]                \n",
      "                                                                 dropout_484[0][0]                \n",
      "                                                                 dropout_485[0][0]                \n",
      "                                                                 dropout_486[0][0]                \n",
      "                                                                 dropout_487[0][0]                \n",
      "                                                                 dropout_488[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_489 (BatchN (None, 622, 324)     1296        concatenate_382[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_489 (Activation)     (None, 622, 324)     0           batch_normalization_489[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_478 (Conv1D)             (None, 622, 14)      22680       activation_489[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_489 (Dropout)           (None, 622, 14)      0           conv1d_478[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_383 (Concatenate)   (None, 622, 338)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "                                                                 dropout_482[0][0]                \n",
      "                                                                 dropout_483[0][0]                \n",
      "                                                                 dropout_484[0][0]                \n",
      "                                                                 dropout_485[0][0]                \n",
      "                                                                 dropout_486[0][0]                \n",
      "                                                                 dropout_487[0][0]                \n",
      "                                                                 dropout_488[0][0]                \n",
      "                                                                 dropout_489[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_490 (BatchN (None, 622, 338)     1352        concatenate_383[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_490 (Activation)     (None, 622, 338)     0           batch_normalization_490[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_479 (Conv1D)             (None, 622, 14)      23660       activation_490[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_490 (Dropout)           (None, 622, 14)      0           conv1d_479[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_384 (Concatenate)   (None, 622, 352)     0           concatenate_360[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "                                                                 dropout_468[0][0]                \n",
      "                                                                 dropout_469[0][0]                \n",
      "                                                                 dropout_470[0][0]                \n",
      "                                                                 dropout_471[0][0]                \n",
      "                                                                 dropout_472[0][0]                \n",
      "                                                                 dropout_473[0][0]                \n",
      "                                                                 dropout_474[0][0]                \n",
      "                                                                 dropout_475[0][0]                \n",
      "                                                                 dropout_476[0][0]                \n",
      "                                                                 dropout_477[0][0]                \n",
      "                                                                 dropout_478[0][0]                \n",
      "                                                                 dropout_479[0][0]                \n",
      "                                                                 dropout_480[0][0]                \n",
      "                                                                 dropout_481[0][0]                \n",
      "                                                                 dropout_482[0][0]                \n",
      "                                                                 dropout_483[0][0]                \n",
      "                                                                 dropout_484[0][0]                \n",
      "                                                                 dropout_485[0][0]                \n",
      "                                                                 dropout_486[0][0]                \n",
      "                                                                 dropout_487[0][0]                \n",
      "                                                                 dropout_488[0][0]                \n",
      "                                                                 dropout_489[0][0]                \n",
      "                                                                 dropout_490[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_491 (BatchN (None, 622, 352)     1408        concatenate_384[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_491 (Activation)     (None, 622, 352)     0           batch_normalization_491[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 218944)       0           activation_491[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 20)           4378880     flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_491 (Dropout)           (None, 20)           0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 2)            42          dropout_491[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 4,695,798\n",
      "Trainable params: 4,686,502\n",
      "Non-trainable params: 9,296\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 227s 182ms/step - loss: 3.5720 - acc: 0.8115 - val_loss: 6.4528 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 17s 2ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 1.6007 - acc: 0.8261 - val_loss: 0.7528 - val_acc: 0.5721\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:140,FP:6,FN:126,TP:12,Macc:0.522930276075,F1:0.15384388662\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.3007 - acc: 0.8688 - val_loss: 0.6704 - val_acc: 0.6037\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:44,FP:102,FN:3,TP:135,Macc:0.639815315948,F1:0.719994841124\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2746 - acc: 0.8784 - val_loss: 0.8038 - val_acc: 0.4672\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:28,FP:118,FN:37,TP:101,Macc:0.461832403551,F1:0.565821078848\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2653 - acc: 0.8818 - val_loss: 0.8374 - val_acc: 0.4924\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:24,FP:122,FN:23,TP:115,Macc:0.498858408083,F1:0.613328180722\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2592 - acc: 0.8851 - val_loss: 0.6595 - val_acc: 0.6444\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:61,FP:85,FN:19,TP:119,Macc:0.640063479772,F1:0.695901092855\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2542 - acc: 0.8861 - val_loss: 0.6339 - val_acc: 0.6702\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:72,FP:74,FN:31,TP:107,Macc:0.634256452406,F1:0.670840948698\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2532 - acc: 0.8865 - val_loss: 1.2522 - val_acc: 0.5523\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2496 - acc: 0.8856 - val_loss: 0.6560 - val_acc: 0.6465\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:56,FP:90,FN:11,TP:127,Macc:0.651925698327,F1:0.715487685815\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2452 - acc: 0.8880 - val_loss: 0.8783 - val_acc: 0.5492\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:27,FP:119,FN:1,TP:137,Macc:0.588842518486,F1:0.695426426612\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2442 - acc: 0.8904 - val_loss: 0.7039 - val_acc: 0.6392\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:138,FP:8,FN:110,TP:28,Macc:0.574051971395,F1:0.321835429819\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2412 - acc: 0.8924 - val_loss: 0.8745 - val_acc: 0.5905\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:40,FP:106,FN:6,TP:132,Macc:0.615247122492,F1:0.702122507563\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2408 - acc: 0.8907 - val_loss: 0.6951 - val_acc: 0.6614\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:81,FP:65,FN:50,TP:88,Macc:0.596237793669,F1:0.604805465502\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2381 - acc: 0.8924 - val_loss: 0.8452 - val_acc: 0.6235\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:49,FP:97,FN:7,TP:131,Macc:0.642445849861,F1:0.715841784116\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2368 - acc: 0.8932 - val_loss: 0.6517 - val_acc: 0.6623\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:67,FP:79,FN:21,TP:117,Macc:0.653365047195,F1:0.700593422142\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2351 - acc: 0.8945 - val_loss: 0.6513 - val_acc: 0.6012\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:108,FP:38,FN:77,TP:61,Macc:0.590877460969,F1:0.514762533695\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2349 - acc: 0.8948 - val_loss: 0.5850 - val_acc: 0.6782\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:108,FP:38,FN:71,TP:67,Macc:0.612616589671,F1:0.551434880779\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2340 - acc: 0.8944 - val_loss: 0.6867 - val_acc: 0.6076\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:55,FP:91,FN:22,TP:116,Macc:0.608645971763,F1:0.672458445276\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2331 - acc: 0.8951 - val_loss: 0.6434 - val_acc: 0.6815\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:92,FP:54,FN:58,TP:80,Macc:0.604923518772,F1:0.588229747781\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 218s 175ms/step - loss: 0.2332 - acc: 0.8955 - val_loss: 0.6171 - val_acc: 0.6851\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:79,FP:67,FN:36,TP:102,Macc:0.640113112755,F1:0.664489622513\n",
      "Loss: 0\n",
      "args (6.0, 3, 2.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_49 (Con (None, 2500, 1)      31          input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_50 (Con (None, 2500, 1)      31          input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_51 (Con (None, 2500, 1)      31          input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_52 (Con (None, 2500, 1)      31          input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_480 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_49[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_482 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_50[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_484 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_51[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_486 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_52[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_492 (BatchN (None, 2496, 8)      32          conv1d_480[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_494 (BatchN (None, 2496, 8)      32          conv1d_482[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_496 (BatchN (None, 2496, 8)      32          conv1d_484[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_498 (BatchN (None, 2496, 8)      32          conv1d_486[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_492 (Activation)     (None, 2496, 8)      0           batch_normalization_492[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_494 (Activation)     (None, 2496, 8)      0           batch_normalization_494[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_496 (Activation)     (None, 2496, 8)      0           batch_normalization_496[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_498 (Activation)     (None, 2496, 8)      0           batch_normalization_498[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_492 (Dropout)           (None, 2496, 8)      0           activation_492[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_494 (Dropout)           (None, 2496, 8)      0           activation_494[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_496 (Dropout)           (None, 2496, 8)      0           activation_496[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_498 (Dropout)           (None, 2496, 8)      0           activation_498[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling1D) (None, 1248, 8)      0           dropout_492[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling1D) (None, 1248, 8)      0           dropout_494[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_101 (MaxPooling1D (None, 1248, 8)      0           dropout_496[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_103 (MaxPooling1D (None, 1248, 8)      0           dropout_498[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_481 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_97[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_483 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_99[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_485 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_101[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_487 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_103[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_493 (BatchN (None, 1244, 4)      16          conv1d_481[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_495 (BatchN (None, 1244, 4)      16          conv1d_483[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_497 (BatchN (None, 1244, 4)      16          conv1d_485[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_499 (BatchN (None, 1244, 4)      16          conv1d_487[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_493 (Activation)     (None, 1244, 4)      0           batch_normalization_493[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_495 (Activation)     (None, 1244, 4)      0           batch_normalization_495[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_497 (Activation)     (None, 1244, 4)      0           batch_normalization_497[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_499 (Activation)     (None, 1244, 4)      0           batch_normalization_499[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_493 (Dropout)           (None, 1244, 4)      0           activation_493[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_495 (Dropout)           (None, 1244, 4)      0           activation_495[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_497 (Dropout)           (None, 1244, 4)      0           activation_497[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_499 (Dropout)           (None, 1244, 4)      0           activation_499[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling1D) (None, 622, 4)       0           dropout_493[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_100 (MaxPooling1D (None, 622, 4)       0           dropout_495[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_102 (MaxPooling1D (None, 622, 4)       0           dropout_497[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_104 (MaxPooling1D (None, 622, 4)       0           dropout_499[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_385 (Concatenate)   (None, 622, 16)      0           max_pooling1d_98[0][0]           \n",
      "                                                                 max_pooling1d_100[0][0]          \n",
      "                                                                 max_pooling1d_102[0][0]          \n",
      "                                                                 max_pooling1d_104[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_500 (BatchN (None, 622, 16)      64          concatenate_385[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_500 (Activation)     (None, 622, 16)      0           batch_normalization_500[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_488 (Conv1D)             (None, 622, 2)       160         activation_500[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_500 (Dropout)           (None, 622, 2)       0           conv1d_488[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_386 (Concatenate)   (None, 622, 18)      0           concatenate_385[0][0]            \n",
      "                                                                 dropout_500[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_501 (BatchN (None, 622, 18)      72          concatenate_386[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_501 (Activation)     (None, 622, 18)      0           batch_normalization_501[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_489 (Conv1D)             (None, 622, 2)       180         activation_501[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_501 (Dropout)           (None, 622, 2)       0           conv1d_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_387 (Concatenate)   (None, 622, 20)      0           concatenate_385[0][0]            \n",
      "                                                                 dropout_500[0][0]                \n",
      "                                                                 dropout_501[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_502 (BatchN (None, 622, 20)      80          concatenate_387[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_502 (Activation)     (None, 622, 20)      0           batch_normalization_502[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_490 (Conv1D)             (None, 622, 2)       200         activation_502[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_502 (Dropout)           (None, 622, 2)       0           conv1d_490[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_388 (Concatenate)   (None, 622, 22)      0           concatenate_385[0][0]            \n",
      "                                                                 dropout_500[0][0]                \n",
      "                                                                 dropout_501[0][0]                \n",
      "                                                                 dropout_502[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_503 (BatchN (None, 622, 22)      88          concatenate_388[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_503 (Activation)     (None, 622, 22)      0           batch_normalization_503[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_491 (Conv1D)             (None, 622, 2)       220         activation_503[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_503 (Dropout)           (None, 622, 2)       0           conv1d_491[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_389 (Concatenate)   (None, 622, 24)      0           concatenate_385[0][0]            \n",
      "                                                                 dropout_500[0][0]                \n",
      "                                                                 dropout_501[0][0]                \n",
      "                                                                 dropout_502[0][0]                \n",
      "                                                                 dropout_503[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_504 (BatchN (None, 622, 24)      96          concatenate_389[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_504 (Activation)     (None, 622, 24)      0           batch_normalization_504[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_492 (Conv1D)             (None, 622, 2)       240         activation_504[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_504 (Dropout)           (None, 622, 2)       0           conv1d_492[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_390 (Concatenate)   (None, 622, 26)      0           concatenate_385[0][0]            \n",
      "                                                                 dropout_500[0][0]                \n",
      "                                                                 dropout_501[0][0]                \n",
      "                                                                 dropout_502[0][0]                \n",
      "                                                                 dropout_503[0][0]                \n",
      "                                                                 dropout_504[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_505 (BatchN (None, 622, 26)      104         concatenate_390[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_505 (Activation)     (None, 622, 26)      0           batch_normalization_505[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_493 (Conv1D)             (None, 622, 2)       260         activation_505[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_505 (Dropout)           (None, 622, 2)       0           conv1d_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_391 (Concatenate)   (None, 622, 28)      0           concatenate_385[0][0]            \n",
      "                                                                 dropout_500[0][0]                \n",
      "                                                                 dropout_501[0][0]                \n",
      "                                                                 dropout_502[0][0]                \n",
      "                                                                 dropout_503[0][0]                \n",
      "                                                                 dropout_504[0][0]                \n",
      "                                                                 dropout_505[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_506 (BatchN (None, 622, 28)      112         concatenate_391[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_506 (Activation)     (None, 622, 28)      0           batch_normalization_506[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_494 (Conv1D)             (None, 622, 28)      3920        activation_506[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_506 (Dropout)           (None, 622, 28)      0           conv1d_494[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_12 (AveragePo (None, 311, 28)      0           dropout_506[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_507 (BatchN (None, 311, 28)      112         average_pooling1d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_507 (Activation)     (None, 311, 28)      0           batch_normalization_507[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_495 (Conv1D)             (None, 311, 2)       280         activation_507[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_507 (Dropout)           (None, 311, 2)       0           conv1d_495[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_392 (Concatenate)   (None, 311, 30)      0           average_pooling1d_12[0][0]       \n",
      "                                                                 dropout_507[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_508 (BatchN (None, 311, 30)      120         concatenate_392[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_508 (Activation)     (None, 311, 30)      0           batch_normalization_508[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_496 (Conv1D)             (None, 311, 2)       300         activation_508[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_508 (Dropout)           (None, 311, 2)       0           conv1d_496[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_393 (Concatenate)   (None, 311, 32)      0           average_pooling1d_12[0][0]       \n",
      "                                                                 dropout_507[0][0]                \n",
      "                                                                 dropout_508[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_509 (BatchN (None, 311, 32)      128         concatenate_393[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_509 (Activation)     (None, 311, 32)      0           batch_normalization_509[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_497 (Conv1D)             (None, 311, 2)       320         activation_509[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_509 (Dropout)           (None, 311, 2)       0           conv1d_497[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_394 (Concatenate)   (None, 311, 34)      0           average_pooling1d_12[0][0]       \n",
      "                                                                 dropout_507[0][0]                \n",
      "                                                                 dropout_508[0][0]                \n",
      "                                                                 dropout_509[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_510 (BatchN (None, 311, 34)      136         concatenate_394[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_510 (Activation)     (None, 311, 34)      0           batch_normalization_510[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_498 (Conv1D)             (None, 311, 2)       340         activation_510[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_510 (Dropout)           (None, 311, 2)       0           conv1d_498[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_395 (Concatenate)   (None, 311, 36)      0           average_pooling1d_12[0][0]       \n",
      "                                                                 dropout_507[0][0]                \n",
      "                                                                 dropout_508[0][0]                \n",
      "                                                                 dropout_509[0][0]                \n",
      "                                                                 dropout_510[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_511 (BatchN (None, 311, 36)      144         concatenate_395[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_511 (Activation)     (None, 311, 36)      0           batch_normalization_511[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_499 (Conv1D)             (None, 311, 2)       360         activation_511[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_511 (Dropout)           (None, 311, 2)       0           conv1d_499[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_396 (Concatenate)   (None, 311, 38)      0           average_pooling1d_12[0][0]       \n",
      "                                                                 dropout_507[0][0]                \n",
      "                                                                 dropout_508[0][0]                \n",
      "                                                                 dropout_509[0][0]                \n",
      "                                                                 dropout_510[0][0]                \n",
      "                                                                 dropout_511[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_512 (BatchN (None, 311, 38)      152         concatenate_396[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_512 (Activation)     (None, 311, 38)      0           batch_normalization_512[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_500 (Conv1D)             (None, 311, 2)       380         activation_512[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_512 (Dropout)           (None, 311, 2)       0           conv1d_500[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_397 (Concatenate)   (None, 311, 40)      0           average_pooling1d_12[0][0]       \n",
      "                                                                 dropout_507[0][0]                \n",
      "                                                                 dropout_508[0][0]                \n",
      "                                                                 dropout_509[0][0]                \n",
      "                                                                 dropout_510[0][0]                \n",
      "                                                                 dropout_511[0][0]                \n",
      "                                                                 dropout_512[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_513 (BatchN (None, 311, 40)      160         concatenate_397[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_513 (Activation)     (None, 311, 40)      0           batch_normalization_513[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_501 (Conv1D)             (None, 311, 40)      8000        activation_513[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_513 (Dropout)           (None, 311, 40)      0           conv1d_501[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_13 (AveragePo (None, 155, 40)      0           dropout_513[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_514 (BatchN (None, 155, 40)      160         average_pooling1d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_514 (Activation)     (None, 155, 40)      0           batch_normalization_514[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_502 (Conv1D)             (None, 155, 2)       400         activation_514[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_514 (Dropout)           (None, 155, 2)       0           conv1d_502[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_398 (Concatenate)   (None, 155, 42)      0           average_pooling1d_13[0][0]       \n",
      "                                                                 dropout_514[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_515 (BatchN (None, 155, 42)      168         concatenate_398[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_515 (Activation)     (None, 155, 42)      0           batch_normalization_515[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_503 (Conv1D)             (None, 155, 2)       420         activation_515[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_515 (Dropout)           (None, 155, 2)       0           conv1d_503[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_399 (Concatenate)   (None, 155, 44)      0           average_pooling1d_13[0][0]       \n",
      "                                                                 dropout_514[0][0]                \n",
      "                                                                 dropout_515[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_516 (BatchN (None, 155, 44)      176         concatenate_399[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_516 (Activation)     (None, 155, 44)      0           batch_normalization_516[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_504 (Conv1D)             (None, 155, 2)       440         activation_516[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_516 (Dropout)           (None, 155, 2)       0           conv1d_504[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_400 (Concatenate)   (None, 155, 46)      0           average_pooling1d_13[0][0]       \n",
      "                                                                 dropout_514[0][0]                \n",
      "                                                                 dropout_515[0][0]                \n",
      "                                                                 dropout_516[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_517 (BatchN (None, 155, 46)      184         concatenate_400[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_517 (Activation)     (None, 155, 46)      0           batch_normalization_517[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_505 (Conv1D)             (None, 155, 2)       460         activation_517[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_517 (Dropout)           (None, 155, 2)       0           conv1d_505[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_401 (Concatenate)   (None, 155, 48)      0           average_pooling1d_13[0][0]       \n",
      "                                                                 dropout_514[0][0]                \n",
      "                                                                 dropout_515[0][0]                \n",
      "                                                                 dropout_516[0][0]                \n",
      "                                                                 dropout_517[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_518 (BatchN (None, 155, 48)      192         concatenate_401[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_518 (Activation)     (None, 155, 48)      0           batch_normalization_518[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_506 (Conv1D)             (None, 155, 2)       480         activation_518[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_518 (Dropout)           (None, 155, 2)       0           conv1d_506[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_402 (Concatenate)   (None, 155, 50)      0           average_pooling1d_13[0][0]       \n",
      "                                                                 dropout_514[0][0]                \n",
      "                                                                 dropout_515[0][0]                \n",
      "                                                                 dropout_516[0][0]                \n",
      "                                                                 dropout_517[0][0]                \n",
      "                                                                 dropout_518[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_519 (BatchN (None, 155, 50)      200         concatenate_402[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_519 (Activation)     (None, 155, 50)      0           batch_normalization_519[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_507 (Conv1D)             (None, 155, 2)       500         activation_519[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_519 (Dropout)           (None, 155, 2)       0           conv1d_507[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_403 (Concatenate)   (None, 155, 52)      0           average_pooling1d_13[0][0]       \n",
      "                                                                 dropout_514[0][0]                \n",
      "                                                                 dropout_515[0][0]                \n",
      "                                                                 dropout_516[0][0]                \n",
      "                                                                 dropout_517[0][0]                \n",
      "                                                                 dropout_518[0][0]                \n",
      "                                                                 dropout_519[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_520 (BatchN (None, 155, 52)      208         concatenate_403[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_520 (Activation)     (None, 155, 52)      0           batch_normalization_520[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 8060)         0           activation_520[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 20)           161200      flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_520 (Dropout)           (None, 20)           0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 2)            42          dropout_520[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 183,074\n",
      "Trainable params: 181,550\n",
      "Non-trainable params: 1,524\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 68s 55ms/step - loss: 0.8019 - acc: 0.8404 - val_loss: 0.8622 - val_acc: 0.6270\n",
      "6710/6710 [==============================] - 10s 2ms/step\n",
      "TN:125,FP:21,FN:106,TP:32,Macc:0.544024179272,F1:0.335074084759\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 59s 48ms/step - loss: 0.3291 - acc: 0.8671 - val_loss: 0.6281 - val_acc: 0.6551\n",
      "6710/6710 [==============================] - 2s 343us/step\n",
      "TN:101,FP:45,FN:68,TP:70,Macc:0.599513553088,F1:0.553354181185\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2980 - acc: 0.8769 - val_loss: 0.8382 - val_acc: 0.5717\n",
      "6710/6710 [==============================] - 2s 354us/step\n",
      "TN:145,FP:1,FN:127,TP:11,Macc:0.536430374339,F1:0.146665025972\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2812 - acc: 0.8828 - val_loss: 1.5305 - val_acc: 0.5678\n",
      "6710/6710 [==============================] - 2s 356us/step\n",
      "TN:146,FP:0,FN:131,TP:7,Macc:0.525362279148,F1:0.0965506987013\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 61s 49ms/step - loss: 0.2688 - acc: 0.8897 - val_loss: 2.1882 - val_acc: 0.5715\n",
      "6710/6710 [==============================] - 2s 347us/step\n",
      "TN:146,FP:0,FN:128,TP:10,Macc:0.536231843499,F1:0.135133729012\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2576 - acc: 0.8966 - val_loss: 3.1404 - val_acc: 0.5627\n",
      "6710/6710 [==============================] - 2s 353us/step\n",
      "TN:146,FP:0,FN:135,TP:3,Macc:0.51086952668,F1:0.0425527267274\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2536 - acc: 0.8979 - val_loss: 4.2838 - val_acc: 0.5547\n",
      "6710/6710 [==============================] - 2s 349us/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 61s 49ms/step - loss: 0.2483 - acc: 0.8994 - val_loss: 4.6185 - val_acc: 0.5624\n",
      "6710/6710 [==============================] - 2s 346us/step\n",
      "TN:146,FP:0,FN:134,TP:4,Macc:0.514492714797,F1:0.0563374171841\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2447 - acc: 0.9018 - val_loss: 4.7456 - val_acc: 0.5578\n",
      "6710/6710 [==============================] - 2s 352us/step\n",
      "TN:146,FP:0,FN:136,TP:2,Macc:0.507246338563,F1:0.0285711142874\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2405 - acc: 0.9051 - val_loss: 4.2923 - val_acc: 0.5581\n",
      "6710/6710 [==============================] - 2s 350us/step\n",
      "TN:146,FP:0,FN:136,TP:2,Macc:0.507246338563,F1:0.0285711142874\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 60s 49ms/step - loss: 0.2387 - acc: 0.9035 - val_loss: 4.3905 - val_acc: 0.5557\n",
      "6710/6710 [==============================] - 2s 352us/step\n",
      "TN:146,FP:0,FN:137,TP:1,Macc:0.503623150446,F1:0.0143883297966\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2363 - acc: 0.9053 - val_loss: 2.9644 - val_acc: 0.5689\n",
      "6710/6710 [==============================] - 2s 359us/step\n",
      "TN:146,FP:0,FN:133,TP:5,Macc:0.518115902914,F1:0.0699293168434\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2331 - acc: 0.9062 - val_loss: 2.4192 - val_acc: 0.5790\n",
      "6710/6710 [==============================] - 2s 347us/step\n",
      "TN:145,FP:1,FN:125,TP:13,Macc:0.543676750573,F1:0.171050767159\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 61s 49ms/step - loss: 0.2295 - acc: 0.9080 - val_loss: 3.7397 - val_acc: 0.5653\n",
      "6710/6710 [==============================] - 2s 357us/step\n",
      "TN:146,FP:0,FN:133,TP:5,Macc:0.518115902914,F1:0.0699293168434\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2300 - acc: 0.9067 - val_loss: 3.2451 - val_acc: 0.5742\n",
      "6710/6710 [==============================] - 2s 351us/step\n",
      "TN:146,FP:0,FN:128,TP:10,Macc:0.536231843499,F1:0.135133729012\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2268 - acc: 0.9098 - val_loss: 3.3163 - val_acc: 0.5794\n",
      "6710/6710 [==============================] - 2s 353us/step\n",
      "TN:146,FP:0,FN:124,TP:14,Macc:0.550724595967,F1:0.18420865999\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2259 - acc: 0.9073 - val_loss: 3.4649 - val_acc: 0.5683\n",
      "6710/6710 [==============================] - 2s 358us/step\n",
      "TN:146,FP:0,FN:133,TP:5,Macc:0.518115902914,F1:0.0699293168434\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2228 - acc: 0.9112 - val_loss: 3.7785 - val_acc: 0.5684\n",
      "6710/6710 [==============================] - 2s 365us/step\n",
      "TN:146,FP:0,FN:131,TP:7,Macc:0.525362279148,F1:0.0965506987013\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2223 - acc: 0.9113 - val_loss: 2.9130 - val_acc: 0.5766\n",
      "6710/6710 [==============================] - 2s 351us/step\n",
      "TN:146,FP:0,FN:126,TP:12,Macc:0.543478219733,F1:0.159998357349\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 60s 48ms/step - loss: 0.2220 - acc: 0.9107 - val_loss: 2.9831 - val_acc: 0.5748\n",
      "6710/6710 [==============================] - 2s 354us/step\n",
      "TN:146,FP:0,FN:128,TP:10,Macc:0.536231843499,F1:0.135133729012\n",
      "Loss: 2\n",
      "args (23.0, 1, 14.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_53 (Con (None, 2500, 1)      31          input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_54 (Con (None, 2500, 1)      31          input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_55 (Con (None, 2500, 1)      31          input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_56 (Con (None, 2500, 1)      31          input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_508 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_53[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_510 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_54[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_512 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_55[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_514 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_56[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_521 (BatchN (None, 2496, 8)      32          conv1d_508[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_523 (BatchN (None, 2496, 8)      32          conv1d_510[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_525 (BatchN (None, 2496, 8)      32          conv1d_512[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_527 (BatchN (None, 2496, 8)      32          conv1d_514[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_521 (Activation)     (None, 2496, 8)      0           batch_normalization_521[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_523 (Activation)     (None, 2496, 8)      0           batch_normalization_523[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_525 (Activation)     (None, 2496, 8)      0           batch_normalization_525[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_527 (Activation)     (None, 2496, 8)      0           batch_normalization_527[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_521 (Dropout)           (None, 2496, 8)      0           activation_521[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_523 (Dropout)           (None, 2496, 8)      0           activation_523[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_525 (Dropout)           (None, 2496, 8)      0           activation_525[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_527 (Dropout)           (None, 2496, 8)      0           activation_527[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_105 (MaxPooling1D (None, 1248, 8)      0           dropout_521[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_107 (MaxPooling1D (None, 1248, 8)      0           dropout_523[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_109 (MaxPooling1D (None, 1248, 8)      0           dropout_525[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_111 (MaxPooling1D (None, 1248, 8)      0           dropout_527[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_509 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_105[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_511 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_107[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_513 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_109[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_515 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_111[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_522 (BatchN (None, 1244, 4)      16          conv1d_509[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_524 (BatchN (None, 1244, 4)      16          conv1d_511[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_526 (BatchN (None, 1244, 4)      16          conv1d_513[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_528 (BatchN (None, 1244, 4)      16          conv1d_515[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_522 (Activation)     (None, 1244, 4)      0           batch_normalization_522[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_524 (Activation)     (None, 1244, 4)      0           batch_normalization_524[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_526 (Activation)     (None, 1244, 4)      0           batch_normalization_526[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_528 (Activation)     (None, 1244, 4)      0           batch_normalization_528[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_522 (Dropout)           (None, 1244, 4)      0           activation_522[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_524 (Dropout)           (None, 1244, 4)      0           activation_524[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_526 (Dropout)           (None, 1244, 4)      0           activation_526[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_528 (Dropout)           (None, 1244, 4)      0           activation_528[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_106 (MaxPooling1D (None, 622, 4)       0           dropout_522[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_108 (MaxPooling1D (None, 622, 4)       0           dropout_524[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_110 (MaxPooling1D (None, 622, 4)       0           dropout_526[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_112 (MaxPooling1D (None, 622, 4)       0           dropout_528[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_404 (Concatenate)   (None, 622, 16)      0           max_pooling1d_106[0][0]          \n",
      "                                                                 max_pooling1d_108[0][0]          \n",
      "                                                                 max_pooling1d_110[0][0]          \n",
      "                                                                 max_pooling1d_112[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_529 (BatchN (None, 622, 16)      64          concatenate_404[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_529 (Activation)     (None, 622, 16)      0           batch_normalization_529[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_516 (Conv1D)             (None, 622, 14)      1120        activation_529[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_529 (Dropout)           (None, 622, 14)      0           conv1d_516[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_405 (Concatenate)   (None, 622, 30)      0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_530 (BatchN (None, 622, 30)      120         concatenate_405[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_530 (Activation)     (None, 622, 30)      0           batch_normalization_530[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_517 (Conv1D)             (None, 622, 14)      2100        activation_530[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_530 (Dropout)           (None, 622, 14)      0           conv1d_517[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_406 (Concatenate)   (None, 622, 44)      0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_531 (BatchN (None, 622, 44)      176         concatenate_406[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_531 (Activation)     (None, 622, 44)      0           batch_normalization_531[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_518 (Conv1D)             (None, 622, 14)      3080        activation_531[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_531 (Dropout)           (None, 622, 14)      0           conv1d_518[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_407 (Concatenate)   (None, 622, 58)      0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_532 (BatchN (None, 622, 58)      232         concatenate_407[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_532 (Activation)     (None, 622, 58)      0           batch_normalization_532[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_519 (Conv1D)             (None, 622, 14)      4060        activation_532[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_532 (Dropout)           (None, 622, 14)      0           conv1d_519[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_408 (Concatenate)   (None, 622, 72)      0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_533 (BatchN (None, 622, 72)      288         concatenate_408[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_533 (Activation)     (None, 622, 72)      0           batch_normalization_533[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_520 (Conv1D)             (None, 622, 14)      5040        activation_533[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_533 (Dropout)           (None, 622, 14)      0           conv1d_520[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_409 (Concatenate)   (None, 622, 86)      0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_534 (BatchN (None, 622, 86)      344         concatenate_409[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_534 (Activation)     (None, 622, 86)      0           batch_normalization_534[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_521 (Conv1D)             (None, 622, 14)      6020        activation_534[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_534 (Dropout)           (None, 622, 14)      0           conv1d_521[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_410 (Concatenate)   (None, 622, 100)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_535 (BatchN (None, 622, 100)     400         concatenate_410[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_535 (Activation)     (None, 622, 100)     0           batch_normalization_535[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_522 (Conv1D)             (None, 622, 14)      7000        activation_535[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_535 (Dropout)           (None, 622, 14)      0           conv1d_522[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_411 (Concatenate)   (None, 622, 114)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_536 (BatchN (None, 622, 114)     456         concatenate_411[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_536 (Activation)     (None, 622, 114)     0           batch_normalization_536[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_523 (Conv1D)             (None, 622, 14)      7980        activation_536[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_536 (Dropout)           (None, 622, 14)      0           conv1d_523[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_412 (Concatenate)   (None, 622, 128)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_537 (BatchN (None, 622, 128)     512         concatenate_412[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_537 (Activation)     (None, 622, 128)     0           batch_normalization_537[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_524 (Conv1D)             (None, 622, 14)      8960        activation_537[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_537 (Dropout)           (None, 622, 14)      0           conv1d_524[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_413 (Concatenate)   (None, 622, 142)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_538 (BatchN (None, 622, 142)     568         concatenate_413[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_538 (Activation)     (None, 622, 142)     0           batch_normalization_538[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_525 (Conv1D)             (None, 622, 14)      9940        activation_538[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_538 (Dropout)           (None, 622, 14)      0           conv1d_525[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_414 (Concatenate)   (None, 622, 156)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_539 (BatchN (None, 622, 156)     624         concatenate_414[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_539 (Activation)     (None, 622, 156)     0           batch_normalization_539[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_526 (Conv1D)             (None, 622, 14)      10920       activation_539[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_539 (Dropout)           (None, 622, 14)      0           conv1d_526[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_415 (Concatenate)   (None, 622, 170)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_540 (BatchN (None, 622, 170)     680         concatenate_415[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_540 (Activation)     (None, 622, 170)     0           batch_normalization_540[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_527 (Conv1D)             (None, 622, 14)      11900       activation_540[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_540 (Dropout)           (None, 622, 14)      0           conv1d_527[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_416 (Concatenate)   (None, 622, 184)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_541 (BatchN (None, 622, 184)     736         concatenate_416[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_541 (Activation)     (None, 622, 184)     0           batch_normalization_541[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_528 (Conv1D)             (None, 622, 14)      12880       activation_541[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_541 (Dropout)           (None, 622, 14)      0           conv1d_528[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_417 (Concatenate)   (None, 622, 198)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_542 (BatchN (None, 622, 198)     792         concatenate_417[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_542 (Activation)     (None, 622, 198)     0           batch_normalization_542[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_529 (Conv1D)             (None, 622, 14)      13860       activation_542[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_542 (Dropout)           (None, 622, 14)      0           conv1d_529[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_418 (Concatenate)   (None, 622, 212)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_543 (BatchN (None, 622, 212)     848         concatenate_418[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_543 (Activation)     (None, 622, 212)     0           batch_normalization_543[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_530 (Conv1D)             (None, 622, 14)      14840       activation_543[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_543 (Dropout)           (None, 622, 14)      0           conv1d_530[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_419 (Concatenate)   (None, 622, 226)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "                                                                 dropout_543[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_544 (BatchN (None, 622, 226)     904         concatenate_419[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_544 (Activation)     (None, 622, 226)     0           batch_normalization_544[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_531 (Conv1D)             (None, 622, 14)      15820       activation_544[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_544 (Dropout)           (None, 622, 14)      0           conv1d_531[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_420 (Concatenate)   (None, 622, 240)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "                                                                 dropout_543[0][0]                \n",
      "                                                                 dropout_544[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_545 (BatchN (None, 622, 240)     960         concatenate_420[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_545 (Activation)     (None, 622, 240)     0           batch_normalization_545[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_532 (Conv1D)             (None, 622, 14)      16800       activation_545[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_545 (Dropout)           (None, 622, 14)      0           conv1d_532[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_421 (Concatenate)   (None, 622, 254)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "                                                                 dropout_543[0][0]                \n",
      "                                                                 dropout_544[0][0]                \n",
      "                                                                 dropout_545[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_546 (BatchN (None, 622, 254)     1016        concatenate_421[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_546 (Activation)     (None, 622, 254)     0           batch_normalization_546[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_533 (Conv1D)             (None, 622, 14)      17780       activation_546[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_546 (Dropout)           (None, 622, 14)      0           conv1d_533[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_422 (Concatenate)   (None, 622, 268)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "                                                                 dropout_543[0][0]                \n",
      "                                                                 dropout_544[0][0]                \n",
      "                                                                 dropout_545[0][0]                \n",
      "                                                                 dropout_546[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_547 (BatchN (None, 622, 268)     1072        concatenate_422[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_547 (Activation)     (None, 622, 268)     0           batch_normalization_547[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_534 (Conv1D)             (None, 622, 14)      18760       activation_547[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_547 (Dropout)           (None, 622, 14)      0           conv1d_534[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_423 (Concatenate)   (None, 622, 282)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "                                                                 dropout_543[0][0]                \n",
      "                                                                 dropout_544[0][0]                \n",
      "                                                                 dropout_545[0][0]                \n",
      "                                                                 dropout_546[0][0]                \n",
      "                                                                 dropout_547[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_548 (BatchN (None, 622, 282)     1128        concatenate_423[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_548 (Activation)     (None, 622, 282)     0           batch_normalization_548[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_535 (Conv1D)             (None, 622, 14)      19740       activation_548[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_548 (Dropout)           (None, 622, 14)      0           conv1d_535[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_424 (Concatenate)   (None, 622, 296)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "                                                                 dropout_543[0][0]                \n",
      "                                                                 dropout_544[0][0]                \n",
      "                                                                 dropout_545[0][0]                \n",
      "                                                                 dropout_546[0][0]                \n",
      "                                                                 dropout_547[0][0]                \n",
      "                                                                 dropout_548[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_549 (BatchN (None, 622, 296)     1184        concatenate_424[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_549 (Activation)     (None, 622, 296)     0           batch_normalization_549[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_536 (Conv1D)             (None, 622, 14)      20720       activation_549[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_549 (Dropout)           (None, 622, 14)      0           conv1d_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_425 (Concatenate)   (None, 622, 310)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "                                                                 dropout_543[0][0]                \n",
      "                                                                 dropout_544[0][0]                \n",
      "                                                                 dropout_545[0][0]                \n",
      "                                                                 dropout_546[0][0]                \n",
      "                                                                 dropout_547[0][0]                \n",
      "                                                                 dropout_548[0][0]                \n",
      "                                                                 dropout_549[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_550 (BatchN (None, 622, 310)     1240        concatenate_425[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 622, 310)     0           batch_normalization_550[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_537 (Conv1D)             (None, 622, 14)      21700       activation_550[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_550 (Dropout)           (None, 622, 14)      0           conv1d_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_426 (Concatenate)   (None, 622, 324)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "                                                                 dropout_543[0][0]                \n",
      "                                                                 dropout_544[0][0]                \n",
      "                                                                 dropout_545[0][0]                \n",
      "                                                                 dropout_546[0][0]                \n",
      "                                                                 dropout_547[0][0]                \n",
      "                                                                 dropout_548[0][0]                \n",
      "                                                                 dropout_549[0][0]                \n",
      "                                                                 dropout_550[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_551 (BatchN (None, 622, 324)     1296        concatenate_426[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 622, 324)     0           batch_normalization_551[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_538 (Conv1D)             (None, 622, 14)      22680       activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_551 (Dropout)           (None, 622, 14)      0           conv1d_538[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_427 (Concatenate)   (None, 622, 338)     0           concatenate_404[0][0]            \n",
      "                                                                 dropout_529[0][0]                \n",
      "                                                                 dropout_530[0][0]                \n",
      "                                                                 dropout_531[0][0]                \n",
      "                                                                 dropout_532[0][0]                \n",
      "                                                                 dropout_533[0][0]                \n",
      "                                                                 dropout_534[0][0]                \n",
      "                                                                 dropout_535[0][0]                \n",
      "                                                                 dropout_536[0][0]                \n",
      "                                                                 dropout_537[0][0]                \n",
      "                                                                 dropout_538[0][0]                \n",
      "                                                                 dropout_539[0][0]                \n",
      "                                                                 dropout_540[0][0]                \n",
      "                                                                 dropout_541[0][0]                \n",
      "                                                                 dropout_542[0][0]                \n",
      "                                                                 dropout_543[0][0]                \n",
      "                                                                 dropout_544[0][0]                \n",
      "                                                                 dropout_545[0][0]                \n",
      "                                                                 dropout_546[0][0]                \n",
      "                                                                 dropout_547[0][0]                \n",
      "                                                                 dropout_548[0][0]                \n",
      "                                                                 dropout_549[0][0]                \n",
      "                                                                 dropout_550[0][0]                \n",
      "                                                                 dropout_551[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_552 (BatchN (None, 622, 338)     1352        concatenate_427[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 622, 338)     0           batch_normalization_552[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 210236)       0           activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 20)           4204720     flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_552 (Dropout)           (None, 20)           0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 2)            42          dropout_552[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 4,496,570\n",
      "Trainable params: 4,487,978\n",
      "Non-trainable params: 8,592\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 215s 172ms/step - loss: 3.5839 - acc: 0.8112 - val_loss: 7.2504 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 17s 3ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 205s 165ms/step - loss: 1.3702 - acc: 0.8138 - val_loss: 0.6069 - val_acc: 0.6925\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:119,FP:27,FN:59,TP:79,Macc:0.693766077113,F1:0.647535519865\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 205s 165ms/step - loss: 0.3201 - acc: 0.8485 - val_loss: 0.6141 - val_acc: 0.6207\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:83,FP:63,FN:40,TP:98,Macc:0.639318989392,F1:0.655512879007\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 205s 165ms/step - loss: 0.3013 - acc: 0.8619 - val_loss: 0.5996 - val_acc: 0.6411\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:94,FP:52,FN:57,TP:81,Macc:0.615396021441,F1:0.597780431254\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2912 - acc: 0.8665 - val_loss: 0.7188 - val_acc: 0.5517\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:29,FP:117,FN:0,TP:138,Macc:0.599315021155,F1:0.702285024529\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 205s 165ms/step - loss: 0.2786 - acc: 0.8746 - val_loss: 0.7541 - val_acc: 0.5377\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:21,FP:125,FN:0,TP:138,Macc:0.571917762946,F1:0.688274298454\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2720 - acc: 0.8783 - val_loss: 0.6861 - val_acc: 0.5967\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:65,FP:81,FN:29,TP:109,Macc:0.617530227707,F1:0.664628740042\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2671 - acc: 0.8801 - val_loss: 0.6543 - val_acc: 0.6300\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:48,FP:98,FN:8,TP:130,Macc:0.635398004468,F1:0.71037730357\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2646 - acc: 0.8802 - val_loss: 0.6848 - val_acc: 0.6182\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:42,FP:104,FN:1,TP:137,Macc:0.640212377629,F1:0.722950009406\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2619 - acc: 0.8833 - val_loss: 0.8424 - val_acc: 0.5070\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:13,FP:133,FN:0,TP:138,Macc:0.544520504736,F1:0.674811671248\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2597 - acc: 0.8850 - val_loss: 0.7587 - val_acc: 0.5738\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:35,FP:111,FN:9,TP:129,Macc:0.58725427176,F1:0.682534543332\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2570 - acc: 0.8856 - val_loss: 0.6279 - val_acc: 0.6650\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:51,FP:95,FN:7,TP:131,Macc:0.649295164413,F1:0.719774997773\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2561 - acc: 0.8851 - val_loss: 0.8629 - val_acc: 0.5073\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:14,FP:132,FN:0,TP:138,Macc:0.547945162012,F1:0.676465627487\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2545 - acc: 0.8865 - val_loss: 0.7634 - val_acc: 0.5785\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:32,FP:114,FN:0,TP:138,Macc:0.609588992984,F1:0.707687237748\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2534 - acc: 0.8880 - val_loss: 0.7629 - val_acc: 0.5841\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:34,FP:112,FN:1,TP:137,Macc:0.61281511942,F1:0.708005248171\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2505 - acc: 0.8889 - val_loss: 0.7722 - val_acc: 0.5817\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:35,FP:111,FN:1,TP:137,Macc:0.616239776696,F1:0.70983946582\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2501 - acc: 0.8888 - val_loss: 0.6669 - val_acc: 0.6498\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:50,FP:96,FN:4,TP:134,Macc:0.656740071488,F1:0.728255669814\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2487 - acc: 0.8897 - val_loss: 0.8173 - val_acc: 0.5627\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:26,FP:120,FN:0,TP:138,Macc:0.589041049327,F1:0.696964663335\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 206s 165ms/step - loss: 0.2482 - acc: 0.8890 - val_loss: 0.7465 - val_acc: 0.6067\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:42,FP:104,FN:4,TP:134,Macc:0.629342813278,F1:0.712760804813\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 205s 165ms/step - loss: 0.2472 - acc: 0.8902 - val_loss: 0.8157 - val_acc: 0.5715\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:32,FP:114,FN:1,TP:137,Macc:0.605965804867,F1:0.704365104258\n",
      "Loss: 0\n",
      "args (24.0, 1, 2.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_57 (Con (None, 2500, 1)      31          input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_58 (Con (None, 2500, 1)      31          input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_59 (Con (None, 2500, 1)      31          input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_60 (Con (None, 2500, 1)      31          input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_539 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_57[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_541 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_58[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_543 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_59[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_545 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_60[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_553 (BatchN (None, 2496, 8)      32          conv1d_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_555 (BatchN (None, 2496, 8)      32          conv1d_541[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_557 (BatchN (None, 2496, 8)      32          conv1d_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 2496, 8)      32          conv1d_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 2496, 8)      0           batch_normalization_553[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 2496, 8)      0           batch_normalization_555[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 2496, 8)      0           batch_normalization_557[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 2496, 8)      0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_553 (Dropout)           (None, 2496, 8)      0           activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_555 (Dropout)           (None, 2496, 8)      0           activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_557 (Dropout)           (None, 2496, 8)      0           activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_559 (Dropout)           (None, 2496, 8)      0           activation_559[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_113 (MaxPooling1D (None, 1248, 8)      0           dropout_553[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_115 (MaxPooling1D (None, 1248, 8)      0           dropout_555[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_117 (MaxPooling1D (None, 1248, 8)      0           dropout_557[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_119 (MaxPooling1D (None, 1248, 8)      0           dropout_559[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_540 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_113[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_542 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_115[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_544 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_117[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_546 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_119[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_554 (BatchN (None, 1244, 4)      16          conv1d_540[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_556 (BatchN (None, 1244, 4)      16          conv1d_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_558 (BatchN (None, 1244, 4)      16          conv1d_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 1244, 4)      16          conv1d_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 1244, 4)      0           batch_normalization_554[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 1244, 4)      0           batch_normalization_556[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 1244, 4)      0           batch_normalization_558[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 1244, 4)      0           batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_554 (Dropout)           (None, 1244, 4)      0           activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_556 (Dropout)           (None, 1244, 4)      0           activation_556[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_558 (Dropout)           (None, 1244, 4)      0           activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_560 (Dropout)           (None, 1244, 4)      0           activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_114 (MaxPooling1D (None, 622, 4)       0           dropout_554[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_116 (MaxPooling1D (None, 622, 4)       0           dropout_556[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_118 (MaxPooling1D (None, 622, 4)       0           dropout_558[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_120 (MaxPooling1D (None, 622, 4)       0           dropout_560[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_428 (Concatenate)   (None, 622, 16)      0           max_pooling1d_114[0][0]          \n",
      "                                                                 max_pooling1d_116[0][0]          \n",
      "                                                                 max_pooling1d_118[0][0]          \n",
      "                                                                 max_pooling1d_120[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_561 (BatchN (None, 622, 16)      64          concatenate_428[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 622, 16)      0           batch_normalization_561[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_547 (Conv1D)             (None, 622, 2)       160         activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_561 (Dropout)           (None, 622, 2)       0           conv1d_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_429 (Concatenate)   (None, 622, 18)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 622, 18)      72          concatenate_429[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 622, 18)      0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_548 (Conv1D)             (None, 622, 2)       180         activation_562[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_562 (Dropout)           (None, 622, 2)       0           conv1d_548[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_430 (Concatenate)   (None, 622, 20)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 622, 20)      80          concatenate_430[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 622, 20)      0           batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_549 (Conv1D)             (None, 622, 2)       200         activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_563 (Dropout)           (None, 622, 2)       0           conv1d_549[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_431 (Concatenate)   (None, 622, 22)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_564 (BatchN (None, 622, 22)      88          concatenate_431[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 622, 22)      0           batch_normalization_564[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_550 (Conv1D)             (None, 622, 2)       220         activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_564 (Dropout)           (None, 622, 2)       0           conv1d_550[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_432 (Concatenate)   (None, 622, 24)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 622, 24)      96          concatenate_432[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 622, 24)      0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_551 (Conv1D)             (None, 622, 2)       240         activation_565[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_565 (Dropout)           (None, 622, 2)       0           conv1d_551[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_433 (Concatenate)   (None, 622, 26)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 622, 26)      104         concatenate_433[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 622, 26)      0           batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_552 (Conv1D)             (None, 622, 2)       260         activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_566 (Dropout)           (None, 622, 2)       0           conv1d_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_434 (Concatenate)   (None, 622, 28)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_567 (BatchN (None, 622, 28)      112         concatenate_434[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 622, 28)      0           batch_normalization_567[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_553 (Conv1D)             (None, 622, 2)       280         activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_567 (Dropout)           (None, 622, 2)       0           conv1d_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_435 (Concatenate)   (None, 622, 30)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 622, 30)      120         concatenate_435[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_568 (Activation)     (None, 622, 30)      0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_554 (Conv1D)             (None, 622, 2)       300         activation_568[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_568 (Dropout)           (None, 622, 2)       0           conv1d_554[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_436 (Concatenate)   (None, 622, 32)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 622, 32)      128         concatenate_436[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_569 (Activation)     (None, 622, 32)      0           batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_555 (Conv1D)             (None, 622, 2)       320         activation_569[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_569 (Dropout)           (None, 622, 2)       0           conv1d_555[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_437 (Concatenate)   (None, 622, 34)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_570 (BatchN (None, 622, 34)      136         concatenate_437[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_570 (Activation)     (None, 622, 34)      0           batch_normalization_570[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_556 (Conv1D)             (None, 622, 2)       340         activation_570[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_570 (Dropout)           (None, 622, 2)       0           conv1d_556[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_438 (Concatenate)   (None, 622, 36)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 622, 36)      144         concatenate_438[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_571 (Activation)     (None, 622, 36)      0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_557 (Conv1D)             (None, 622, 2)       360         activation_571[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_571 (Dropout)           (None, 622, 2)       0           conv1d_557[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_439 (Concatenate)   (None, 622, 38)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 622, 38)      152         concatenate_439[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_572 (Activation)     (None, 622, 38)      0           batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_558 (Conv1D)             (None, 622, 2)       380         activation_572[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_572 (Dropout)           (None, 622, 2)       0           conv1d_558[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_440 (Concatenate)   (None, 622, 40)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_573 (BatchN (None, 622, 40)      160         concatenate_440[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_573 (Activation)     (None, 622, 40)      0           batch_normalization_573[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_559 (Conv1D)             (None, 622, 2)       400         activation_573[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_573 (Dropout)           (None, 622, 2)       0           conv1d_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_441 (Concatenate)   (None, 622, 42)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 622, 42)      168         concatenate_441[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_574 (Activation)     (None, 622, 42)      0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_560 (Conv1D)             (None, 622, 2)       420         activation_574[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_574 (Dropout)           (None, 622, 2)       0           conv1d_560[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_442 (Concatenate)   (None, 622, 44)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 622, 44)      176         concatenate_442[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_575 (Activation)     (None, 622, 44)      0           batch_normalization_575[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_561 (Conv1D)             (None, 622, 2)       440         activation_575[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_575 (Dropout)           (None, 622, 2)       0           conv1d_561[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_443 (Concatenate)   (None, 622, 46)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 622, 46)      184         concatenate_443[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_576 (Activation)     (None, 622, 46)      0           batch_normalization_576[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_562 (Conv1D)             (None, 622, 2)       460         activation_576[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_576 (Dropout)           (None, 622, 2)       0           conv1d_562[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_444 (Concatenate)   (None, 622, 48)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "                                                                 dropout_576[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_577 (BatchN (None, 622, 48)      192         concatenate_444[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_577 (Activation)     (None, 622, 48)      0           batch_normalization_577[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_563 (Conv1D)             (None, 622, 2)       480         activation_577[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_577 (Dropout)           (None, 622, 2)       0           conv1d_563[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_445 (Concatenate)   (None, 622, 50)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "                                                                 dropout_576[0][0]                \n",
      "                                                                 dropout_577[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 622, 50)      200         concatenate_445[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_578 (Activation)     (None, 622, 50)      0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_564 (Conv1D)             (None, 622, 2)       500         activation_578[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_578 (Dropout)           (None, 622, 2)       0           conv1d_564[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_446 (Concatenate)   (None, 622, 52)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "                                                                 dropout_576[0][0]                \n",
      "                                                                 dropout_577[0][0]                \n",
      "                                                                 dropout_578[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 622, 52)      208         concatenate_446[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_579 (Activation)     (None, 622, 52)      0           batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_565 (Conv1D)             (None, 622, 2)       520         activation_579[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_579 (Dropout)           (None, 622, 2)       0           conv1d_565[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_447 (Concatenate)   (None, 622, 54)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "                                                                 dropout_576[0][0]                \n",
      "                                                                 dropout_577[0][0]                \n",
      "                                                                 dropout_578[0][0]                \n",
      "                                                                 dropout_579[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 622, 54)      216         concatenate_447[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_580 (Activation)     (None, 622, 54)      0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_566 (Conv1D)             (None, 622, 2)       540         activation_580[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_580 (Dropout)           (None, 622, 2)       0           conv1d_566[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_448 (Concatenate)   (None, 622, 56)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "                                                                 dropout_576[0][0]                \n",
      "                                                                 dropout_577[0][0]                \n",
      "                                                                 dropout_578[0][0]                \n",
      "                                                                 dropout_579[0][0]                \n",
      "                                                                 dropout_580[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 622, 56)      224         concatenate_448[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_581 (Activation)     (None, 622, 56)      0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_567 (Conv1D)             (None, 622, 2)       560         activation_581[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_581 (Dropout)           (None, 622, 2)       0           conv1d_567[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_449 (Concatenate)   (None, 622, 58)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "                                                                 dropout_576[0][0]                \n",
      "                                                                 dropout_577[0][0]                \n",
      "                                                                 dropout_578[0][0]                \n",
      "                                                                 dropout_579[0][0]                \n",
      "                                                                 dropout_580[0][0]                \n",
      "                                                                 dropout_581[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 622, 58)      232         concatenate_449[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_582 (Activation)     (None, 622, 58)      0           batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_568 (Conv1D)             (None, 622, 2)       580         activation_582[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_582 (Dropout)           (None, 622, 2)       0           conv1d_568[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_450 (Concatenate)   (None, 622, 60)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "                                                                 dropout_576[0][0]                \n",
      "                                                                 dropout_577[0][0]                \n",
      "                                                                 dropout_578[0][0]                \n",
      "                                                                 dropout_579[0][0]                \n",
      "                                                                 dropout_580[0][0]                \n",
      "                                                                 dropout_581[0][0]                \n",
      "                                                                 dropout_582[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 622, 60)      240         concatenate_450[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_583 (Activation)     (None, 622, 60)      0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_569 (Conv1D)             (None, 622, 2)       600         activation_583[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_583 (Dropout)           (None, 622, 2)       0           conv1d_569[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_451 (Concatenate)   (None, 622, 62)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "                                                                 dropout_576[0][0]                \n",
      "                                                                 dropout_577[0][0]                \n",
      "                                                                 dropout_578[0][0]                \n",
      "                                                                 dropout_579[0][0]                \n",
      "                                                                 dropout_580[0][0]                \n",
      "                                                                 dropout_581[0][0]                \n",
      "                                                                 dropout_582[0][0]                \n",
      "                                                                 dropout_583[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 622, 62)      248         concatenate_451[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_584 (Activation)     (None, 622, 62)      0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_570 (Conv1D)             (None, 622, 2)       620         activation_584[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_584 (Dropout)           (None, 622, 2)       0           conv1d_570[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_452 (Concatenate)   (None, 622, 64)      0           concatenate_428[0][0]            \n",
      "                                                                 dropout_561[0][0]                \n",
      "                                                                 dropout_562[0][0]                \n",
      "                                                                 dropout_563[0][0]                \n",
      "                                                                 dropout_564[0][0]                \n",
      "                                                                 dropout_565[0][0]                \n",
      "                                                                 dropout_566[0][0]                \n",
      "                                                                 dropout_567[0][0]                \n",
      "                                                                 dropout_568[0][0]                \n",
      "                                                                 dropout_569[0][0]                \n",
      "                                                                 dropout_570[0][0]                \n",
      "                                                                 dropout_571[0][0]                \n",
      "                                                                 dropout_572[0][0]                \n",
      "                                                                 dropout_573[0][0]                \n",
      "                                                                 dropout_574[0][0]                \n",
      "                                                                 dropout_575[0][0]                \n",
      "                                                                 dropout_576[0][0]                \n",
      "                                                                 dropout_577[0][0]                \n",
      "                                                                 dropout_578[0][0]                \n",
      "                                                                 dropout_579[0][0]                \n",
      "                                                                 dropout_580[0][0]                \n",
      "                                                                 dropout_581[0][0]                \n",
      "                                                                 dropout_582[0][0]                \n",
      "                                                                 dropout_583[0][0]                \n",
      "                                                                 dropout_584[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 622, 64)      256         concatenate_452[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_585 (Activation)     (None, 622, 64)      0           batch_normalization_585[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 39808)        0           activation_585[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 20)           796160      flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_585 (Dropout)           (None, 20)           0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 2)            42          dropout_585[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 810,678\n",
      "Trainable params: 808,582\n",
      "Non-trainable params: 2,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 98s 79ms/step - loss: 0.8261 - acc: 0.8226 - val_loss: 0.6856 - val_acc: 0.5924\n",
      "6710/6710 [==============================] - 13s 2ms/step\n",
      "TN:37,FP:109,FN:8,TP:130,Macc:0.597726774429,F1:0.689650026957\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 114s 91ms/step - loss: 0.3435 - acc: 0.8385 - val_loss: 0.6359 - val_acc: 0.6796\n",
      "6710/6710 [==============================] - 4s 544us/step\n",
      "TN:77,FP:69,FN:27,TP:111,Macc:0.665872491255,F1:0.698107755233\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.3190 - acc: 0.8491 - val_loss: 0.6274 - val_acc: 0.6392\n",
      "6710/6710 [==============================] - 4s 584us/step\n",
      "TN:53,FP:93,FN:12,TP:126,Macc:0.638028538381,F1:0.705877092617\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.3096 - acc: 0.8569 - val_loss: 0.6472 - val_acc: 0.6080\n",
      "6710/6710 [==============================] - 4s 556us/step\n",
      "TN:42,FP:104,FN:4,TP:134,Macc:0.629342813278,F1:0.712760804813\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 89s 72ms/step - loss: 0.2945 - acc: 0.8722 - val_loss: 0.8288 - val_acc: 0.6724\n",
      "6710/6710 [==============================] - 4s 543us/step\n",
      "TN:113,FP:33,FN:66,TP:72,Macc:0.647855816637,F1:0.592587140424\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2860 - acc: 0.8766 - val_loss: 0.6827 - val_acc: 0.5963\n",
      "6710/6710 [==============================] - 4s 543us/step\n",
      "TN:42,FP:104,FN:6,TP:132,Macc:0.622096437044,F1:0.70587718909\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 89s 71ms/step - loss: 0.2815 - acc: 0.8794 - val_loss: 0.7567 - val_acc: 0.6915\n",
      "6710/6710 [==============================] - 4s 549us/step\n",
      "TN:110,FP:36,FN:67,TP:71,Macc:0.633958656691,F1:0.579586372795\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 89s 71ms/step - loss: 0.2802 - acc: 0.8795 - val_loss: 0.6904 - val_acc: 0.6952\n",
      "6710/6710 [==============================] - 4s 535us/step\n",
      "TN:94,FP:52,FN:43,TP:95,Macc:0.666120655079,F1:0.666661120735\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 89s 71ms/step - loss: 0.2740 - acc: 0.8831 - val_loss: 0.6047 - val_acc: 0.7045\n",
      "6710/6710 [==============================] - 4s 548us/step\n",
      "TN:81,FP:65,FN:36,TP:102,Macc:0.646962427307,F1:0.668846960539\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 89s 71ms/step - loss: 0.2725 - acc: 0.8844 - val_loss: 0.6493 - val_acc: 0.7094\n",
      "6710/6710 [==============================] - 4s 540us/step\n",
      "TN:95,FP:51,FN:49,TP:89,Macc:0.647806183654,F1:0.640282219446\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2698 - acc: 0.8853 - val_loss: 0.6564 - val_acc: 0.6429\n",
      "6710/6710 [==============================] - 4s 567us/step\n",
      "TN:55,FP:91,FN:9,TP:129,Macc:0.655747417284,F1:0.720665135364\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2678 - acc: 0.8877 - val_loss: 0.7343 - val_acc: 0.5729\n",
      "6710/6710 [==============================] - 4s 577us/step\n",
      "TN:34,FP:112,FN:0,TP:138,Macc:0.616438307537,F1:0.711335124172\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2648 - acc: 0.8873 - val_loss: 0.8443 - val_acc: 0.6858\n",
      "6710/6710 [==============================] - 4s 571us/step\n",
      "TN:112,FP:34,FN:63,TP:75,Macc:0.655300723712,F1:0.607281971167\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2632 - acc: 0.8877 - val_loss: 0.7318 - val_acc: 0.7009\n",
      "6710/6710 [==============================] - 4s 561us/step\n",
      "TN:107,FP:39,FN:52,TP:86,Macc:0.678032506617,F1:0.653986854215\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 89s 71ms/step - loss: 0.2629 - acc: 0.8879 - val_loss: 0.7715 - val_acc: 0.6928\n",
      "6710/6710 [==============================] - 4s 561us/step\n",
      "TN:103,FP:43,FN:57,TP:81,Macc:0.646217936927,F1:0.61831507452\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2621 - acc: 0.8887 - val_loss: 0.7038 - val_acc: 0.6884\n",
      "6710/6710 [==============================] - 4s 563us/step\n",
      "TN:98,FP:48,FN:57,TP:81,Macc:0.629094650546,F1:0.606736029339\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2599 - acc: 0.8885 - val_loss: 0.6627 - val_acc: 0.6396\n",
      "6710/6710 [==============================] - 4s 555us/step\n",
      "TN:51,FP:95,FN:7,TP:131,Macc:0.649295164413,F1:0.719774997773\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2573 - acc: 0.8895 - val_loss: 0.9516 - val_acc: 0.6730\n",
      "6710/6710 [==============================] - 4s 555us/step\n",
      "TN:113,FP:33,FN:65,TP:73,Macc:0.651479004754,F1:0.598355196434\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2590 - acc: 0.8891 - val_loss: 0.6815 - val_acc: 0.6337\n",
      "6710/6710 [==============================] - 4s 557us/step\n",
      "TN:51,FP:95,FN:9,TP:129,Macc:0.642048788179,F1:0.712701949459\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.2579 - acc: 0.8899 - val_loss: 0.6205 - val_acc: 0.7115\n",
      "6710/6710 [==============================] - 4s 538us/step\n",
      "TN:85,FP:61,FN:39,TP:99,Macc:0.649791492061,F1:0.664424011171\n",
      "Loss: 0\n",
      "args (20.0, 1, 8.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_61 (Con (None, 2500, 1)      31          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_62 (Con (None, 2500, 1)      31          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_63 (Con (None, 2500, 1)      31          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_64 (Con (None, 2500, 1)      31          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_571 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_61[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_573 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_62[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_575 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_63[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_577 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_64[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 2496, 8)      32          conv1d_571[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 2496, 8)      32          conv1d_573[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 2496, 8)      32          conv1d_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 2496, 8)      32          conv1d_577[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_586 (Activation)     (None, 2496, 8)      0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_588 (Activation)     (None, 2496, 8)      0           batch_normalization_588[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_590 (Activation)     (None, 2496, 8)      0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_592 (Activation)     (None, 2496, 8)      0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_586 (Dropout)           (None, 2496, 8)      0           activation_586[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_588 (Dropout)           (None, 2496, 8)      0           activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_590 (Dropout)           (None, 2496, 8)      0           activation_590[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_592 (Dropout)           (None, 2496, 8)      0           activation_592[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_121 (MaxPooling1D (None, 1248, 8)      0           dropout_586[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_123 (MaxPooling1D (None, 1248, 8)      0           dropout_588[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_125 (MaxPooling1D (None, 1248, 8)      0           dropout_590[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_127 (MaxPooling1D (None, 1248, 8)      0           dropout_592[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_572 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_121[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_574 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_123[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_576 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_125[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_578 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_127[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 1244, 4)      16          conv1d_572[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 1244, 4)      16          conv1d_574[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 1244, 4)      16          conv1d_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 1244, 4)      16          conv1d_578[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_587 (Activation)     (None, 1244, 4)      0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_589 (Activation)     (None, 1244, 4)      0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_591 (Activation)     (None, 1244, 4)      0           batch_normalization_591[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_593 (Activation)     (None, 1244, 4)      0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_587 (Dropout)           (None, 1244, 4)      0           activation_587[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_589 (Dropout)           (None, 1244, 4)      0           activation_589[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_591 (Dropout)           (None, 1244, 4)      0           activation_591[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_593 (Dropout)           (None, 1244, 4)      0           activation_593[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_122 (MaxPooling1D (None, 622, 4)       0           dropout_587[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_124 (MaxPooling1D (None, 622, 4)       0           dropout_589[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_126 (MaxPooling1D (None, 622, 4)       0           dropout_591[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_128 (MaxPooling1D (None, 622, 4)       0           dropout_593[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_453 (Concatenate)   (None, 622, 16)      0           max_pooling1d_122[0][0]          \n",
      "                                                                 max_pooling1d_124[0][0]          \n",
      "                                                                 max_pooling1d_126[0][0]          \n",
      "                                                                 max_pooling1d_128[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 622, 16)      64          concatenate_453[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_594 (Activation)     (None, 622, 16)      0           batch_normalization_594[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_579 (Conv1D)             (None, 622, 8)       640         activation_594[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_594 (Dropout)           (None, 622, 8)       0           conv1d_579[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_454 (Concatenate)   (None, 622, 24)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 622, 24)      96          concatenate_454[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_595 (Activation)     (None, 622, 24)      0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_580 (Conv1D)             (None, 622, 8)       960         activation_595[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_595 (Dropout)           (None, 622, 8)       0           conv1d_580[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_455 (Concatenate)   (None, 622, 32)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 622, 32)      128         concatenate_455[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_596 (Activation)     (None, 622, 32)      0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_581 (Conv1D)             (None, 622, 8)       1280        activation_596[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_596 (Dropout)           (None, 622, 8)       0           conv1d_581[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_456 (Concatenate)   (None, 622, 40)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 622, 40)      160         concatenate_456[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_597 (Activation)     (None, 622, 40)      0           batch_normalization_597[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_582 (Conv1D)             (None, 622, 8)       1600        activation_597[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_597 (Dropout)           (None, 622, 8)       0           conv1d_582[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_457 (Concatenate)   (None, 622, 48)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 622, 48)      192         concatenate_457[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_598 (Activation)     (None, 622, 48)      0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_583 (Conv1D)             (None, 622, 8)       1920        activation_598[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_598 (Dropout)           (None, 622, 8)       0           conv1d_583[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_458 (Concatenate)   (None, 622, 56)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 622, 56)      224         concatenate_458[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_599 (Activation)     (None, 622, 56)      0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_584 (Conv1D)             (None, 622, 8)       2240        activation_599[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_599 (Dropout)           (None, 622, 8)       0           conv1d_584[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_459 (Concatenate)   (None, 622, 64)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 622, 64)      256         concatenate_459[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_600 (Activation)     (None, 622, 64)      0           batch_normalization_600[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_585 (Conv1D)             (None, 622, 8)       2560        activation_600[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_600 (Dropout)           (None, 622, 8)       0           conv1d_585[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_460 (Concatenate)   (None, 622, 72)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 622, 72)      288         concatenate_460[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_601 (Activation)     (None, 622, 72)      0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_586 (Conv1D)             (None, 622, 8)       2880        activation_601[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_601 (Dropout)           (None, 622, 8)       0           conv1d_586[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_461 (Concatenate)   (None, 622, 80)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_602 (BatchN (None, 622, 80)      320         concatenate_461[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_602 (Activation)     (None, 622, 80)      0           batch_normalization_602[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_587 (Conv1D)             (None, 622, 8)       3200        activation_602[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_602 (Dropout)           (None, 622, 8)       0           conv1d_587[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_462 (Concatenate)   (None, 622, 88)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_603 (BatchN (None, 622, 88)      352         concatenate_462[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_603 (Activation)     (None, 622, 88)      0           batch_normalization_603[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_588 (Conv1D)             (None, 622, 8)       3520        activation_603[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_603 (Dropout)           (None, 622, 8)       0           conv1d_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_463 (Concatenate)   (None, 622, 96)      0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_604 (BatchN (None, 622, 96)      384         concatenate_463[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_604 (Activation)     (None, 622, 96)      0           batch_normalization_604[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_589 (Conv1D)             (None, 622, 8)       3840        activation_604[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_604 (Dropout)           (None, 622, 8)       0           conv1d_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_464 (Concatenate)   (None, 622, 104)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_605 (BatchN (None, 622, 104)     416         concatenate_464[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_605 (Activation)     (None, 622, 104)     0           batch_normalization_605[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_590 (Conv1D)             (None, 622, 8)       4160        activation_605[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_605 (Dropout)           (None, 622, 8)       0           conv1d_590[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_465 (Concatenate)   (None, 622, 112)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "                                                                 dropout_605[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_606 (BatchN (None, 622, 112)     448         concatenate_465[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_606 (Activation)     (None, 622, 112)     0           batch_normalization_606[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_591 (Conv1D)             (None, 622, 8)       4480        activation_606[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_606 (Dropout)           (None, 622, 8)       0           conv1d_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_466 (Concatenate)   (None, 622, 120)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "                                                                 dropout_605[0][0]                \n",
      "                                                                 dropout_606[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_607 (BatchN (None, 622, 120)     480         concatenate_466[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_607 (Activation)     (None, 622, 120)     0           batch_normalization_607[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_592 (Conv1D)             (None, 622, 8)       4800        activation_607[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_607 (Dropout)           (None, 622, 8)       0           conv1d_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_467 (Concatenate)   (None, 622, 128)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "                                                                 dropout_605[0][0]                \n",
      "                                                                 dropout_606[0][0]                \n",
      "                                                                 dropout_607[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 622, 128)     512         concatenate_467[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_608 (Activation)     (None, 622, 128)     0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_593 (Conv1D)             (None, 622, 8)       5120        activation_608[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_608 (Dropout)           (None, 622, 8)       0           conv1d_593[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_468 (Concatenate)   (None, 622, 136)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "                                                                 dropout_605[0][0]                \n",
      "                                                                 dropout_606[0][0]                \n",
      "                                                                 dropout_607[0][0]                \n",
      "                                                                 dropout_608[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 622, 136)     544         concatenate_468[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_609 (Activation)     (None, 622, 136)     0           batch_normalization_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_594 (Conv1D)             (None, 622, 8)       5440        activation_609[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_609 (Dropout)           (None, 622, 8)       0           conv1d_594[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_469 (Concatenate)   (None, 622, 144)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "                                                                 dropout_605[0][0]                \n",
      "                                                                 dropout_606[0][0]                \n",
      "                                                                 dropout_607[0][0]                \n",
      "                                                                 dropout_608[0][0]                \n",
      "                                                                 dropout_609[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_610 (BatchN (None, 622, 144)     576         concatenate_469[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_610 (Activation)     (None, 622, 144)     0           batch_normalization_610[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_595 (Conv1D)             (None, 622, 8)       5760        activation_610[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_610 (Dropout)           (None, 622, 8)       0           conv1d_595[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_470 (Concatenate)   (None, 622, 152)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "                                                                 dropout_605[0][0]                \n",
      "                                                                 dropout_606[0][0]                \n",
      "                                                                 dropout_607[0][0]                \n",
      "                                                                 dropout_608[0][0]                \n",
      "                                                                 dropout_609[0][0]                \n",
      "                                                                 dropout_610[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_611 (BatchN (None, 622, 152)     608         concatenate_470[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_611 (Activation)     (None, 622, 152)     0           batch_normalization_611[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_596 (Conv1D)             (None, 622, 8)       6080        activation_611[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_611 (Dropout)           (None, 622, 8)       0           conv1d_596[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_471 (Concatenate)   (None, 622, 160)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "                                                                 dropout_605[0][0]                \n",
      "                                                                 dropout_606[0][0]                \n",
      "                                                                 dropout_607[0][0]                \n",
      "                                                                 dropout_608[0][0]                \n",
      "                                                                 dropout_609[0][0]                \n",
      "                                                                 dropout_610[0][0]                \n",
      "                                                                 dropout_611[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_612 (BatchN (None, 622, 160)     640         concatenate_471[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_612 (Activation)     (None, 622, 160)     0           batch_normalization_612[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_597 (Conv1D)             (None, 622, 8)       6400        activation_612[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_612 (Dropout)           (None, 622, 8)       0           conv1d_597[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_472 (Concatenate)   (None, 622, 168)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "                                                                 dropout_605[0][0]                \n",
      "                                                                 dropout_606[0][0]                \n",
      "                                                                 dropout_607[0][0]                \n",
      "                                                                 dropout_608[0][0]                \n",
      "                                                                 dropout_609[0][0]                \n",
      "                                                                 dropout_610[0][0]                \n",
      "                                                                 dropout_611[0][0]                \n",
      "                                                                 dropout_612[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_613 (BatchN (None, 622, 168)     672         concatenate_472[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_613 (Activation)     (None, 622, 168)     0           batch_normalization_613[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_598 (Conv1D)             (None, 622, 8)       6720        activation_613[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_613 (Dropout)           (None, 622, 8)       0           conv1d_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_473 (Concatenate)   (None, 622, 176)     0           concatenate_453[0][0]            \n",
      "                                                                 dropout_594[0][0]                \n",
      "                                                                 dropout_595[0][0]                \n",
      "                                                                 dropout_596[0][0]                \n",
      "                                                                 dropout_597[0][0]                \n",
      "                                                                 dropout_598[0][0]                \n",
      "                                                                 dropout_599[0][0]                \n",
      "                                                                 dropout_600[0][0]                \n",
      "                                                                 dropout_601[0][0]                \n",
      "                                                                 dropout_602[0][0]                \n",
      "                                                                 dropout_603[0][0]                \n",
      "                                                                 dropout_604[0][0]                \n",
      "                                                                 dropout_605[0][0]                \n",
      "                                                                 dropout_606[0][0]                \n",
      "                                                                 dropout_607[0][0]                \n",
      "                                                                 dropout_608[0][0]                \n",
      "                                                                 dropout_609[0][0]                \n",
      "                                                                 dropout_610[0][0]                \n",
      "                                                                 dropout_611[0][0]                \n",
      "                                                                 dropout_612[0][0]                \n",
      "                                                                 dropout_613[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_614 (BatchN (None, 622, 176)     704         concatenate_473[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_614 (Activation)     (None, 622, 176)     0           batch_normalization_614[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 109472)       0           activation_614[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 20)           2189440     flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_614 (Dropout)           (None, 20)           0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 2)            42          dropout_614[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,272,262\n",
      "Trainable params: 2,268,134\n",
      "Non-trainable params: 4,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 137s 110ms/step - loss: 3.5015 - acc: 0.8114 - val_loss: 7.2414 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 17s 3ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 124s 100ms/step - loss: 1.3113 - acc: 0.8128 - val_loss: 0.6355 - val_acc: 0.6396\n",
      "6710/6710 [==============================] - 5s 787us/step\n",
      "TN:46,FP:100,FN:6,TP:132,Macc:0.635795066149,F1:0.713508326114\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 118s 94ms/step - loss: 0.3124 - acc: 0.8334 - val_loss: 0.5818 - val_acc: 0.6937\n",
      "6710/6710 [==============================] - 5s 812us/step\n",
      "TN:62,FP:84,FN:12,TP:126,Macc:0.668850453867,F1:0.724132620728\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.3009 - acc: 0.8423 - val_loss: 0.5660 - val_acc: 0.7337\n",
      "6710/6710 [==============================] - 5s 793us/step\n",
      "TN:90,FP:56,FN:31,TP:107,Macc:0.695900283378,F1:0.710957941168\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 118s 94ms/step - loss: 0.2949 - acc: 0.8501 - val_loss: 0.5388 - val_acc: 0.7289\n",
      "6710/6710 [==============================] - 5s 795us/step\n",
      "TN:75,FP:71,FN:18,TP:120,Macc:0.691631869756,F1:0.729477876666\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2864 - acc: 0.8620 - val_loss: 0.5771 - val_acc: 0.6815\n",
      "6710/6710 [==============================] - 5s 800us/step\n",
      "TN:59,FP:87,FN:6,TP:132,Macc:0.68031561074,F1:0.739490535922\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2756 - acc: 0.8764 - val_loss: 0.6167 - val_acc: 0.6550\n",
      "6710/6710 [==============================] - 5s 810us/step\n",
      "TN:58,FP:88,FN:10,TP:128,Macc:0.662398200996,F1:0.723158563924\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2712 - acc: 0.8799 - val_loss: 0.5496 - val_acc: 0.7040\n",
      "6710/6710 [==============================] - 5s 793us/step\n",
      "TN:77,FP:69,FN:24,TP:114,Macc:0.676742055606,F1:0.710274933281\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2634 - acc: 0.8829 - val_loss: 0.5928 - val_acc: 0.6785\n",
      "6710/6710 [==============================] - 6s 825us/step\n",
      "TN:61,FP:85,FN:10,TP:128,Macc:0.672672172825,F1:0.729339434783\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2598 - acc: 0.8849 - val_loss: 0.6534 - val_acc: 0.6393\n",
      "6710/6710 [==============================] - 5s 802us/step\n",
      "TN:56,FP:90,FN:19,TP:119,Macc:0.622940193391,F1:0.685873649352\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2599 - acc: 0.8856 - val_loss: 0.7027 - val_acc: 0.6103\n",
      "6710/6710 [==============================] - 5s 810us/step\n",
      "TN:44,FP:102,FN:2,TP:136,Macc:0.643438504065,F1:0.723399102062\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2556 - acc: 0.8878 - val_loss: 0.6979 - val_acc: 0.6188\n",
      "6710/6710 [==============================] - 5s 781us/step\n",
      "TN:47,FP:99,FN:10,TP:128,Macc:0.624726970958,F1:0.701364647784\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2530 - acc: 0.8883 - val_loss: 0.6009 - val_acc: 0.6796\n",
      "6710/6710 [==============================] - 5s 806us/step\n",
      "TN:68,FP:78,FN:19,TP:119,Macc:0.664036080705,F1:0.710442385177\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2493 - acc: 0.8904 - val_loss: 0.6119 - val_acc: 0.6861\n",
      "6710/6710 [==============================] - 5s 762us/step\n",
      "TN:61,FP:85,FN:5,TP:133,Macc:0.69078811341,F1:0.74718574284\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2490 - acc: 0.8897 - val_loss: 0.6448 - val_acc: 0.6614\n",
      "6710/6710 [==============================] - 5s 780us/step\n",
      "TN:55,FP:91,FN:16,TP:122,Macc:0.630385100465,F1:0.695151402739\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2462 - acc: 0.8919 - val_loss: 0.5178 - val_acc: 0.7270\n",
      "6710/6710 [==============================] - 5s 796us/step\n",
      "TN:73,FP:73,FN:19,TP:119,Macc:0.681159367086,F1:0.721206720444\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2452 - acc: 0.8911 - val_loss: 0.6066 - val_acc: 0.6797\n",
      "6710/6710 [==============================] - 5s 814us/step\n",
      "TN:61,FP:85,FN:14,TP:124,Macc:0.658179420357,F1:0.714692091328\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2441 - acc: 0.8931 - val_loss: 0.6088 - val_acc: 0.6905\n",
      "6710/6710 [==============================] - 5s 807us/step\n",
      "TN:60,FP:86,FN:8,TP:130,Macc:0.676493891782,F1:0.73445799825\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2423 - acc: 0.8935 - val_loss: 0.5593 - val_acc: 0.7156\n",
      "6710/6710 [==============================] - 5s 755us/step\n",
      "TN:79,FP:67,FN:34,TP:104,Macc:0.647359488989,F1:0.673133673424\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 118s 95ms/step - loss: 0.2401 - acc: 0.8943 - val_loss: 0.6541 - val_acc: 0.6700\n",
      "6710/6710 [==============================] - 5s 779us/step\n",
      "TN:58,FP:88,FN:11,TP:127,Macc:0.658775012879,F1:0.719541459098\n",
      "Loss: 0\n",
      "args (11.0, 3, 11.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_65 (Con (None, 2500, 1)      31          input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_66 (Con (None, 2500, 1)      31          input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_67 (Con (None, 2500, 1)      31          input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_68 (Con (None, 2500, 1)      31          input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_599 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_65[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_601 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_66[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_603 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_67[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_605 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_68[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_615 (BatchN (None, 2496, 8)      32          conv1d_599[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_617 (BatchN (None, 2496, 8)      32          conv1d_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_619 (BatchN (None, 2496, 8)      32          conv1d_603[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_621 (BatchN (None, 2496, 8)      32          conv1d_605[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 2496, 8)      0           batch_normalization_615[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 2496, 8)      0           batch_normalization_617[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 2496, 8)      0           batch_normalization_619[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 2496, 8)      0           batch_normalization_621[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_615 (Dropout)           (None, 2496, 8)      0           activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_617 (Dropout)           (None, 2496, 8)      0           activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_619 (Dropout)           (None, 2496, 8)      0           activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_621 (Dropout)           (None, 2496, 8)      0           activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_129 (MaxPooling1D (None, 1248, 8)      0           dropout_615[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_131 (MaxPooling1D (None, 1248, 8)      0           dropout_617[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_133 (MaxPooling1D (None, 1248, 8)      0           dropout_619[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_135 (MaxPooling1D (None, 1248, 8)      0           dropout_621[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_600 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_129[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_602 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_131[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_604 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_133[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_606 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_135[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_616 (BatchN (None, 1244, 4)      16          conv1d_600[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_618 (BatchN (None, 1244, 4)      16          conv1d_602[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_620 (BatchN (None, 1244, 4)      16          conv1d_604[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_622 (BatchN (None, 1244, 4)      16          conv1d_606[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 1244, 4)      0           batch_normalization_616[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 1244, 4)      0           batch_normalization_618[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 1244, 4)      0           batch_normalization_620[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 1244, 4)      0           batch_normalization_622[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_616 (Dropout)           (None, 1244, 4)      0           activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_618 (Dropout)           (None, 1244, 4)      0           activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_620 (Dropout)           (None, 1244, 4)      0           activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_622 (Dropout)           (None, 1244, 4)      0           activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_130 (MaxPooling1D (None, 622, 4)       0           dropout_616[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_132 (MaxPooling1D (None, 622, 4)       0           dropout_618[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_134 (MaxPooling1D (None, 622, 4)       0           dropout_620[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_136 (MaxPooling1D (None, 622, 4)       0           dropout_622[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_474 (Concatenate)   (None, 622, 16)      0           max_pooling1d_130[0][0]          \n",
      "                                                                 max_pooling1d_132[0][0]          \n",
      "                                                                 max_pooling1d_134[0][0]          \n",
      "                                                                 max_pooling1d_136[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_623 (BatchN (None, 622, 16)      64          concatenate_474[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 622, 16)      0           batch_normalization_623[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_607 (Conv1D)             (None, 622, 11)      880         activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_623 (Dropout)           (None, 622, 11)      0           conv1d_607[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_475 (Concatenate)   (None, 622, 27)      0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_624 (BatchN (None, 622, 27)      108         concatenate_475[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 622, 27)      0           batch_normalization_624[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_608 (Conv1D)             (None, 622, 11)      1485        activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_624 (Dropout)           (None, 622, 11)      0           conv1d_608[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_476 (Concatenate)   (None, 622, 38)      0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_625 (BatchN (None, 622, 38)      152         concatenate_476[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 622, 38)      0           batch_normalization_625[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_609 (Conv1D)             (None, 622, 11)      2090        activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_625 (Dropout)           (None, 622, 11)      0           conv1d_609[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_477 (Concatenate)   (None, 622, 49)      0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "                                                                 dropout_625[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_626 (BatchN (None, 622, 49)      196         concatenate_477[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 622, 49)      0           batch_normalization_626[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_610 (Conv1D)             (None, 622, 11)      2695        activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_626 (Dropout)           (None, 622, 11)      0           conv1d_610[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_478 (Concatenate)   (None, 622, 60)      0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "                                                                 dropout_625[0][0]                \n",
      "                                                                 dropout_626[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_627 (BatchN (None, 622, 60)      240         concatenate_478[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 622, 60)      0           batch_normalization_627[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_611 (Conv1D)             (None, 622, 11)      3300        activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_627 (Dropout)           (None, 622, 11)      0           conv1d_611[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_479 (Concatenate)   (None, 622, 71)      0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "                                                                 dropout_625[0][0]                \n",
      "                                                                 dropout_626[0][0]                \n",
      "                                                                 dropout_627[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, 622, 71)      284         concatenate_479[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 622, 71)      0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_612 (Conv1D)             (None, 622, 11)      3905        activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_628 (Dropout)           (None, 622, 11)      0           conv1d_612[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_480 (Concatenate)   (None, 622, 82)      0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "                                                                 dropout_625[0][0]                \n",
      "                                                                 dropout_626[0][0]                \n",
      "                                                                 dropout_627[0][0]                \n",
      "                                                                 dropout_628[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, 622, 82)      328         concatenate_480[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 622, 82)      0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_613 (Conv1D)             (None, 622, 11)      4510        activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_629 (Dropout)           (None, 622, 11)      0           conv1d_613[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_481 (Concatenate)   (None, 622, 93)      0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "                                                                 dropout_625[0][0]                \n",
      "                                                                 dropout_626[0][0]                \n",
      "                                                                 dropout_627[0][0]                \n",
      "                                                                 dropout_628[0][0]                \n",
      "                                                                 dropout_629[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, 622, 93)      372         concatenate_481[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 622, 93)      0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_614 (Conv1D)             (None, 622, 11)      5115        activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_630 (Dropout)           (None, 622, 11)      0           conv1d_614[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_482 (Concatenate)   (None, 622, 104)     0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "                                                                 dropout_625[0][0]                \n",
      "                                                                 dropout_626[0][0]                \n",
      "                                                                 dropout_627[0][0]                \n",
      "                                                                 dropout_628[0][0]                \n",
      "                                                                 dropout_629[0][0]                \n",
      "                                                                 dropout_630[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, 622, 104)     416         concatenate_482[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 622, 104)     0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_615 (Conv1D)             (None, 622, 11)      5720        activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_631 (Dropout)           (None, 622, 11)      0           conv1d_615[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_483 (Concatenate)   (None, 622, 115)     0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "                                                                 dropout_625[0][0]                \n",
      "                                                                 dropout_626[0][0]                \n",
      "                                                                 dropout_627[0][0]                \n",
      "                                                                 dropout_628[0][0]                \n",
      "                                                                 dropout_629[0][0]                \n",
      "                                                                 dropout_630[0][0]                \n",
      "                                                                 dropout_631[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, 622, 115)     460         concatenate_483[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 622, 115)     0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_616 (Conv1D)             (None, 622, 11)      6325        activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_632 (Dropout)           (None, 622, 11)      0           conv1d_616[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_484 (Concatenate)   (None, 622, 126)     0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "                                                                 dropout_625[0][0]                \n",
      "                                                                 dropout_626[0][0]                \n",
      "                                                                 dropout_627[0][0]                \n",
      "                                                                 dropout_628[0][0]                \n",
      "                                                                 dropout_629[0][0]                \n",
      "                                                                 dropout_630[0][0]                \n",
      "                                                                 dropout_631[0][0]                \n",
      "                                                                 dropout_632[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, 622, 126)     504         concatenate_484[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 622, 126)     0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_617 (Conv1D)             (None, 622, 11)      6930        activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_633 (Dropout)           (None, 622, 11)      0           conv1d_617[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_485 (Concatenate)   (None, 622, 137)     0           concatenate_474[0][0]            \n",
      "                                                                 dropout_623[0][0]                \n",
      "                                                                 dropout_624[0][0]                \n",
      "                                                                 dropout_625[0][0]                \n",
      "                                                                 dropout_626[0][0]                \n",
      "                                                                 dropout_627[0][0]                \n",
      "                                                                 dropout_628[0][0]                \n",
      "                                                                 dropout_629[0][0]                \n",
      "                                                                 dropout_630[0][0]                \n",
      "                                                                 dropout_631[0][0]                \n",
      "                                                                 dropout_632[0][0]                \n",
      "                                                                 dropout_633[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, 622, 137)     548         concatenate_485[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 622, 137)     0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_618 (Conv1D)             (None, 622, 137)     93845       activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_634 (Dropout)           (None, 622, 137)     0           conv1d_618[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_14 (AveragePo (None, 311, 137)     0           dropout_634[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, 311, 137)     548         average_pooling1d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 311, 137)     0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_619 (Conv1D)             (None, 311, 11)      7535        activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_635 (Dropout)           (None, 311, 11)      0           conv1d_619[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_486 (Concatenate)   (None, 311, 148)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, 311, 148)     592         concatenate_486[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 311, 148)     0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_620 (Conv1D)             (None, 311, 11)      8140        activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_636 (Dropout)           (None, 311, 11)      0           conv1d_620[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_487 (Concatenate)   (None, 311, 159)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, 311, 159)     636         concatenate_487[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 311, 159)     0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_621 (Conv1D)             (None, 311, 11)      8745        activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_637 (Dropout)           (None, 311, 11)      0           conv1d_621[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_488 (Concatenate)   (None, 311, 170)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "                                                                 dropout_637[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, 311, 170)     680         concatenate_488[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 311, 170)     0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_622 (Conv1D)             (None, 311, 11)      9350        activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_638 (Dropout)           (None, 311, 11)      0           conv1d_622[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_489 (Concatenate)   (None, 311, 181)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "                                                                 dropout_637[0][0]                \n",
      "                                                                 dropout_638[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, 311, 181)     724         concatenate_489[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 311, 181)     0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_623 (Conv1D)             (None, 311, 11)      9955        activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_639 (Dropout)           (None, 311, 11)      0           conv1d_623[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_490 (Concatenate)   (None, 311, 192)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "                                                                 dropout_637[0][0]                \n",
      "                                                                 dropout_638[0][0]                \n",
      "                                                                 dropout_639[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, 311, 192)     768         concatenate_490[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 311, 192)     0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_624 (Conv1D)             (None, 311, 11)      10560       activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_640 (Dropout)           (None, 311, 11)      0           conv1d_624[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_491 (Concatenate)   (None, 311, 203)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "                                                                 dropout_637[0][0]                \n",
      "                                                                 dropout_638[0][0]                \n",
      "                                                                 dropout_639[0][0]                \n",
      "                                                                 dropout_640[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, 311, 203)     812         concatenate_491[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 311, 203)     0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_625 (Conv1D)             (None, 311, 11)      11165       activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_641 (Dropout)           (None, 311, 11)      0           conv1d_625[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_492 (Concatenate)   (None, 311, 214)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "                                                                 dropout_637[0][0]                \n",
      "                                                                 dropout_638[0][0]                \n",
      "                                                                 dropout_639[0][0]                \n",
      "                                                                 dropout_640[0][0]                \n",
      "                                                                 dropout_641[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, 311, 214)     856         concatenate_492[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 311, 214)     0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_626 (Conv1D)             (None, 311, 11)      11770       activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_642 (Dropout)           (None, 311, 11)      0           conv1d_626[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_493 (Concatenate)   (None, 311, 225)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "                                                                 dropout_637[0][0]                \n",
      "                                                                 dropout_638[0][0]                \n",
      "                                                                 dropout_639[0][0]                \n",
      "                                                                 dropout_640[0][0]                \n",
      "                                                                 dropout_641[0][0]                \n",
      "                                                                 dropout_642[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_643 (BatchN (None, 311, 225)     900         concatenate_493[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, 311, 225)     0           batch_normalization_643[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_627 (Conv1D)             (None, 311, 11)      12375       activation_643[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_643 (Dropout)           (None, 311, 11)      0           conv1d_627[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_494 (Concatenate)   (None, 311, 236)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "                                                                 dropout_637[0][0]                \n",
      "                                                                 dropout_638[0][0]                \n",
      "                                                                 dropout_639[0][0]                \n",
      "                                                                 dropout_640[0][0]                \n",
      "                                                                 dropout_641[0][0]                \n",
      "                                                                 dropout_642[0][0]                \n",
      "                                                                 dropout_643[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_644 (BatchN (None, 311, 236)     944         concatenate_494[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, 311, 236)     0           batch_normalization_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_628 (Conv1D)             (None, 311, 11)      12980       activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_644 (Dropout)           (None, 311, 11)      0           conv1d_628[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_495 (Concatenate)   (None, 311, 247)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "                                                                 dropout_637[0][0]                \n",
      "                                                                 dropout_638[0][0]                \n",
      "                                                                 dropout_639[0][0]                \n",
      "                                                                 dropout_640[0][0]                \n",
      "                                                                 dropout_641[0][0]                \n",
      "                                                                 dropout_642[0][0]                \n",
      "                                                                 dropout_643[0][0]                \n",
      "                                                                 dropout_644[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_645 (BatchN (None, 311, 247)     988         concatenate_495[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, 311, 247)     0           batch_normalization_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_629 (Conv1D)             (None, 311, 11)      13585       activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_645 (Dropout)           (None, 311, 11)      0           conv1d_629[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_496 (Concatenate)   (None, 311, 258)     0           average_pooling1d_14[0][0]       \n",
      "                                                                 dropout_635[0][0]                \n",
      "                                                                 dropout_636[0][0]                \n",
      "                                                                 dropout_637[0][0]                \n",
      "                                                                 dropout_638[0][0]                \n",
      "                                                                 dropout_639[0][0]                \n",
      "                                                                 dropout_640[0][0]                \n",
      "                                                                 dropout_641[0][0]                \n",
      "                                                                 dropout_642[0][0]                \n",
      "                                                                 dropout_643[0][0]                \n",
      "                                                                 dropout_644[0][0]                \n",
      "                                                                 dropout_645[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_646 (BatchN (None, 311, 258)     1032        concatenate_496[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, 311, 258)     0           batch_normalization_646[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_630 (Conv1D)             (None, 311, 258)     332820      activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_646 (Dropout)           (None, 311, 258)     0           conv1d_630[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_15 (AveragePo (None, 155, 258)     0           dropout_646[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_647 (BatchN (None, 155, 258)     1032        average_pooling1d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, 155, 258)     0           batch_normalization_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_631 (Conv1D)             (None, 155, 11)      14190       activation_647[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_647 (Dropout)           (None, 155, 11)      0           conv1d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_497 (Concatenate)   (None, 155, 269)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_648 (BatchN (None, 155, 269)     1076        concatenate_497[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_648 (Activation)     (None, 155, 269)     0           batch_normalization_648[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_632 (Conv1D)             (None, 155, 11)      14795       activation_648[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_648 (Dropout)           (None, 155, 11)      0           conv1d_632[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_498 (Concatenate)   (None, 155, 280)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_649 (BatchN (None, 155, 280)     1120        concatenate_498[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_649 (Activation)     (None, 155, 280)     0           batch_normalization_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_633 (Conv1D)             (None, 155, 11)      15400       activation_649[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_649 (Dropout)           (None, 155, 11)      0           conv1d_633[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_499 (Concatenate)   (None, 155, 291)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "                                                                 dropout_649[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, 155, 291)     1164        concatenate_499[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_650 (Activation)     (None, 155, 291)     0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_634 (Conv1D)             (None, 155, 11)      16005       activation_650[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_650 (Dropout)           (None, 155, 11)      0           conv1d_634[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_500 (Concatenate)   (None, 155, 302)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "                                                                 dropout_649[0][0]                \n",
      "                                                                 dropout_650[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, 155, 302)     1208        concatenate_500[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_651 (Activation)     (None, 155, 302)     0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_635 (Conv1D)             (None, 155, 11)      16610       activation_651[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_651 (Dropout)           (None, 155, 11)      0           conv1d_635[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_501 (Concatenate)   (None, 155, 313)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "                                                                 dropout_649[0][0]                \n",
      "                                                                 dropout_650[0][0]                \n",
      "                                                                 dropout_651[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, 155, 313)     1252        concatenate_501[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_652 (Activation)     (None, 155, 313)     0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_636 (Conv1D)             (None, 155, 11)      17215       activation_652[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_652 (Dropout)           (None, 155, 11)      0           conv1d_636[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_502 (Concatenate)   (None, 155, 324)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "                                                                 dropout_649[0][0]                \n",
      "                                                                 dropout_650[0][0]                \n",
      "                                                                 dropout_651[0][0]                \n",
      "                                                                 dropout_652[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, 155, 324)     1296        concatenate_502[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_653 (Activation)     (None, 155, 324)     0           batch_normalization_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_637 (Conv1D)             (None, 155, 11)      17820       activation_653[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_653 (Dropout)           (None, 155, 11)      0           conv1d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_503 (Concatenate)   (None, 155, 335)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "                                                                 dropout_649[0][0]                \n",
      "                                                                 dropout_650[0][0]                \n",
      "                                                                 dropout_651[0][0]                \n",
      "                                                                 dropout_652[0][0]                \n",
      "                                                                 dropout_653[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, 155, 335)     1340        concatenate_503[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_654 (Activation)     (None, 155, 335)     0           batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_638 (Conv1D)             (None, 155, 11)      18425       activation_654[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_654 (Dropout)           (None, 155, 11)      0           conv1d_638[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_504 (Concatenate)   (None, 155, 346)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "                                                                 dropout_649[0][0]                \n",
      "                                                                 dropout_650[0][0]                \n",
      "                                                                 dropout_651[0][0]                \n",
      "                                                                 dropout_652[0][0]                \n",
      "                                                                 dropout_653[0][0]                \n",
      "                                                                 dropout_654[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, 155, 346)     1384        concatenate_504[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_655 (Activation)     (None, 155, 346)     0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_639 (Conv1D)             (None, 155, 11)      19030       activation_655[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_655 (Dropout)           (None, 155, 11)      0           conv1d_639[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_505 (Concatenate)   (None, 155, 357)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "                                                                 dropout_649[0][0]                \n",
      "                                                                 dropout_650[0][0]                \n",
      "                                                                 dropout_651[0][0]                \n",
      "                                                                 dropout_652[0][0]                \n",
      "                                                                 dropout_653[0][0]                \n",
      "                                                                 dropout_654[0][0]                \n",
      "                                                                 dropout_655[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 155, 357)     1428        concatenate_505[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_656 (Activation)     (None, 155, 357)     0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_640 (Conv1D)             (None, 155, 11)      19635       activation_656[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_656 (Dropout)           (None, 155, 11)      0           conv1d_640[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_506 (Concatenate)   (None, 155, 368)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "                                                                 dropout_649[0][0]                \n",
      "                                                                 dropout_650[0][0]                \n",
      "                                                                 dropout_651[0][0]                \n",
      "                                                                 dropout_652[0][0]                \n",
      "                                                                 dropout_653[0][0]                \n",
      "                                                                 dropout_654[0][0]                \n",
      "                                                                 dropout_655[0][0]                \n",
      "                                                                 dropout_656[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 155, 368)     1472        concatenate_506[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_657 (Activation)     (None, 155, 368)     0           batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_641 (Conv1D)             (None, 155, 11)      20240       activation_657[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_657 (Dropout)           (None, 155, 11)      0           conv1d_641[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_507 (Concatenate)   (None, 155, 379)     0           average_pooling1d_15[0][0]       \n",
      "                                                                 dropout_647[0][0]                \n",
      "                                                                 dropout_648[0][0]                \n",
      "                                                                 dropout_649[0][0]                \n",
      "                                                                 dropout_650[0][0]                \n",
      "                                                                 dropout_651[0][0]                \n",
      "                                                                 dropout_652[0][0]                \n",
      "                                                                 dropout_653[0][0]                \n",
      "                                                                 dropout_654[0][0]                \n",
      "                                                                 dropout_655[0][0]                \n",
      "                                                                 dropout_656[0][0]                \n",
      "                                                                 dropout_657[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 155, 379)     1516        concatenate_507[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_658 (Activation)     (None, 155, 379)     0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 58745)        0           activation_658[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 20)           1174900     flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_658 (Dropout)           (None, 20)           0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 2)            42          dropout_658[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,979,643\n",
      "Trainable params: 1,965,327\n",
      "Non-trainable params: 14,316\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 207s 166ms/step - loss: 3.4910 - acc: 0.8103 - val_loss: 0.8397 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 19s 3ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 191s 153ms/step - loss: 0.3711 - acc: 0.8426 - val_loss: 0.7087 - val_acc: 0.5352\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:23,FP:123,FN:0,TP:138,Macc:0.578767077498,F1:0.691724307873\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 191s 154ms/step - loss: 0.3081 - acc: 0.8655 - val_loss: 0.6815 - val_acc: 0.6019\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:47,FP:99,FN:11,TP:127,Macc:0.621103782841,F1:0.697796977124\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 193s 154ms/step - loss: 0.2884 - acc: 0.8727 - val_loss: 0.6691 - val_acc: 0.6437\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:66,FP:80,FN:28,TP:110,Macc:0.6245780731,F1:0.670726300608\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 192s 154ms/step - loss: 0.2787 - acc: 0.8766 - val_loss: 0.6995 - val_acc: 0.5931\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:36,FP:110,FN:8,TP:130,Macc:0.594302117153,F1:0.687825548315\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 191s 153ms/step - loss: 0.2678 - acc: 0.8842 - val_loss: 0.7068 - val_acc: 0.6136\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:49,FP:97,FN:13,TP:125,Macc:0.620706721159,F1:0.69443920149\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 191s 153ms/step - loss: 0.2636 - acc: 0.8855 - val_loss: 0.7433 - val_acc: 0.6803\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:87,FP:59,FN:44,TP:94,Macc:0.638524866029,F1:0.646042575784\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 192s 154ms/step - loss: 0.2589 - acc: 0.8889 - val_loss: 0.7352 - val_acc: 0.6404\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:68,FP:78,FN:32,TP:106,Macc:0.616934635184,F1:0.658379660474\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 191s 153ms/step - loss: 0.2543 - acc: 0.8918 - val_loss: 0.7574 - val_acc: 0.6590\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:82,FP:64,FN:46,TP:92,Macc:0.614155203414,F1:0.625844813968\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 191s 153ms/step - loss: 0.2468 - acc: 0.8971 - val_loss: 0.7461 - val_acc: 0.6778\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:79,FP:67,FN:40,TP:98,Macc:0.625620360287,F1:0.64685918322\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 192s 154ms/step - loss: 0.2430 - acc: 0.8998 - val_loss: 0.6363 - val_acc: 0.7012\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:69,FP:77,FN:19,TP:119,Macc:0.667460737982,F1:0.712569469257\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 192s 154ms/step - loss: 0.2401 - acc: 0.9002 - val_loss: 0.6351 - val_acc: 0.7118\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:77,FP:69,FN:30,TP:108,Macc:0.655002926904,F1:0.685708822174\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 191s 153ms/step - loss: 0.2350 - acc: 0.9034 - val_loss: 0.7418 - val_acc: 0.7171\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:91,FP:55,FN:47,TP:91,Macc:0.641353930783,F1:0.640839525191\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 192s 154ms/step - loss: 0.2335 - acc: 0.9036 - val_loss: 1.0432 - val_acc: 0.5939\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:145,FP:1,FN:122,TP:16,Macc:0.554546314924,F1:0.20644943536\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 191s 153ms/step - loss: 0.2313 - acc: 0.9045 - val_loss: 0.7989 - val_acc: 0.6692\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:123,FP:23,FN:84,TP:54,Macc:0.616885003293,F1:0.502320472782\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 190s 153ms/step - loss: 0.2271 - acc: 0.9067 - val_loss: 0.6756 - val_acc: 0.7058\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:117,FP:29,FN:61,TP:77,Macc:0.679670386327,F1:0.631142078721\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 190s 153ms/step - loss: 0.2291 - acc: 0.9057 - val_loss: 0.7483 - val_acc: 0.6508\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:127,FP:19,FN:76,TP:62,Macc:0.659569137334,F1:0.566204861413\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 191s 153ms/step - loss: 0.2258 - acc: 0.9070 - val_loss: 0.7718 - val_acc: 0.6809\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:108,FP:38,FN:60,TP:78,Macc:0.652471658958,F1:0.61416771646\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 192s 154ms/step - loss: 0.2224 - acc: 0.9088 - val_loss: 1.1287 - val_acc: 0.5927\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:137,FP:9,FN:118,TP:20,Macc:0.541641809183,F1:0.239517769628\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 192s 154ms/step - loss: 0.2227 - acc: 0.9083 - val_loss: 0.6611 - val_acc: 0.6732\n",
      "6710/6710 [==============================] - 8s 1ms/step\n",
      "TN:106,FP:40,FN:56,TP:82,Macc:0.660115096873,F1:0.630763698272\n",
      "Loss: 0\n",
      "args (5.0, 3, 3.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_69 (Con (None, 2500, 1)      31          input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_70 (Con (None, 2500, 1)      31          input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_71 (Con (None, 2500, 1)      31          input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_72 (Con (None, 2500, 1)      31          input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_642 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_69[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_644 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_70[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_646 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_71[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_648 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_72[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 2496, 8)      32          conv1d_642[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 2496, 8)      32          conv1d_644[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_663 (BatchN (None, 2496, 8)      32          conv1d_646[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_665 (BatchN (None, 2496, 8)      32          conv1d_648[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_659 (Activation)     (None, 2496, 8)      0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_661 (Activation)     (None, 2496, 8)      0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_663 (Activation)     (None, 2496, 8)      0           batch_normalization_663[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_665 (Activation)     (None, 2496, 8)      0           batch_normalization_665[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_659 (Dropout)           (None, 2496, 8)      0           activation_659[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_661 (Dropout)           (None, 2496, 8)      0           activation_661[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_663 (Dropout)           (None, 2496, 8)      0           activation_663[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_665 (Dropout)           (None, 2496, 8)      0           activation_665[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_137 (MaxPooling1D (None, 1248, 8)      0           dropout_659[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_139 (MaxPooling1D (None, 1248, 8)      0           dropout_661[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_141 (MaxPooling1D (None, 1248, 8)      0           dropout_663[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_143 (MaxPooling1D (None, 1248, 8)      0           dropout_665[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_643 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_137[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_645 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_139[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_647 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_141[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_649 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_143[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 1244, 4)      16          conv1d_643[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_662 (BatchN (None, 1244, 4)      16          conv1d_645[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_664 (BatchN (None, 1244, 4)      16          conv1d_647[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_666 (BatchN (None, 1244, 4)      16          conv1d_649[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_660 (Activation)     (None, 1244, 4)      0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_662 (Activation)     (None, 1244, 4)      0           batch_normalization_662[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_664 (Activation)     (None, 1244, 4)      0           batch_normalization_664[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_666 (Activation)     (None, 1244, 4)      0           batch_normalization_666[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_660 (Dropout)           (None, 1244, 4)      0           activation_660[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_662 (Dropout)           (None, 1244, 4)      0           activation_662[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_664 (Dropout)           (None, 1244, 4)      0           activation_664[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_666 (Dropout)           (None, 1244, 4)      0           activation_666[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_138 (MaxPooling1D (None, 622, 4)       0           dropout_660[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_140 (MaxPooling1D (None, 622, 4)       0           dropout_662[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_142 (MaxPooling1D (None, 622, 4)       0           dropout_664[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_144 (MaxPooling1D (None, 622, 4)       0           dropout_666[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_508 (Concatenate)   (None, 622, 16)      0           max_pooling1d_138[0][0]          \n",
      "                                                                 max_pooling1d_140[0][0]          \n",
      "                                                                 max_pooling1d_142[0][0]          \n",
      "                                                                 max_pooling1d_144[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_667 (BatchN (None, 622, 16)      64          concatenate_508[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_667 (Activation)     (None, 622, 16)      0           batch_normalization_667[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_650 (Conv1D)             (None, 622, 3)       240         activation_667[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_667 (Dropout)           (None, 622, 3)       0           conv1d_650[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_509 (Concatenate)   (None, 622, 19)      0           concatenate_508[0][0]            \n",
      "                                                                 dropout_667[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_668 (BatchN (None, 622, 19)      76          concatenate_509[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_668 (Activation)     (None, 622, 19)      0           batch_normalization_668[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_651 (Conv1D)             (None, 622, 3)       285         activation_668[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_668 (Dropout)           (None, 622, 3)       0           conv1d_651[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_510 (Concatenate)   (None, 622, 22)      0           concatenate_508[0][0]            \n",
      "                                                                 dropout_667[0][0]                \n",
      "                                                                 dropout_668[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_669 (BatchN (None, 622, 22)      88          concatenate_510[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_669 (Activation)     (None, 622, 22)      0           batch_normalization_669[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_652 (Conv1D)             (None, 622, 3)       330         activation_669[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_669 (Dropout)           (None, 622, 3)       0           conv1d_652[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_511 (Concatenate)   (None, 622, 25)      0           concatenate_508[0][0]            \n",
      "                                                                 dropout_667[0][0]                \n",
      "                                                                 dropout_668[0][0]                \n",
      "                                                                 dropout_669[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_670 (BatchN (None, 622, 25)      100         concatenate_511[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_670 (Activation)     (None, 622, 25)      0           batch_normalization_670[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_653 (Conv1D)             (None, 622, 3)       375         activation_670[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_670 (Dropout)           (None, 622, 3)       0           conv1d_653[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_512 (Concatenate)   (None, 622, 28)      0           concatenate_508[0][0]            \n",
      "                                                                 dropout_667[0][0]                \n",
      "                                                                 dropout_668[0][0]                \n",
      "                                                                 dropout_669[0][0]                \n",
      "                                                                 dropout_670[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_671 (BatchN (None, 622, 28)      112         concatenate_512[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_671 (Activation)     (None, 622, 28)      0           batch_normalization_671[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_654 (Conv1D)             (None, 622, 3)       420         activation_671[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_671 (Dropout)           (None, 622, 3)       0           conv1d_654[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_513 (Concatenate)   (None, 622, 31)      0           concatenate_508[0][0]            \n",
      "                                                                 dropout_667[0][0]                \n",
      "                                                                 dropout_668[0][0]                \n",
      "                                                                 dropout_669[0][0]                \n",
      "                                                                 dropout_670[0][0]                \n",
      "                                                                 dropout_671[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_672 (BatchN (None, 622, 31)      124         concatenate_513[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_672 (Activation)     (None, 622, 31)      0           batch_normalization_672[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_655 (Conv1D)             (None, 622, 31)      4805        activation_672[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_672 (Dropout)           (None, 622, 31)      0           conv1d_655[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_16 (AveragePo (None, 311, 31)      0           dropout_672[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_673 (BatchN (None, 311, 31)      124         average_pooling1d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_673 (Activation)     (None, 311, 31)      0           batch_normalization_673[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_656 (Conv1D)             (None, 311, 3)       465         activation_673[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_673 (Dropout)           (None, 311, 3)       0           conv1d_656[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_514 (Concatenate)   (None, 311, 34)      0           average_pooling1d_16[0][0]       \n",
      "                                                                 dropout_673[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_674 (BatchN (None, 311, 34)      136         concatenate_514[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_674 (Activation)     (None, 311, 34)      0           batch_normalization_674[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_657 (Conv1D)             (None, 311, 3)       510         activation_674[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_674 (Dropout)           (None, 311, 3)       0           conv1d_657[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_515 (Concatenate)   (None, 311, 37)      0           average_pooling1d_16[0][0]       \n",
      "                                                                 dropout_673[0][0]                \n",
      "                                                                 dropout_674[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_675 (BatchN (None, 311, 37)      148         concatenate_515[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_675 (Activation)     (None, 311, 37)      0           batch_normalization_675[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_658 (Conv1D)             (None, 311, 3)       555         activation_675[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_675 (Dropout)           (None, 311, 3)       0           conv1d_658[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_516 (Concatenate)   (None, 311, 40)      0           average_pooling1d_16[0][0]       \n",
      "                                                                 dropout_673[0][0]                \n",
      "                                                                 dropout_674[0][0]                \n",
      "                                                                 dropout_675[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_676 (BatchN (None, 311, 40)      160         concatenate_516[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_676 (Activation)     (None, 311, 40)      0           batch_normalization_676[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_659 (Conv1D)             (None, 311, 3)       600         activation_676[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_676 (Dropout)           (None, 311, 3)       0           conv1d_659[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_517 (Concatenate)   (None, 311, 43)      0           average_pooling1d_16[0][0]       \n",
      "                                                                 dropout_673[0][0]                \n",
      "                                                                 dropout_674[0][0]                \n",
      "                                                                 dropout_675[0][0]                \n",
      "                                                                 dropout_676[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_677 (BatchN (None, 311, 43)      172         concatenate_517[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_677 (Activation)     (None, 311, 43)      0           batch_normalization_677[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_660 (Conv1D)             (None, 311, 3)       645         activation_677[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_677 (Dropout)           (None, 311, 3)       0           conv1d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_518 (Concatenate)   (None, 311, 46)      0           average_pooling1d_16[0][0]       \n",
      "                                                                 dropout_673[0][0]                \n",
      "                                                                 dropout_674[0][0]                \n",
      "                                                                 dropout_675[0][0]                \n",
      "                                                                 dropout_676[0][0]                \n",
      "                                                                 dropout_677[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_678 (BatchN (None, 311, 46)      184         concatenate_518[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_678 (Activation)     (None, 311, 46)      0           batch_normalization_678[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_661 (Conv1D)             (None, 311, 46)      10580       activation_678[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_678 (Dropout)           (None, 311, 46)      0           conv1d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_17 (AveragePo (None, 155, 46)      0           dropout_678[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_679 (BatchN (None, 155, 46)      184         average_pooling1d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_679 (Activation)     (None, 155, 46)      0           batch_normalization_679[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_662 (Conv1D)             (None, 155, 3)       690         activation_679[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_679 (Dropout)           (None, 155, 3)       0           conv1d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_519 (Concatenate)   (None, 155, 49)      0           average_pooling1d_17[0][0]       \n",
      "                                                                 dropout_679[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_680 (BatchN (None, 155, 49)      196         concatenate_519[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_680 (Activation)     (None, 155, 49)      0           batch_normalization_680[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_663 (Conv1D)             (None, 155, 3)       735         activation_680[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_680 (Dropout)           (None, 155, 3)       0           conv1d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_520 (Concatenate)   (None, 155, 52)      0           average_pooling1d_17[0][0]       \n",
      "                                                                 dropout_679[0][0]                \n",
      "                                                                 dropout_680[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_681 (BatchN (None, 155, 52)      208         concatenate_520[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_681 (Activation)     (None, 155, 52)      0           batch_normalization_681[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_664 (Conv1D)             (None, 155, 3)       780         activation_681[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_681 (Dropout)           (None, 155, 3)       0           conv1d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_521 (Concatenate)   (None, 155, 55)      0           average_pooling1d_17[0][0]       \n",
      "                                                                 dropout_679[0][0]                \n",
      "                                                                 dropout_680[0][0]                \n",
      "                                                                 dropout_681[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_682 (BatchN (None, 155, 55)      220         concatenate_521[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_682 (Activation)     (None, 155, 55)      0           batch_normalization_682[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_665 (Conv1D)             (None, 155, 3)       825         activation_682[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_682 (Dropout)           (None, 155, 3)       0           conv1d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_522 (Concatenate)   (None, 155, 58)      0           average_pooling1d_17[0][0]       \n",
      "                                                                 dropout_679[0][0]                \n",
      "                                                                 dropout_680[0][0]                \n",
      "                                                                 dropout_681[0][0]                \n",
      "                                                                 dropout_682[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_683 (BatchN (None, 155, 58)      232         concatenate_522[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_683 (Activation)     (None, 155, 58)      0           batch_normalization_683[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_666 (Conv1D)             (None, 155, 3)       870         activation_683[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_683 (Dropout)           (None, 155, 3)       0           conv1d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_523 (Concatenate)   (None, 155, 61)      0           average_pooling1d_17[0][0]       \n",
      "                                                                 dropout_679[0][0]                \n",
      "                                                                 dropout_680[0][0]                \n",
      "                                                                 dropout_681[0][0]                \n",
      "                                                                 dropout_682[0][0]                \n",
      "                                                                 dropout_683[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_684 (BatchN (None, 155, 61)      244         concatenate_523[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_684 (Activation)     (None, 155, 61)      0           batch_normalization_684[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 9455)         0           activation_684[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 20)           189100      flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_684 (Dropout)           (None, 20)           0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 2)            42          dropout_684[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 216,740\n",
      "Trainable params: 215,258\n",
      "Non-trainable params: 1,482\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 68s 54ms/step - loss: 0.8094 - acc: 0.8430 - val_loss: 0.7489 - val_acc: 0.5282\n",
      "6710/6710 [==============================] - 14s 2ms/step\n",
      "TN:24,FP:122,FN:5,TP:133,Macc:0.564075794189,F1:0.676839733334\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 57s 46ms/step - loss: 0.3381 - acc: 0.8597 - val_loss: 0.6510 - val_acc: 0.6572\n",
      "6710/6710 [==============================] - 2s 372us/step\n",
      "TN:66,FP:80,FN:15,TP:123,Macc:0.671679518621,F1:0.721402277969\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 58s 46ms/step - loss: 0.3110 - acc: 0.8686 - val_loss: 0.6784 - val_acc: 0.6097\n",
      "6710/6710 [==============================] - 3s 373us/step\n",
      "TN:47,FP:99,FN:8,TP:130,Macc:0.631973347192,F1:0.708441662207\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 58s 46ms/step - loss: 0.2892 - acc: 0.8785 - val_loss: 0.7067 - val_acc: 0.7104\n",
      "6710/6710 [==============================] - 3s 400us/step\n",
      "TN:98,FP:48,FN:53,TP:85,Macc:0.643587403014,F1:0.627300724058\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 58s 46ms/step - loss: 0.2766 - acc: 0.8847 - val_loss: 0.6381 - val_acc: 0.6967\n",
      "6710/6710 [==============================] - 3s 379us/step\n",
      "TN:77,FP:69,FN:31,TP:107,Macc:0.651379738787,F1:0.681523195264\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 58s 46ms/step - loss: 0.2655 - acc: 0.8904 - val_loss: 0.6213 - val_acc: 0.6897\n",
      "6710/6710 [==============================] - 3s 377us/step\n",
      "TN:67,FP:79,FN:14,TP:124,Macc:0.678727364014,F1:0.72726738023\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 58s 46ms/step - loss: 0.2600 - acc: 0.8929 - val_loss: 0.5913 - val_acc: 0.7079\n",
      "6710/6710 [==============================] - 3s 379us/step\n",
      "TN:76,FP:70,FN:20,TP:118,Macc:0.687810150798,F1:0.723920960934\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2622 - acc: 0.8914 - val_loss: 0.6416 - val_acc: 0.6681\n",
      "6710/6710 [==============================] - 3s 379us/step\n",
      "TN:53,FP:93,FN:7,TP:131,Macc:0.656144478966,F1:0.723751672544\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 59s 48ms/step - loss: 0.2534 - acc: 0.8946 - val_loss: 0.5511 - val_acc: 0.7331\n",
      "6710/6710 [==============================] - 3s 385us/step\n",
      "TN:70,FP:76,FN:9,TP:129,Macc:0.707117276428,F1:0.752181250571\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2474 - acc: 0.8987 - val_loss: 0.5967 - val_acc: 0.7080\n",
      "6710/6710 [==============================] - 3s 384us/step\n",
      "TN:59,FP:87,FN:9,TP:129,Macc:0.669446046389,F1:0.728808281087\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2444 - acc: 0.8996 - val_loss: 0.6184 - val_acc: 0.6914\n",
      "6710/6710 [==============================] - 3s 384us/step\n",
      "TN:63,FP:83,FN:14,TP:124,Macc:0.665028734909,F1:0.71883525391\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2414 - acc: 0.9014 - val_loss: 0.6150 - val_acc: 0.7048\n",
      "6710/6710 [==============================] - 3s 376us/step\n",
      "TN:59,FP:87,FN:8,TP:130,Macc:0.673069234506,F1:0.732389093217\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2384 - acc: 0.9026 - val_loss: 0.6223 - val_acc: 0.7010\n",
      "6710/6710 [==============================] - 3s 381us/step\n",
      "TN:55,FP:91,FN:5,TP:133,Macc:0.670240169752,F1:0.734801395629\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2361 - acc: 0.9030 - val_loss: 0.6477 - val_acc: 0.6915\n",
      "6710/6710 [==============================] - 3s 377us/step\n",
      "TN:55,FP:91,FN:4,TP:134,Macc:0.673863357869,F1:0.738286782239\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2346 - acc: 0.9035 - val_loss: 0.6235 - val_acc: 0.7013\n",
      "6710/6710 [==============================] - 3s 376us/step\n",
      "TN:66,FP:80,FN:19,TP:119,Macc:0.657186766153,F1:0.706226088145\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2318 - acc: 0.9044 - val_loss: 0.6115 - val_acc: 0.6984\n",
      "6710/6710 [==============================] - 3s 385us/step\n",
      "TN:65,FP:81,FN:13,TP:125,Macc:0.675501237579,F1:0.726738854521\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2303 - acc: 0.9056 - val_loss: 0.6565 - val_acc: 0.6809\n",
      "6710/6710 [==============================] - 3s 387us/step\n",
      "TN:67,FP:79,FN:26,TP:112,Macc:0.63524910661,F1:0.680845661076\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2289 - acc: 0.9065 - val_loss: 0.6662 - val_acc: 0.6858\n",
      "6710/6710 [==============================] - 3s 382us/step\n",
      "TN:52,FP:94,FN:7,TP:131,Macc:0.65271982169,F1:0.721757857621\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2268 - acc: 0.9067 - val_loss: 0.6981 - val_acc: 0.6733\n",
      "6710/6710 [==============================] - 3s 385us/step\n",
      "TN:49,FP:97,FN:9,TP:129,Macc:0.635199473627,F1:0.708785987449\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 58s 47ms/step - loss: 0.2271 - acc: 0.9063 - val_loss: 0.6238 - val_acc: 0.7167\n",
      "6710/6710 [==============================] - 3s 385us/step\n",
      "TN:70,FP:76,FN:16,TP:122,Macc:0.681754959609,F1:0.726185104064\n",
      "Loss: 0\n",
      "args (17.0, 2, 6.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_73 (Con (None, 2500, 1)      31          input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_74 (Con (None, 2500, 1)      31          input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_75 (Con (None, 2500, 1)      31          input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_76 (Con (None, 2500, 1)      31          input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_667 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_73[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_669 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_74[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_671 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_75[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_673 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_76[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_685 (BatchN (None, 2496, 8)      32          conv1d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_687 (BatchN (None, 2496, 8)      32          conv1d_669[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_689 (BatchN (None, 2496, 8)      32          conv1d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_691 (BatchN (None, 2496, 8)      32          conv1d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_685 (Activation)     (None, 2496, 8)      0           batch_normalization_685[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_687 (Activation)     (None, 2496, 8)      0           batch_normalization_687[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_689 (Activation)     (None, 2496, 8)      0           batch_normalization_689[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_691 (Activation)     (None, 2496, 8)      0           batch_normalization_691[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_685 (Dropout)           (None, 2496, 8)      0           activation_685[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_687 (Dropout)           (None, 2496, 8)      0           activation_687[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_689 (Dropout)           (None, 2496, 8)      0           activation_689[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_691 (Dropout)           (None, 2496, 8)      0           activation_691[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_145 (MaxPooling1D (None, 1248, 8)      0           dropout_685[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_147 (MaxPooling1D (None, 1248, 8)      0           dropout_687[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_149 (MaxPooling1D (None, 1248, 8)      0           dropout_689[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_151 (MaxPooling1D (None, 1248, 8)      0           dropout_691[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_668 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_145[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_670 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_147[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_672 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_149[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_674 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_151[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_686 (BatchN (None, 1244, 4)      16          conv1d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_688 (BatchN (None, 1244, 4)      16          conv1d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_690 (BatchN (None, 1244, 4)      16          conv1d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_692 (BatchN (None, 1244, 4)      16          conv1d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_686 (Activation)     (None, 1244, 4)      0           batch_normalization_686[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_688 (Activation)     (None, 1244, 4)      0           batch_normalization_688[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_690 (Activation)     (None, 1244, 4)      0           batch_normalization_690[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_692 (Activation)     (None, 1244, 4)      0           batch_normalization_692[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_686 (Dropout)           (None, 1244, 4)      0           activation_686[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_688 (Dropout)           (None, 1244, 4)      0           activation_688[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_690 (Dropout)           (None, 1244, 4)      0           activation_690[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_692 (Dropout)           (None, 1244, 4)      0           activation_692[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_146 (MaxPooling1D (None, 622, 4)       0           dropout_686[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_148 (MaxPooling1D (None, 622, 4)       0           dropout_688[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_150 (MaxPooling1D (None, 622, 4)       0           dropout_690[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_152 (MaxPooling1D (None, 622, 4)       0           dropout_692[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_524 (Concatenate)   (None, 622, 16)      0           max_pooling1d_146[0][0]          \n",
      "                                                                 max_pooling1d_148[0][0]          \n",
      "                                                                 max_pooling1d_150[0][0]          \n",
      "                                                                 max_pooling1d_152[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_693 (BatchN (None, 622, 16)      64          concatenate_524[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_693 (Activation)     (None, 622, 16)      0           batch_normalization_693[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_675 (Conv1D)             (None, 622, 6)       480         activation_693[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_693 (Dropout)           (None, 622, 6)       0           conv1d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_525 (Concatenate)   (None, 622, 22)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_694 (BatchN (None, 622, 22)      88          concatenate_525[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_694 (Activation)     (None, 622, 22)      0           batch_normalization_694[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_676 (Conv1D)             (None, 622, 6)       660         activation_694[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_694 (Dropout)           (None, 622, 6)       0           conv1d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_526 (Concatenate)   (None, 622, 28)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_695 (BatchN (None, 622, 28)      112         concatenate_526[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_695 (Activation)     (None, 622, 28)      0           batch_normalization_695[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_677 (Conv1D)             (None, 622, 6)       840         activation_695[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_695 (Dropout)           (None, 622, 6)       0           conv1d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_527 (Concatenate)   (None, 622, 34)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_696 (BatchN (None, 622, 34)      136         concatenate_527[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_696 (Activation)     (None, 622, 34)      0           batch_normalization_696[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_678 (Conv1D)             (None, 622, 6)       1020        activation_696[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_696 (Dropout)           (None, 622, 6)       0           conv1d_678[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_528 (Concatenate)   (None, 622, 40)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_697 (BatchN (None, 622, 40)      160         concatenate_528[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_697 (Activation)     (None, 622, 40)      0           batch_normalization_697[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_679 (Conv1D)             (None, 622, 6)       1200        activation_697[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_697 (Dropout)           (None, 622, 6)       0           conv1d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_529 (Concatenate)   (None, 622, 46)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_698 (BatchN (None, 622, 46)      184         concatenate_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_698 (Activation)     (None, 622, 46)      0           batch_normalization_698[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_680 (Conv1D)             (None, 622, 6)       1380        activation_698[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_698 (Dropout)           (None, 622, 6)       0           conv1d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_530 (Concatenate)   (None, 622, 52)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_699 (BatchN (None, 622, 52)      208         concatenate_530[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_699 (Activation)     (None, 622, 52)      0           batch_normalization_699[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_681 (Conv1D)             (None, 622, 6)       1560        activation_699[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_699 (Dropout)           (None, 622, 6)       0           conv1d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_531 (Concatenate)   (None, 622, 58)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_700 (BatchN (None, 622, 58)      232         concatenate_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_700 (Activation)     (None, 622, 58)      0           batch_normalization_700[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_682 (Conv1D)             (None, 622, 6)       1740        activation_700[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_700 (Dropout)           (None, 622, 6)       0           conv1d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_532 (Concatenate)   (None, 622, 64)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_701 (BatchN (None, 622, 64)      256         concatenate_532[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_701 (Activation)     (None, 622, 64)      0           batch_normalization_701[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_683 (Conv1D)             (None, 622, 6)       1920        activation_701[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_701 (Dropout)           (None, 622, 6)       0           conv1d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_533 (Concatenate)   (None, 622, 70)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "                                                                 dropout_701[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_702 (BatchN (None, 622, 70)      280         concatenate_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_702 (Activation)     (None, 622, 70)      0           batch_normalization_702[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_684 (Conv1D)             (None, 622, 6)       2100        activation_702[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_702 (Dropout)           (None, 622, 6)       0           conv1d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_534 (Concatenate)   (None, 622, 76)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "                                                                 dropout_701[0][0]                \n",
      "                                                                 dropout_702[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_703 (BatchN (None, 622, 76)      304         concatenate_534[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_703 (Activation)     (None, 622, 76)      0           batch_normalization_703[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_685 (Conv1D)             (None, 622, 6)       2280        activation_703[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_703 (Dropout)           (None, 622, 6)       0           conv1d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_535 (Concatenate)   (None, 622, 82)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "                                                                 dropout_701[0][0]                \n",
      "                                                                 dropout_702[0][0]                \n",
      "                                                                 dropout_703[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_704 (BatchN (None, 622, 82)      328         concatenate_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_704 (Activation)     (None, 622, 82)      0           batch_normalization_704[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_686 (Conv1D)             (None, 622, 6)       2460        activation_704[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_704 (Dropout)           (None, 622, 6)       0           conv1d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_536 (Concatenate)   (None, 622, 88)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "                                                                 dropout_701[0][0]                \n",
      "                                                                 dropout_702[0][0]                \n",
      "                                                                 dropout_703[0][0]                \n",
      "                                                                 dropout_704[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_705 (BatchN (None, 622, 88)      352         concatenate_536[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_705 (Activation)     (None, 622, 88)      0           batch_normalization_705[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_687 (Conv1D)             (None, 622, 6)       2640        activation_705[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_705 (Dropout)           (None, 622, 6)       0           conv1d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_537 (Concatenate)   (None, 622, 94)      0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "                                                                 dropout_701[0][0]                \n",
      "                                                                 dropout_702[0][0]                \n",
      "                                                                 dropout_703[0][0]                \n",
      "                                                                 dropout_704[0][0]                \n",
      "                                                                 dropout_705[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_706 (BatchN (None, 622, 94)      376         concatenate_537[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_706 (Activation)     (None, 622, 94)      0           batch_normalization_706[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_688 (Conv1D)             (None, 622, 6)       2820        activation_706[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_706 (Dropout)           (None, 622, 6)       0           conv1d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_538 (Concatenate)   (None, 622, 100)     0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "                                                                 dropout_701[0][0]                \n",
      "                                                                 dropout_702[0][0]                \n",
      "                                                                 dropout_703[0][0]                \n",
      "                                                                 dropout_704[0][0]                \n",
      "                                                                 dropout_705[0][0]                \n",
      "                                                                 dropout_706[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_707 (BatchN (None, 622, 100)     400         concatenate_538[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_707 (Activation)     (None, 622, 100)     0           batch_normalization_707[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_689 (Conv1D)             (None, 622, 6)       3000        activation_707[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_707 (Dropout)           (None, 622, 6)       0           conv1d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_539 (Concatenate)   (None, 622, 106)     0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "                                                                 dropout_701[0][0]                \n",
      "                                                                 dropout_702[0][0]                \n",
      "                                                                 dropout_703[0][0]                \n",
      "                                                                 dropout_704[0][0]                \n",
      "                                                                 dropout_705[0][0]                \n",
      "                                                                 dropout_706[0][0]                \n",
      "                                                                 dropout_707[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_708 (BatchN (None, 622, 106)     424         concatenate_539[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_708 (Activation)     (None, 622, 106)     0           batch_normalization_708[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_690 (Conv1D)             (None, 622, 6)       3180        activation_708[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_708 (Dropout)           (None, 622, 6)       0           conv1d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_540 (Concatenate)   (None, 622, 112)     0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "                                                                 dropout_701[0][0]                \n",
      "                                                                 dropout_702[0][0]                \n",
      "                                                                 dropout_703[0][0]                \n",
      "                                                                 dropout_704[0][0]                \n",
      "                                                                 dropout_705[0][0]                \n",
      "                                                                 dropout_706[0][0]                \n",
      "                                                                 dropout_707[0][0]                \n",
      "                                                                 dropout_708[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_709 (BatchN (None, 622, 112)     448         concatenate_540[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_709 (Activation)     (None, 622, 112)     0           batch_normalization_709[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_691 (Conv1D)             (None, 622, 6)       3360        activation_709[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_709 (Dropout)           (None, 622, 6)       0           conv1d_691[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_541 (Concatenate)   (None, 622, 118)     0           concatenate_524[0][0]            \n",
      "                                                                 dropout_693[0][0]                \n",
      "                                                                 dropout_694[0][0]                \n",
      "                                                                 dropout_695[0][0]                \n",
      "                                                                 dropout_696[0][0]                \n",
      "                                                                 dropout_697[0][0]                \n",
      "                                                                 dropout_698[0][0]                \n",
      "                                                                 dropout_699[0][0]                \n",
      "                                                                 dropout_700[0][0]                \n",
      "                                                                 dropout_701[0][0]                \n",
      "                                                                 dropout_702[0][0]                \n",
      "                                                                 dropout_703[0][0]                \n",
      "                                                                 dropout_704[0][0]                \n",
      "                                                                 dropout_705[0][0]                \n",
      "                                                                 dropout_706[0][0]                \n",
      "                                                                 dropout_707[0][0]                \n",
      "                                                                 dropout_708[0][0]                \n",
      "                                                                 dropout_709[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_710 (BatchN (None, 622, 118)     472         concatenate_541[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_710 (Activation)     (None, 622, 118)     0           batch_normalization_710[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_692 (Conv1D)             (None, 622, 118)     69620       activation_710[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_710 (Dropout)           (None, 622, 118)     0           conv1d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_18 (AveragePo (None, 311, 118)     0           dropout_710[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_711 (BatchN (None, 311, 118)     472         average_pooling1d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_711 (Activation)     (None, 311, 118)     0           batch_normalization_711[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_693 (Conv1D)             (None, 311, 6)       3540        activation_711[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_711 (Dropout)           (None, 311, 6)       0           conv1d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_542 (Concatenate)   (None, 311, 124)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_712 (BatchN (None, 311, 124)     496         concatenate_542[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_712 (Activation)     (None, 311, 124)     0           batch_normalization_712[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_694 (Conv1D)             (None, 311, 6)       3720        activation_712[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_712 (Dropout)           (None, 311, 6)       0           conv1d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_543 (Concatenate)   (None, 311, 130)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_713 (BatchN (None, 311, 130)     520         concatenate_543[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_713 (Activation)     (None, 311, 130)     0           batch_normalization_713[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_695 (Conv1D)             (None, 311, 6)       3900        activation_713[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_713 (Dropout)           (None, 311, 6)       0           conv1d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_544 (Concatenate)   (None, 311, 136)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_714 (BatchN (None, 311, 136)     544         concatenate_544[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_714 (Activation)     (None, 311, 136)     0           batch_normalization_714[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_696 (Conv1D)             (None, 311, 6)       4080        activation_714[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_714 (Dropout)           (None, 311, 6)       0           conv1d_696[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_545 (Concatenate)   (None, 311, 142)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_715 (BatchN (None, 311, 142)     568         concatenate_545[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_715 (Activation)     (None, 311, 142)     0           batch_normalization_715[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_697 (Conv1D)             (None, 311, 6)       4260        activation_715[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_715 (Dropout)           (None, 311, 6)       0           conv1d_697[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_546 (Concatenate)   (None, 311, 148)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_716 (BatchN (None, 311, 148)     592         concatenate_546[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_716 (Activation)     (None, 311, 148)     0           batch_normalization_716[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_698 (Conv1D)             (None, 311, 6)       4440        activation_716[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_716 (Dropout)           (None, 311, 6)       0           conv1d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_547 (Concatenate)   (None, 311, 154)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_717 (BatchN (None, 311, 154)     616         concatenate_547[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_717 (Activation)     (None, 311, 154)     0           batch_normalization_717[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_699 (Conv1D)             (None, 311, 6)       4620        activation_717[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_717 (Dropout)           (None, 311, 6)       0           conv1d_699[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_548 (Concatenate)   (None, 311, 160)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_718 (BatchN (None, 311, 160)     640         concatenate_548[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_718 (Activation)     (None, 311, 160)     0           batch_normalization_718[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_700 (Conv1D)             (None, 311, 6)       4800        activation_718[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_718 (Dropout)           (None, 311, 6)       0           conv1d_700[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_549 (Concatenate)   (None, 311, 166)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_719 (BatchN (None, 311, 166)     664         concatenate_549[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_719 (Activation)     (None, 311, 166)     0           batch_normalization_719[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_701 (Conv1D)             (None, 311, 6)       4980        activation_719[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_719 (Dropout)           (None, 311, 6)       0           conv1d_701[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_550 (Concatenate)   (None, 311, 172)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "                                                                 dropout_719[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_720 (BatchN (None, 311, 172)     688         concatenate_550[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_720 (Activation)     (None, 311, 172)     0           batch_normalization_720[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_702 (Conv1D)             (None, 311, 6)       5160        activation_720[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_720 (Dropout)           (None, 311, 6)       0           conv1d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_551 (Concatenate)   (None, 311, 178)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "                                                                 dropout_719[0][0]                \n",
      "                                                                 dropout_720[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_721 (BatchN (None, 311, 178)     712         concatenate_551[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_721 (Activation)     (None, 311, 178)     0           batch_normalization_721[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_703 (Conv1D)             (None, 311, 6)       5340        activation_721[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_721 (Dropout)           (None, 311, 6)       0           conv1d_703[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_552 (Concatenate)   (None, 311, 184)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "                                                                 dropout_719[0][0]                \n",
      "                                                                 dropout_720[0][0]                \n",
      "                                                                 dropout_721[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_722 (BatchN (None, 311, 184)     736         concatenate_552[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_722 (Activation)     (None, 311, 184)     0           batch_normalization_722[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_704 (Conv1D)             (None, 311, 6)       5520        activation_722[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_722 (Dropout)           (None, 311, 6)       0           conv1d_704[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_553 (Concatenate)   (None, 311, 190)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "                                                                 dropout_719[0][0]                \n",
      "                                                                 dropout_720[0][0]                \n",
      "                                                                 dropout_721[0][0]                \n",
      "                                                                 dropout_722[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_723 (BatchN (None, 311, 190)     760         concatenate_553[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_723 (Activation)     (None, 311, 190)     0           batch_normalization_723[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_705 (Conv1D)             (None, 311, 6)       5700        activation_723[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_723 (Dropout)           (None, 311, 6)       0           conv1d_705[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_554 (Concatenate)   (None, 311, 196)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "                                                                 dropout_719[0][0]                \n",
      "                                                                 dropout_720[0][0]                \n",
      "                                                                 dropout_721[0][0]                \n",
      "                                                                 dropout_722[0][0]                \n",
      "                                                                 dropout_723[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_724 (BatchN (None, 311, 196)     784         concatenate_554[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_724 (Activation)     (None, 311, 196)     0           batch_normalization_724[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_706 (Conv1D)             (None, 311, 6)       5880        activation_724[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_724 (Dropout)           (None, 311, 6)       0           conv1d_706[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_555 (Concatenate)   (None, 311, 202)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "                                                                 dropout_719[0][0]                \n",
      "                                                                 dropout_720[0][0]                \n",
      "                                                                 dropout_721[0][0]                \n",
      "                                                                 dropout_722[0][0]                \n",
      "                                                                 dropout_723[0][0]                \n",
      "                                                                 dropout_724[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_725 (BatchN (None, 311, 202)     808         concatenate_555[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_725 (Activation)     (None, 311, 202)     0           batch_normalization_725[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_707 (Conv1D)             (None, 311, 6)       6060        activation_725[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_725 (Dropout)           (None, 311, 6)       0           conv1d_707[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_556 (Concatenate)   (None, 311, 208)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "                                                                 dropout_719[0][0]                \n",
      "                                                                 dropout_720[0][0]                \n",
      "                                                                 dropout_721[0][0]                \n",
      "                                                                 dropout_722[0][0]                \n",
      "                                                                 dropout_723[0][0]                \n",
      "                                                                 dropout_724[0][0]                \n",
      "                                                                 dropout_725[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_726 (BatchN (None, 311, 208)     832         concatenate_556[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_726 (Activation)     (None, 311, 208)     0           batch_normalization_726[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_708 (Conv1D)             (None, 311, 6)       6240        activation_726[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_726 (Dropout)           (None, 311, 6)       0           conv1d_708[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_557 (Concatenate)   (None, 311, 214)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "                                                                 dropout_719[0][0]                \n",
      "                                                                 dropout_720[0][0]                \n",
      "                                                                 dropout_721[0][0]                \n",
      "                                                                 dropout_722[0][0]                \n",
      "                                                                 dropout_723[0][0]                \n",
      "                                                                 dropout_724[0][0]                \n",
      "                                                                 dropout_725[0][0]                \n",
      "                                                                 dropout_726[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_727 (BatchN (None, 311, 214)     856         concatenate_557[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_727 (Activation)     (None, 311, 214)     0           batch_normalization_727[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_709 (Conv1D)             (None, 311, 6)       6420        activation_727[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_727 (Dropout)           (None, 311, 6)       0           conv1d_709[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_558 (Concatenate)   (None, 311, 220)     0           average_pooling1d_18[0][0]       \n",
      "                                                                 dropout_711[0][0]                \n",
      "                                                                 dropout_712[0][0]                \n",
      "                                                                 dropout_713[0][0]                \n",
      "                                                                 dropout_714[0][0]                \n",
      "                                                                 dropout_715[0][0]                \n",
      "                                                                 dropout_716[0][0]                \n",
      "                                                                 dropout_717[0][0]                \n",
      "                                                                 dropout_718[0][0]                \n",
      "                                                                 dropout_719[0][0]                \n",
      "                                                                 dropout_720[0][0]                \n",
      "                                                                 dropout_721[0][0]                \n",
      "                                                                 dropout_722[0][0]                \n",
      "                                                                 dropout_723[0][0]                \n",
      "                                                                 dropout_724[0][0]                \n",
      "                                                                 dropout_725[0][0]                \n",
      "                                                                 dropout_726[0][0]                \n",
      "                                                                 dropout_727[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_728 (BatchN (None, 311, 220)     880         concatenate_558[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_728 (Activation)     (None, 311, 220)     0           batch_normalization_728[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 68420)        0           activation_728[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 20)           1368400     flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_728 (Dropout)           (None, 20)           0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 2)            42          dropout_728[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,573,470\n",
      "Trainable params: 1,564,878\n",
      "Non-trainable params: 8,592\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 179s 144ms/step - loss: 3.5601 - acc: 0.8116 - val_loss: 4.9873 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 19s 3ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 164s 132ms/step - loss: 0.4707 - acc: 0.8097 - val_loss: 0.7171 - val_acc: 0.5519\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:146,FP:0,FN:138,TP:0,Macc:0.499999962329,F1:0.0\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 164s 132ms/step - loss: 0.3268 - acc: 0.8328 - val_loss: 0.7045 - val_acc: 0.4943\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:9,FP:137,FN:0,TP:138,Macc:0.530821875631,F1:0.668275941315\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 166s 133ms/step - loss: 0.3039 - acc: 0.8614 - val_loss: 0.7551 - val_acc: 0.4821\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:7,FP:139,FN:0,TP:138,Macc:0.523972561079,F1:0.665055322759\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 164s 132ms/step - loss: 0.2869 - acc: 0.8776 - val_loss: 0.6914 - val_acc: 0.5839\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:36,FP:110,FN:2,TP:136,Macc:0.616041245855,F1:0.708328227846\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 164s 132ms/step - loss: 0.2755 - acc: 0.8810 - val_loss: 0.7656 - val_acc: 0.5337\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:28,FP:118,FN:3,TP:135,Macc:0.585020799528,F1:0.69053202136\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2728 - acc: 0.8827 - val_loss: 0.8641 - val_acc: 0.4703\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:4,FP:142,FN:0,TP:138,Macc:0.51369858925,F1:0.66028218135\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 164s 132ms/step - loss: 0.2683 - acc: 0.8852 - val_loss: 0.8418 - val_acc: 0.4914\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:10,FP:136,FN:0,TP:138,Macc:0.534246532907,F1:0.669897976188\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2641 - acc: 0.8865 - val_loss: 0.8125 - val_acc: 0.5194\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:19,FP:127,FN:0,TP:138,Macc:0.565068448393,F1:0.684858532432\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 164s 132ms/step - loss: 0.2582 - acc: 0.8884 - val_loss: 0.8336 - val_acc: 0.5122\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:17,FP:129,FN:0,TP:138,Macc:0.558219133841,F1:0.681476502496\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2619 - acc: 0.8865 - val_loss: 0.8644 - val_acc: 0.4896\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:7,FP:139,FN:0,TP:138,Macc:0.523972561079,F1:0.665055322759\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2571 - acc: 0.8896 - val_loss: 0.8458 - val_acc: 0.5124\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:17,FP:129,FN:1,TP:137,Macc:0.554595945724,F1:0.678212836988\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2556 - acc: 0.8906 - val_loss: 0.8181 - val_acc: 0.5323\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:24,FP:122,FN:0,TP:138,Macc:0.582191734774,F1:0.693462315179\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2524 - acc: 0.8922 - val_loss: 0.8382 - val_acc: 0.5209\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:17,FP:129,FN:0,TP:138,Macc:0.558219133841,F1:0.681476502496\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 164s 132ms/step - loss: 0.2516 - acc: 0.8922 - val_loss: 0.7957 - val_acc: 0.5553\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:29,FP:117,FN:2,TP:136,Macc:0.592068644921,F1:0.695647110586\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2512 - acc: 0.8927 - val_loss: 0.7553 - val_acc: 0.5869\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:38,FP:108,FN:2,TP:136,Macc:0.622890560407,F1:0.71203676734\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2487 - acc: 0.8934 - val_loss: 0.7891 - val_acc: 0.5718\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:33,FP:113,FN:2,TP:136,Macc:0.605767274026,F1:0.702837289809\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2485 - acc: 0.8937 - val_loss: 0.7764 - val_acc: 0.5803\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:36,FP:110,FN:1,TP:137,Macc:0.619664433972,F1:0.71168321191\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2493 - acc: 0.8937 - val_loss: 0.7143 - val_acc: 0.6188\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:48,FP:98,FN:4,TP:134,Macc:0.649890756936,F1:0.724319136282\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.2503 - acc: 0.8926 - val_loss: 0.7139 - val_acc: 0.6213\n",
      "6710/6710 [==============================] - 7s 1ms/step\n",
      "TN:44,FP:102,FN:2,TP:136,Macc:0.643438504065,F1:0.723399102062\n",
      "Loss: 0\n",
      "args (22.0, 3, 3.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_77 (Con (None, 2500, 1)      31          input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_78 (Con (None, 2500, 1)      31          input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_79 (Con (None, 2500, 1)      31          input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_80 (Con (None, 2500, 1)      31          input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_710 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_77[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_712 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_78[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_714 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_79[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_716 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_80[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_729 (BatchN (None, 2496, 8)      32          conv1d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_731 (BatchN (None, 2496, 8)      32          conv1d_712[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_733 (BatchN (None, 2496, 8)      32          conv1d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_735 (BatchN (None, 2496, 8)      32          conv1d_716[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_729 (Activation)     (None, 2496, 8)      0           batch_normalization_729[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_731 (Activation)     (None, 2496, 8)      0           batch_normalization_731[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_733 (Activation)     (None, 2496, 8)      0           batch_normalization_733[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_735 (Activation)     (None, 2496, 8)      0           batch_normalization_735[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_729 (Dropout)           (None, 2496, 8)      0           activation_729[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_731 (Dropout)           (None, 2496, 8)      0           activation_731[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_733 (Dropout)           (None, 2496, 8)      0           activation_733[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_735 (Dropout)           (None, 2496, 8)      0           activation_735[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_153 (MaxPooling1D (None, 1248, 8)      0           dropout_729[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_155 (MaxPooling1D (None, 1248, 8)      0           dropout_731[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_157 (MaxPooling1D (None, 1248, 8)      0           dropout_733[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_159 (MaxPooling1D (None, 1248, 8)      0           dropout_735[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_711 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_153[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_713 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_155[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_715 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_157[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_717 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_159[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_730 (BatchN (None, 1244, 4)      16          conv1d_711[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_732 (BatchN (None, 1244, 4)      16          conv1d_713[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_734 (BatchN (None, 1244, 4)      16          conv1d_715[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_736 (BatchN (None, 1244, 4)      16          conv1d_717[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_730 (Activation)     (None, 1244, 4)      0           batch_normalization_730[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_732 (Activation)     (None, 1244, 4)      0           batch_normalization_732[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_734 (Activation)     (None, 1244, 4)      0           batch_normalization_734[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_736 (Activation)     (None, 1244, 4)      0           batch_normalization_736[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_730 (Dropout)           (None, 1244, 4)      0           activation_730[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_732 (Dropout)           (None, 1244, 4)      0           activation_732[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_734 (Dropout)           (None, 1244, 4)      0           activation_734[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_736 (Dropout)           (None, 1244, 4)      0           activation_736[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_154 (MaxPooling1D (None, 622, 4)       0           dropout_730[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_156 (MaxPooling1D (None, 622, 4)       0           dropout_732[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_158 (MaxPooling1D (None, 622, 4)       0           dropout_734[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_160 (MaxPooling1D (None, 622, 4)       0           dropout_736[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_559 (Concatenate)   (None, 622, 16)      0           max_pooling1d_154[0][0]          \n",
      "                                                                 max_pooling1d_156[0][0]          \n",
      "                                                                 max_pooling1d_158[0][0]          \n",
      "                                                                 max_pooling1d_160[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_737 (BatchN (None, 622, 16)      64          concatenate_559[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_737 (Activation)     (None, 622, 16)      0           batch_normalization_737[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_718 (Conv1D)             (None, 622, 3)       240         activation_737[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_737 (Dropout)           (None, 622, 3)       0           conv1d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_560 (Concatenate)   (None, 622, 19)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_738 (BatchN (None, 622, 19)      76          concatenate_560[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_738 (Activation)     (None, 622, 19)      0           batch_normalization_738[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_719 (Conv1D)             (None, 622, 3)       285         activation_738[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_738 (Dropout)           (None, 622, 3)       0           conv1d_719[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_561 (Concatenate)   (None, 622, 22)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_739 (BatchN (None, 622, 22)      88          concatenate_561[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_739 (Activation)     (None, 622, 22)      0           batch_normalization_739[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_720 (Conv1D)             (None, 622, 3)       330         activation_739[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_739 (Dropout)           (None, 622, 3)       0           conv1d_720[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_562 (Concatenate)   (None, 622, 25)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_740 (BatchN (None, 622, 25)      100         concatenate_562[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_740 (Activation)     (None, 622, 25)      0           batch_normalization_740[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_721 (Conv1D)             (None, 622, 3)       375         activation_740[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_740 (Dropout)           (None, 622, 3)       0           conv1d_721[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_563 (Concatenate)   (None, 622, 28)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_741 (BatchN (None, 622, 28)      112         concatenate_563[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_741 (Activation)     (None, 622, 28)      0           batch_normalization_741[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_722 (Conv1D)             (None, 622, 3)       420         activation_741[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_741 (Dropout)           (None, 622, 3)       0           conv1d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_564 (Concatenate)   (None, 622, 31)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_742 (BatchN (None, 622, 31)      124         concatenate_564[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_742 (Activation)     (None, 622, 31)      0           batch_normalization_742[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_723 (Conv1D)             (None, 622, 3)       465         activation_742[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_742 (Dropout)           (None, 622, 3)       0           conv1d_723[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_565 (Concatenate)   (None, 622, 34)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_743 (BatchN (None, 622, 34)      136         concatenate_565[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_743 (Activation)     (None, 622, 34)      0           batch_normalization_743[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_724 (Conv1D)             (None, 622, 3)       510         activation_743[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_743 (Dropout)           (None, 622, 3)       0           conv1d_724[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_566 (Concatenate)   (None, 622, 37)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_744 (BatchN (None, 622, 37)      148         concatenate_566[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_744 (Activation)     (None, 622, 37)      0           batch_normalization_744[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_725 (Conv1D)             (None, 622, 3)       555         activation_744[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_744 (Dropout)           (None, 622, 3)       0           conv1d_725[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_567 (Concatenate)   (None, 622, 40)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_745 (BatchN (None, 622, 40)      160         concatenate_567[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_745 (Activation)     (None, 622, 40)      0           batch_normalization_745[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_726 (Conv1D)             (None, 622, 3)       600         activation_745[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_745 (Dropout)           (None, 622, 3)       0           conv1d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_568 (Concatenate)   (None, 622, 43)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_746 (BatchN (None, 622, 43)      172         concatenate_568[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_746 (Activation)     (None, 622, 43)      0           batch_normalization_746[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_727 (Conv1D)             (None, 622, 3)       645         activation_746[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_746 (Dropout)           (None, 622, 3)       0           conv1d_727[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_569 (Concatenate)   (None, 622, 46)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_747 (BatchN (None, 622, 46)      184         concatenate_569[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_747 (Activation)     (None, 622, 46)      0           batch_normalization_747[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_728 (Conv1D)             (None, 622, 3)       690         activation_747[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_747 (Dropout)           (None, 622, 3)       0           conv1d_728[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_570 (Concatenate)   (None, 622, 49)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_748 (BatchN (None, 622, 49)      196         concatenate_570[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_748 (Activation)     (None, 622, 49)      0           batch_normalization_748[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_729 (Conv1D)             (None, 622, 3)       735         activation_748[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_748 (Dropout)           (None, 622, 3)       0           conv1d_729[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_571 (Concatenate)   (None, 622, 52)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_749 (BatchN (None, 622, 52)      208         concatenate_571[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_749 (Activation)     (None, 622, 52)      0           batch_normalization_749[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_730 (Conv1D)             (None, 622, 3)       780         activation_749[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_749 (Dropout)           (None, 622, 3)       0           conv1d_730[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_572 (Concatenate)   (None, 622, 55)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_750 (BatchN (None, 622, 55)      220         concatenate_572[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_750 (Activation)     (None, 622, 55)      0           batch_normalization_750[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_731 (Conv1D)             (None, 622, 3)       825         activation_750[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_750 (Dropout)           (None, 622, 3)       0           conv1d_731[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_573 (Concatenate)   (None, 622, 58)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "                                                                 dropout_750[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_751 (BatchN (None, 622, 58)      232         concatenate_573[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_751 (Activation)     (None, 622, 58)      0           batch_normalization_751[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_732 (Conv1D)             (None, 622, 3)       870         activation_751[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_751 (Dropout)           (None, 622, 3)       0           conv1d_732[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_574 (Concatenate)   (None, 622, 61)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "                                                                 dropout_750[0][0]                \n",
      "                                                                 dropout_751[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_752 (BatchN (None, 622, 61)      244         concatenate_574[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_752 (Activation)     (None, 622, 61)      0           batch_normalization_752[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_733 (Conv1D)             (None, 622, 3)       915         activation_752[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_752 (Dropout)           (None, 622, 3)       0           conv1d_733[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_575 (Concatenate)   (None, 622, 64)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "                                                                 dropout_750[0][0]                \n",
      "                                                                 dropout_751[0][0]                \n",
      "                                                                 dropout_752[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_753 (BatchN (None, 622, 64)      256         concatenate_575[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_753 (Activation)     (None, 622, 64)      0           batch_normalization_753[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_734 (Conv1D)             (None, 622, 3)       960         activation_753[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_753 (Dropout)           (None, 622, 3)       0           conv1d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_576 (Concatenate)   (None, 622, 67)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "                                                                 dropout_750[0][0]                \n",
      "                                                                 dropout_751[0][0]                \n",
      "                                                                 dropout_752[0][0]                \n",
      "                                                                 dropout_753[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_754 (BatchN (None, 622, 67)      268         concatenate_576[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_754 (Activation)     (None, 622, 67)      0           batch_normalization_754[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_735 (Conv1D)             (None, 622, 3)       1005        activation_754[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_754 (Dropout)           (None, 622, 3)       0           conv1d_735[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_577 (Concatenate)   (None, 622, 70)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "                                                                 dropout_750[0][0]                \n",
      "                                                                 dropout_751[0][0]                \n",
      "                                                                 dropout_752[0][0]                \n",
      "                                                                 dropout_753[0][0]                \n",
      "                                                                 dropout_754[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_755 (BatchN (None, 622, 70)      280         concatenate_577[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_755 (Activation)     (None, 622, 70)      0           batch_normalization_755[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_736 (Conv1D)             (None, 622, 3)       1050        activation_755[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_755 (Dropout)           (None, 622, 3)       0           conv1d_736[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_578 (Concatenate)   (None, 622, 73)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "                                                                 dropout_750[0][0]                \n",
      "                                                                 dropout_751[0][0]                \n",
      "                                                                 dropout_752[0][0]                \n",
      "                                                                 dropout_753[0][0]                \n",
      "                                                                 dropout_754[0][0]                \n",
      "                                                                 dropout_755[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_756 (BatchN (None, 622, 73)      292         concatenate_578[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_756 (Activation)     (None, 622, 73)      0           batch_normalization_756[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_737 (Conv1D)             (None, 622, 3)       1095        activation_756[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_756 (Dropout)           (None, 622, 3)       0           conv1d_737[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_579 (Concatenate)   (None, 622, 76)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "                                                                 dropout_750[0][0]                \n",
      "                                                                 dropout_751[0][0]                \n",
      "                                                                 dropout_752[0][0]                \n",
      "                                                                 dropout_753[0][0]                \n",
      "                                                                 dropout_754[0][0]                \n",
      "                                                                 dropout_755[0][0]                \n",
      "                                                                 dropout_756[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_757 (BatchN (None, 622, 76)      304         concatenate_579[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_757 (Activation)     (None, 622, 76)      0           batch_normalization_757[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_738 (Conv1D)             (None, 622, 3)       1140        activation_757[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_757 (Dropout)           (None, 622, 3)       0           conv1d_738[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_580 (Concatenate)   (None, 622, 79)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "                                                                 dropout_750[0][0]                \n",
      "                                                                 dropout_751[0][0]                \n",
      "                                                                 dropout_752[0][0]                \n",
      "                                                                 dropout_753[0][0]                \n",
      "                                                                 dropout_754[0][0]                \n",
      "                                                                 dropout_755[0][0]                \n",
      "                                                                 dropout_756[0][0]                \n",
      "                                                                 dropout_757[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_758 (BatchN (None, 622, 79)      316         concatenate_580[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_758 (Activation)     (None, 622, 79)      0           batch_normalization_758[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_739 (Conv1D)             (None, 622, 3)       1185        activation_758[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_758 (Dropout)           (None, 622, 3)       0           conv1d_739[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_581 (Concatenate)   (None, 622, 82)      0           concatenate_559[0][0]            \n",
      "                                                                 dropout_737[0][0]                \n",
      "                                                                 dropout_738[0][0]                \n",
      "                                                                 dropout_739[0][0]                \n",
      "                                                                 dropout_740[0][0]                \n",
      "                                                                 dropout_741[0][0]                \n",
      "                                                                 dropout_742[0][0]                \n",
      "                                                                 dropout_743[0][0]                \n",
      "                                                                 dropout_744[0][0]                \n",
      "                                                                 dropout_745[0][0]                \n",
      "                                                                 dropout_746[0][0]                \n",
      "                                                                 dropout_747[0][0]                \n",
      "                                                                 dropout_748[0][0]                \n",
      "                                                                 dropout_749[0][0]                \n",
      "                                                                 dropout_750[0][0]                \n",
      "                                                                 dropout_751[0][0]                \n",
      "                                                                 dropout_752[0][0]                \n",
      "                                                                 dropout_753[0][0]                \n",
      "                                                                 dropout_754[0][0]                \n",
      "                                                                 dropout_755[0][0]                \n",
      "                                                                 dropout_756[0][0]                \n",
      "                                                                 dropout_757[0][0]                \n",
      "                                                                 dropout_758[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_759 (BatchN (None, 622, 82)      328         concatenate_581[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_759 (Activation)     (None, 622, 82)      0           batch_normalization_759[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_740 (Conv1D)             (None, 622, 82)      33620       activation_759[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_759 (Dropout)           (None, 622, 82)      0           conv1d_740[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_19 (AveragePo (None, 311, 82)      0           dropout_759[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_760 (BatchN (None, 311, 82)      328         average_pooling1d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_760 (Activation)     (None, 311, 82)      0           batch_normalization_760[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_741 (Conv1D)             (None, 311, 3)       1230        activation_760[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_760 (Dropout)           (None, 311, 3)       0           conv1d_741[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_582 (Concatenate)   (None, 311, 85)      0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_761 (BatchN (None, 311, 85)      340         concatenate_582[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_761 (Activation)     (None, 311, 85)      0           batch_normalization_761[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_742 (Conv1D)             (None, 311, 3)       1275        activation_761[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_761 (Dropout)           (None, 311, 3)       0           conv1d_742[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_583 (Concatenate)   (None, 311, 88)      0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_762 (BatchN (None, 311, 88)      352         concatenate_583[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_762 (Activation)     (None, 311, 88)      0           batch_normalization_762[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_743 (Conv1D)             (None, 311, 3)       1320        activation_762[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_762 (Dropout)           (None, 311, 3)       0           conv1d_743[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_584 (Concatenate)   (None, 311, 91)      0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_763 (BatchN (None, 311, 91)      364         concatenate_584[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_763 (Activation)     (None, 311, 91)      0           batch_normalization_763[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_744 (Conv1D)             (None, 311, 3)       1365        activation_763[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_763 (Dropout)           (None, 311, 3)       0           conv1d_744[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_585 (Concatenate)   (None, 311, 94)      0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_764 (BatchN (None, 311, 94)      376         concatenate_585[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_764 (Activation)     (None, 311, 94)      0           batch_normalization_764[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_745 (Conv1D)             (None, 311, 3)       1410        activation_764[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_764 (Dropout)           (None, 311, 3)       0           conv1d_745[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_586 (Concatenate)   (None, 311, 97)      0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_765 (BatchN (None, 311, 97)      388         concatenate_586[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_765 (Activation)     (None, 311, 97)      0           batch_normalization_765[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_746 (Conv1D)             (None, 311, 3)       1455        activation_765[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_765 (Dropout)           (None, 311, 3)       0           conv1d_746[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_587 (Concatenate)   (None, 311, 100)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_766 (BatchN (None, 311, 100)     400         concatenate_587[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_766 (Activation)     (None, 311, 100)     0           batch_normalization_766[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_747 (Conv1D)             (None, 311, 3)       1500        activation_766[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_766 (Dropout)           (None, 311, 3)       0           conv1d_747[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_588 (Concatenate)   (None, 311, 103)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_767 (BatchN (None, 311, 103)     412         concatenate_588[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_767 (Activation)     (None, 311, 103)     0           batch_normalization_767[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_748 (Conv1D)             (None, 311, 3)       1545        activation_767[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_767 (Dropout)           (None, 311, 3)       0           conv1d_748[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_589 (Concatenate)   (None, 311, 106)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_768 (BatchN (None, 311, 106)     424         concatenate_589[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_768 (Activation)     (None, 311, 106)     0           batch_normalization_768[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_749 (Conv1D)             (None, 311, 3)       1590        activation_768[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_768 (Dropout)           (None, 311, 3)       0           conv1d_749[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_590 (Concatenate)   (None, 311, 109)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_769 (BatchN (None, 311, 109)     436         concatenate_590[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_769 (Activation)     (None, 311, 109)     0           batch_normalization_769[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_750 (Conv1D)             (None, 311, 3)       1635        activation_769[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_769 (Dropout)           (None, 311, 3)       0           conv1d_750[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_591 (Concatenate)   (None, 311, 112)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_770 (BatchN (None, 311, 112)     448         concatenate_591[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_770 (Activation)     (None, 311, 112)     0           batch_normalization_770[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_751 (Conv1D)             (None, 311, 3)       1680        activation_770[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_770 (Dropout)           (None, 311, 3)       0           conv1d_751[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_592 (Concatenate)   (None, 311, 115)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_771 (BatchN (None, 311, 115)     460         concatenate_592[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_771 (Activation)     (None, 311, 115)     0           batch_normalization_771[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_752 (Conv1D)             (None, 311, 3)       1725        activation_771[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_771 (Dropout)           (None, 311, 3)       0           conv1d_752[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_593 (Concatenate)   (None, 311, 118)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_772 (BatchN (None, 311, 118)     472         concatenate_593[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_772 (Activation)     (None, 311, 118)     0           batch_normalization_772[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_753 (Conv1D)             (None, 311, 3)       1770        activation_772[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_772 (Dropout)           (None, 311, 3)       0           conv1d_753[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_594 (Concatenate)   (None, 311, 121)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_773 (BatchN (None, 311, 121)     484         concatenate_594[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_773 (Activation)     (None, 311, 121)     0           batch_normalization_773[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_754 (Conv1D)             (None, 311, 3)       1815        activation_773[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_773 (Dropout)           (None, 311, 3)       0           conv1d_754[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_595 (Concatenate)   (None, 311, 124)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "                                                                 dropout_773[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_774 (BatchN (None, 311, 124)     496         concatenate_595[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_774 (Activation)     (None, 311, 124)     0           batch_normalization_774[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_755 (Conv1D)             (None, 311, 3)       1860        activation_774[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_774 (Dropout)           (None, 311, 3)       0           conv1d_755[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_596 (Concatenate)   (None, 311, 127)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "                                                                 dropout_773[0][0]                \n",
      "                                                                 dropout_774[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_775 (BatchN (None, 311, 127)     508         concatenate_596[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_775 (Activation)     (None, 311, 127)     0           batch_normalization_775[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_756 (Conv1D)             (None, 311, 3)       1905        activation_775[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_775 (Dropout)           (None, 311, 3)       0           conv1d_756[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_597 (Concatenate)   (None, 311, 130)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "                                                                 dropout_773[0][0]                \n",
      "                                                                 dropout_774[0][0]                \n",
      "                                                                 dropout_775[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_776 (BatchN (None, 311, 130)     520         concatenate_597[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_776 (Activation)     (None, 311, 130)     0           batch_normalization_776[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_757 (Conv1D)             (None, 311, 3)       1950        activation_776[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_776 (Dropout)           (None, 311, 3)       0           conv1d_757[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_598 (Concatenate)   (None, 311, 133)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "                                                                 dropout_773[0][0]                \n",
      "                                                                 dropout_774[0][0]                \n",
      "                                                                 dropout_775[0][0]                \n",
      "                                                                 dropout_776[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_777 (BatchN (None, 311, 133)     532         concatenate_598[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_777 (Activation)     (None, 311, 133)     0           batch_normalization_777[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_758 (Conv1D)             (None, 311, 3)       1995        activation_777[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_777 (Dropout)           (None, 311, 3)       0           conv1d_758[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_599 (Concatenate)   (None, 311, 136)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "                                                                 dropout_773[0][0]                \n",
      "                                                                 dropout_774[0][0]                \n",
      "                                                                 dropout_775[0][0]                \n",
      "                                                                 dropout_776[0][0]                \n",
      "                                                                 dropout_777[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_778 (BatchN (None, 311, 136)     544         concatenate_599[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_778 (Activation)     (None, 311, 136)     0           batch_normalization_778[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_759 (Conv1D)             (None, 311, 3)       2040        activation_778[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_778 (Dropout)           (None, 311, 3)       0           conv1d_759[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_600 (Concatenate)   (None, 311, 139)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "                                                                 dropout_773[0][0]                \n",
      "                                                                 dropout_774[0][0]                \n",
      "                                                                 dropout_775[0][0]                \n",
      "                                                                 dropout_776[0][0]                \n",
      "                                                                 dropout_777[0][0]                \n",
      "                                                                 dropout_778[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_779 (BatchN (None, 311, 139)     556         concatenate_600[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_779 (Activation)     (None, 311, 139)     0           batch_normalization_779[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_760 (Conv1D)             (None, 311, 3)       2085        activation_779[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_779 (Dropout)           (None, 311, 3)       0           conv1d_760[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_601 (Concatenate)   (None, 311, 142)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "                                                                 dropout_773[0][0]                \n",
      "                                                                 dropout_774[0][0]                \n",
      "                                                                 dropout_775[0][0]                \n",
      "                                                                 dropout_776[0][0]                \n",
      "                                                                 dropout_777[0][0]                \n",
      "                                                                 dropout_778[0][0]                \n",
      "                                                                 dropout_779[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_780 (BatchN (None, 311, 142)     568         concatenate_601[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_780 (Activation)     (None, 311, 142)     0           batch_normalization_780[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_761 (Conv1D)             (None, 311, 3)       2130        activation_780[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_780 (Dropout)           (None, 311, 3)       0           conv1d_761[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_602 (Concatenate)   (None, 311, 145)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "                                                                 dropout_773[0][0]                \n",
      "                                                                 dropout_774[0][0]                \n",
      "                                                                 dropout_775[0][0]                \n",
      "                                                                 dropout_776[0][0]                \n",
      "                                                                 dropout_777[0][0]                \n",
      "                                                                 dropout_778[0][0]                \n",
      "                                                                 dropout_779[0][0]                \n",
      "                                                                 dropout_780[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_781 (BatchN (None, 311, 145)     580         concatenate_602[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_781 (Activation)     (None, 311, 145)     0           batch_normalization_781[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_762 (Conv1D)             (None, 311, 3)       2175        activation_781[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_781 (Dropout)           (None, 311, 3)       0           conv1d_762[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_603 (Concatenate)   (None, 311, 148)     0           average_pooling1d_19[0][0]       \n",
      "                                                                 dropout_760[0][0]                \n",
      "                                                                 dropout_761[0][0]                \n",
      "                                                                 dropout_762[0][0]                \n",
      "                                                                 dropout_763[0][0]                \n",
      "                                                                 dropout_764[0][0]                \n",
      "                                                                 dropout_765[0][0]                \n",
      "                                                                 dropout_766[0][0]                \n",
      "                                                                 dropout_767[0][0]                \n",
      "                                                                 dropout_768[0][0]                \n",
      "                                                                 dropout_769[0][0]                \n",
      "                                                                 dropout_770[0][0]                \n",
      "                                                                 dropout_771[0][0]                \n",
      "                                                                 dropout_772[0][0]                \n",
      "                                                                 dropout_773[0][0]                \n",
      "                                                                 dropout_774[0][0]                \n",
      "                                                                 dropout_775[0][0]                \n",
      "                                                                 dropout_776[0][0]                \n",
      "                                                                 dropout_777[0][0]                \n",
      "                                                                 dropout_778[0][0]                \n",
      "                                                                 dropout_779[0][0]                \n",
      "                                                                 dropout_780[0][0]                \n",
      "                                                                 dropout_781[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_782 (BatchN (None, 311, 148)     592         concatenate_603[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_782 (Activation)     (None, 311, 148)     0           batch_normalization_782[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_763 (Conv1D)             (None, 311, 148)     109520      activation_782[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_782 (Dropout)           (None, 311, 148)     0           conv1d_763[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_20 (AveragePo (None, 155, 148)     0           dropout_782[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_783 (BatchN (None, 155, 148)     592         average_pooling1d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_783 (Activation)     (None, 155, 148)     0           batch_normalization_783[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_764 (Conv1D)             (None, 155, 3)       2220        activation_783[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_783 (Dropout)           (None, 155, 3)       0           conv1d_764[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_604 (Concatenate)   (None, 155, 151)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_784 (BatchN (None, 155, 151)     604         concatenate_604[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_784 (Activation)     (None, 155, 151)     0           batch_normalization_784[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_765 (Conv1D)             (None, 155, 3)       2265        activation_784[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_784 (Dropout)           (None, 155, 3)       0           conv1d_765[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_605 (Concatenate)   (None, 155, 154)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_785 (BatchN (None, 155, 154)     616         concatenate_605[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_785 (Activation)     (None, 155, 154)     0           batch_normalization_785[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_766 (Conv1D)             (None, 155, 3)       2310        activation_785[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_785 (Dropout)           (None, 155, 3)       0           conv1d_766[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_606 (Concatenate)   (None, 155, 157)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_786 (BatchN (None, 155, 157)     628         concatenate_606[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_786 (Activation)     (None, 155, 157)     0           batch_normalization_786[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_767 (Conv1D)             (None, 155, 3)       2355        activation_786[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_786 (Dropout)           (None, 155, 3)       0           conv1d_767[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_607 (Concatenate)   (None, 155, 160)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_787 (BatchN (None, 155, 160)     640         concatenate_607[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_787 (Activation)     (None, 155, 160)     0           batch_normalization_787[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_768 (Conv1D)             (None, 155, 3)       2400        activation_787[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_787 (Dropout)           (None, 155, 3)       0           conv1d_768[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_608 (Concatenate)   (None, 155, 163)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_788 (BatchN (None, 155, 163)     652         concatenate_608[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_788 (Activation)     (None, 155, 163)     0           batch_normalization_788[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_769 (Conv1D)             (None, 155, 3)       2445        activation_788[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_788 (Dropout)           (None, 155, 3)       0           conv1d_769[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_609 (Concatenate)   (None, 155, 166)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_789 (BatchN (None, 155, 166)     664         concatenate_609[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_789 (Activation)     (None, 155, 166)     0           batch_normalization_789[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_770 (Conv1D)             (None, 155, 3)       2490        activation_789[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_789 (Dropout)           (None, 155, 3)       0           conv1d_770[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_610 (Concatenate)   (None, 155, 169)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_790 (BatchN (None, 155, 169)     676         concatenate_610[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_790 (Activation)     (None, 155, 169)     0           batch_normalization_790[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_771 (Conv1D)             (None, 155, 3)       2535        activation_790[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_790 (Dropout)           (None, 155, 3)       0           conv1d_771[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_611 (Concatenate)   (None, 155, 172)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_791 (BatchN (None, 155, 172)     688         concatenate_611[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_791 (Activation)     (None, 155, 172)     0           batch_normalization_791[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_772 (Conv1D)             (None, 155, 3)       2580        activation_791[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_791 (Dropout)           (None, 155, 3)       0           conv1d_772[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_612 (Concatenate)   (None, 155, 175)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_792 (BatchN (None, 155, 175)     700         concatenate_612[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_792 (Activation)     (None, 155, 175)     0           batch_normalization_792[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_773 (Conv1D)             (None, 155, 3)       2625        activation_792[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_792 (Dropout)           (None, 155, 3)       0           conv1d_773[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_613 (Concatenate)   (None, 155, 178)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_793 (BatchN (None, 155, 178)     712         concatenate_613[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_793 (Activation)     (None, 155, 178)     0           batch_normalization_793[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_774 (Conv1D)             (None, 155, 3)       2670        activation_793[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_793 (Dropout)           (None, 155, 3)       0           conv1d_774[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_614 (Concatenate)   (None, 155, 181)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_794 (BatchN (None, 155, 181)     724         concatenate_614[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_794 (Activation)     (None, 155, 181)     0           batch_normalization_794[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_775 (Conv1D)             (None, 155, 3)       2715        activation_794[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_794 (Dropout)           (None, 155, 3)       0           conv1d_775[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_615 (Concatenate)   (None, 155, 184)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_795 (BatchN (None, 155, 184)     736         concatenate_615[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_795 (Activation)     (None, 155, 184)     0           batch_normalization_795[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_776 (Conv1D)             (None, 155, 3)       2760        activation_795[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_795 (Dropout)           (None, 155, 3)       0           conv1d_776[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_616 (Concatenate)   (None, 155, 187)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_796 (BatchN (None, 155, 187)     748         concatenate_616[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_796 (Activation)     (None, 155, 187)     0           batch_normalization_796[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_777 (Conv1D)             (None, 155, 3)       2805        activation_796[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_796 (Dropout)           (None, 155, 3)       0           conv1d_777[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_617 (Concatenate)   (None, 155, 190)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "                                                                 dropout_796[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_797 (BatchN (None, 155, 190)     760         concatenate_617[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_797 (Activation)     (None, 155, 190)     0           batch_normalization_797[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_778 (Conv1D)             (None, 155, 3)       2850        activation_797[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_797 (Dropout)           (None, 155, 3)       0           conv1d_778[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_618 (Concatenate)   (None, 155, 193)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "                                                                 dropout_796[0][0]                \n",
      "                                                                 dropout_797[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_798 (BatchN (None, 155, 193)     772         concatenate_618[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_798 (Activation)     (None, 155, 193)     0           batch_normalization_798[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_779 (Conv1D)             (None, 155, 3)       2895        activation_798[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_798 (Dropout)           (None, 155, 3)       0           conv1d_779[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_619 (Concatenate)   (None, 155, 196)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "                                                                 dropout_796[0][0]                \n",
      "                                                                 dropout_797[0][0]                \n",
      "                                                                 dropout_798[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_799 (BatchN (None, 155, 196)     784         concatenate_619[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_799 (Activation)     (None, 155, 196)     0           batch_normalization_799[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_780 (Conv1D)             (None, 155, 3)       2940        activation_799[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_799 (Dropout)           (None, 155, 3)       0           conv1d_780[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_620 (Concatenate)   (None, 155, 199)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "                                                                 dropout_796[0][0]                \n",
      "                                                                 dropout_797[0][0]                \n",
      "                                                                 dropout_798[0][0]                \n",
      "                                                                 dropout_799[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_800 (BatchN (None, 155, 199)     796         concatenate_620[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_800 (Activation)     (None, 155, 199)     0           batch_normalization_800[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_781 (Conv1D)             (None, 155, 3)       2985        activation_800[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_800 (Dropout)           (None, 155, 3)       0           conv1d_781[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_621 (Concatenate)   (None, 155, 202)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "                                                                 dropout_796[0][0]                \n",
      "                                                                 dropout_797[0][0]                \n",
      "                                                                 dropout_798[0][0]                \n",
      "                                                                 dropout_799[0][0]                \n",
      "                                                                 dropout_800[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_801 (BatchN (None, 155, 202)     808         concatenate_621[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_801 (Activation)     (None, 155, 202)     0           batch_normalization_801[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_782 (Conv1D)             (None, 155, 3)       3030        activation_801[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_801 (Dropout)           (None, 155, 3)       0           conv1d_782[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_622 (Concatenate)   (None, 155, 205)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "                                                                 dropout_796[0][0]                \n",
      "                                                                 dropout_797[0][0]                \n",
      "                                                                 dropout_798[0][0]                \n",
      "                                                                 dropout_799[0][0]                \n",
      "                                                                 dropout_800[0][0]                \n",
      "                                                                 dropout_801[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_802 (BatchN (None, 155, 205)     820         concatenate_622[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_802 (Activation)     (None, 155, 205)     0           batch_normalization_802[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_783 (Conv1D)             (None, 155, 3)       3075        activation_802[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_802 (Dropout)           (None, 155, 3)       0           conv1d_783[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_623 (Concatenate)   (None, 155, 208)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "                                                                 dropout_796[0][0]                \n",
      "                                                                 dropout_797[0][0]                \n",
      "                                                                 dropout_798[0][0]                \n",
      "                                                                 dropout_799[0][0]                \n",
      "                                                                 dropout_800[0][0]                \n",
      "                                                                 dropout_801[0][0]                \n",
      "                                                                 dropout_802[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_803 (BatchN (None, 155, 208)     832         concatenate_623[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_803 (Activation)     (None, 155, 208)     0           batch_normalization_803[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_784 (Conv1D)             (None, 155, 3)       3120        activation_803[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_803 (Dropout)           (None, 155, 3)       0           conv1d_784[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_624 (Concatenate)   (None, 155, 211)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "                                                                 dropout_796[0][0]                \n",
      "                                                                 dropout_797[0][0]                \n",
      "                                                                 dropout_798[0][0]                \n",
      "                                                                 dropout_799[0][0]                \n",
      "                                                                 dropout_800[0][0]                \n",
      "                                                                 dropout_801[0][0]                \n",
      "                                                                 dropout_802[0][0]                \n",
      "                                                                 dropout_803[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_804 (BatchN (None, 155, 211)     844         concatenate_624[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_804 (Activation)     (None, 155, 211)     0           batch_normalization_804[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_785 (Conv1D)             (None, 155, 3)       3165        activation_804[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_804 (Dropout)           (None, 155, 3)       0           conv1d_785[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_625 (Concatenate)   (None, 155, 214)     0           average_pooling1d_20[0][0]       \n",
      "                                                                 dropout_783[0][0]                \n",
      "                                                                 dropout_784[0][0]                \n",
      "                                                                 dropout_785[0][0]                \n",
      "                                                                 dropout_786[0][0]                \n",
      "                                                                 dropout_787[0][0]                \n",
      "                                                                 dropout_788[0][0]                \n",
      "                                                                 dropout_789[0][0]                \n",
      "                                                                 dropout_790[0][0]                \n",
      "                                                                 dropout_791[0][0]                \n",
      "                                                                 dropout_792[0][0]                \n",
      "                                                                 dropout_793[0][0]                \n",
      "                                                                 dropout_794[0][0]                \n",
      "                                                                 dropout_795[0][0]                \n",
      "                                                                 dropout_796[0][0]                \n",
      "                                                                 dropout_797[0][0]                \n",
      "                                                                 dropout_798[0][0]                \n",
      "                                                                 dropout_799[0][0]                \n",
      "                                                                 dropout_800[0][0]                \n",
      "                                                                 dropout_801[0][0]                \n",
      "                                                                 dropout_802[0][0]                \n",
      "                                                                 dropout_803[0][0]                \n",
      "                                                                 dropout_804[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_805 (BatchN (None, 155, 214)     856         concatenate_625[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_805 (Activation)     (None, 155, 214)     0           batch_normalization_805[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 33170)        0           activation_805[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 20)           663400      flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_805 (Dropout)           (None, 20)           0           dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 2)            42          dropout_805[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 951,803\n",
      "Trainable params: 935,837\n",
      "Non-trainable params: 15,966\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 249s 199ms/step - loss: 1.1201 - acc: 0.8281 - val_loss: 0.7359 - val_acc: 0.6410\n",
      "6710/6710 [==============================] - 26s 4ms/step\n",
      "TN:55,FP:91,FN:13,TP:125,Macc:0.641254664816,F1:0.706209412436\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 229s 183ms/step - loss: 0.3807 - acc: 0.8513 - val_loss: 0.6068 - val_acc: 0.7434\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:94,FP:52,FN:31,TP:107,Macc:0.709598912483,F1:0.720533194704\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.3207 - acc: 0.8768 - val_loss: 1.2560 - val_acc: 0.5735\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:130,TP:8,Macc:0.528985467265,F1:0.109587885167\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2969 - acc: 0.8857 - val_loss: 2.8816 - val_acc: 0.5700\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:130,TP:8,Macc:0.528985467265,F1:0.109587885167\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 223s 178ms/step - loss: 0.2833 - acc: 0.8887 - val_loss: 2.7630 - val_acc: 0.5636\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:133,TP:5,Macc:0.518115902914,F1:0.0699293168434\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2739 - acc: 0.8910 - val_loss: 3.4063 - val_acc: 0.5598\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:135,TP:3,Macc:0.51086952668,F1:0.0425527267274\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2696 - acc: 0.8920 - val_loss: 2.3113 - val_acc: 0.5636\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:133,TP:5,Macc:0.518115902914,F1:0.0699293168434\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2639 - acc: 0.8941 - val_loss: 2.8239 - val_acc: 0.5686\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:130,TP:8,Macc:0.528985467265,F1:0.109587885167\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2585 - acc: 0.8964 - val_loss: 2.6749 - val_acc: 0.5566\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:137,TP:1,Macc:0.503623150446,F1:0.0143883297966\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2571 - acc: 0.8967 - val_loss: 2.2274 - val_acc: 0.5686\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:131,TP:7,Macc:0.525362279148,F1:0.0965506987013\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 224s 180ms/step - loss: 0.2553 - acc: 0.8956 - val_loss: 3.0511 - val_acc: 0.5620\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:135,TP:3,Macc:0.51086952668,F1:0.0425527267274\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2501 - acc: 0.8997 - val_loss: 2.6041 - val_acc: 0.5718\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:131,TP:7,Macc:0.525362279148,F1:0.0965506987013\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2473 - acc: 0.8997 - val_loss: 2.1294 - val_acc: 0.5806\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:125,TP:13,Macc:0.54710140785,F1:0.172183674417\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2452 - acc: 0.9009 - val_loss: 2.1234 - val_acc: 0.5909\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:121,TP:17,Macc:0.561594160318,F1:0.219352659333\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2433 - acc: 0.9000 - val_loss: 2.6030 - val_acc: 0.5726\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:130,TP:8,Macc:0.528985467265,F1:0.109587885167\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2420 - acc: 0.9012 - val_loss: 2.5844 - val_acc: 0.5639\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:134,TP:4,Macc:0.514492714797,F1:0.0563374171841\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2392 - acc: 0.9031 - val_loss: 2.4116 - val_acc: 0.5678\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:131,TP:7,Macc:0.525362279148,F1:0.0965506987013\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2371 - acc: 0.9041 - val_loss: 2.4724 - val_acc: 0.5705\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:130,TP:8,Macc:0.528985467265,F1:0.109587885167\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2363 - acc: 0.9048 - val_loss: 2.5616 - val_acc: 0.5711\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:131,TP:7,Macc:0.525362279148,F1:0.0965506987013\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 223s 179ms/step - loss: 0.2368 - acc: 0.9039 - val_loss: 2.1075 - val_acc: 0.5757\n",
      "6710/6710 [==============================] - 9s 1ms/step\n",
      "TN:146,FP:0,FN:129,TP:9,Macc:0.532608655382,F1:0.122447696805\n",
      "Loss: 2\n",
      "args (15.0, 1, 6.0)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_81 (Con (None, 2500, 1)      31          input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_82 (Con (None, 2500, 1)      31          input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_83 (Con (None, 2500, 1)      31          input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_linearphase_type_84 (Con (None, 2500, 1)      31          input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_786 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_81[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_788 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_82[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_790 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_83[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_792 (Conv1D)             (None, 2496, 8)      40          conv1d_linearphase_type_84[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_806 (BatchN (None, 2496, 8)      32          conv1d_786[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_808 (BatchN (None, 2496, 8)      32          conv1d_788[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_810 (BatchN (None, 2496, 8)      32          conv1d_790[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_812 (BatchN (None, 2496, 8)      32          conv1d_792[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_806 (Activation)     (None, 2496, 8)      0           batch_normalization_806[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_808 (Activation)     (None, 2496, 8)      0           batch_normalization_808[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_810 (Activation)     (None, 2496, 8)      0           batch_normalization_810[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_812 (Activation)     (None, 2496, 8)      0           batch_normalization_812[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_806 (Dropout)           (None, 2496, 8)      0           activation_806[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_808 (Dropout)           (None, 2496, 8)      0           activation_808[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_810 (Dropout)           (None, 2496, 8)      0           activation_810[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_812 (Dropout)           (None, 2496, 8)      0           activation_812[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_161 (MaxPooling1D (None, 1248, 8)      0           dropout_806[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_163 (MaxPooling1D (None, 1248, 8)      0           dropout_808[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_165 (MaxPooling1D (None, 1248, 8)      0           dropout_810[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_167 (MaxPooling1D (None, 1248, 8)      0           dropout_812[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_787 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_161[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_789 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_163[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_791 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_165[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_793 (Conv1D)             (None, 1244, 4)      160         max_pooling1d_167[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_807 (BatchN (None, 1244, 4)      16          conv1d_787[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_809 (BatchN (None, 1244, 4)      16          conv1d_789[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_811 (BatchN (None, 1244, 4)      16          conv1d_791[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_813 (BatchN (None, 1244, 4)      16          conv1d_793[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_807 (Activation)     (None, 1244, 4)      0           batch_normalization_807[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_809 (Activation)     (None, 1244, 4)      0           batch_normalization_809[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_811 (Activation)     (None, 1244, 4)      0           batch_normalization_811[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_813 (Activation)     (None, 1244, 4)      0           batch_normalization_813[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_807 (Dropout)           (None, 1244, 4)      0           activation_807[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_809 (Dropout)           (None, 1244, 4)      0           activation_809[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_811 (Dropout)           (None, 1244, 4)      0           activation_811[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_813 (Dropout)           (None, 1244, 4)      0           activation_813[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_162 (MaxPooling1D (None, 622, 4)       0           dropout_807[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_164 (MaxPooling1D (None, 622, 4)       0           dropout_809[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_166 (MaxPooling1D (None, 622, 4)       0           dropout_811[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_168 (MaxPooling1D (None, 622, 4)       0           dropout_813[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_626 (Concatenate)   (None, 622, 16)      0           max_pooling1d_162[0][0]          \n",
      "                                                                 max_pooling1d_164[0][0]          \n",
      "                                                                 max_pooling1d_166[0][0]          \n",
      "                                                                 max_pooling1d_168[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_814 (BatchN (None, 622, 16)      64          concatenate_626[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_814 (Activation)     (None, 622, 16)      0           batch_normalization_814[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_794 (Conv1D)             (None, 622, 6)       480         activation_814[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_814 (Dropout)           (None, 622, 6)       0           conv1d_794[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_627 (Concatenate)   (None, 622, 22)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_815 (BatchN (None, 622, 22)      88          concatenate_627[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_815 (Activation)     (None, 622, 22)      0           batch_normalization_815[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_795 (Conv1D)             (None, 622, 6)       660         activation_815[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_815 (Dropout)           (None, 622, 6)       0           conv1d_795[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_628 (Concatenate)   (None, 622, 28)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_816 (BatchN (None, 622, 28)      112         concatenate_628[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_816 (Activation)     (None, 622, 28)      0           batch_normalization_816[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_796 (Conv1D)             (None, 622, 6)       840         activation_816[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_816 (Dropout)           (None, 622, 6)       0           conv1d_796[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_629 (Concatenate)   (None, 622, 34)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_817 (BatchN (None, 622, 34)      136         concatenate_629[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_817 (Activation)     (None, 622, 34)      0           batch_normalization_817[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_797 (Conv1D)             (None, 622, 6)       1020        activation_817[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_817 (Dropout)           (None, 622, 6)       0           conv1d_797[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_630 (Concatenate)   (None, 622, 40)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_818 (BatchN (None, 622, 40)      160         concatenate_630[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_818 (Activation)     (None, 622, 40)      0           batch_normalization_818[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_798 (Conv1D)             (None, 622, 6)       1200        activation_818[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_818 (Dropout)           (None, 622, 6)       0           conv1d_798[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_631 (Concatenate)   (None, 622, 46)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_819 (BatchN (None, 622, 46)      184         concatenate_631[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_819 (Activation)     (None, 622, 46)      0           batch_normalization_819[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_799 (Conv1D)             (None, 622, 6)       1380        activation_819[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_819 (Dropout)           (None, 622, 6)       0           conv1d_799[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_632 (Concatenate)   (None, 622, 52)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_820 (BatchN (None, 622, 52)      208         concatenate_632[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_820 (Activation)     (None, 622, 52)      0           batch_normalization_820[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_800 (Conv1D)             (None, 622, 6)       1560        activation_820[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_820 (Dropout)           (None, 622, 6)       0           conv1d_800[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_633 (Concatenate)   (None, 622, 58)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "                                                                 dropout_820[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_821 (BatchN (None, 622, 58)      232         concatenate_633[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_821 (Activation)     (None, 622, 58)      0           batch_normalization_821[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_801 (Conv1D)             (None, 622, 6)       1740        activation_821[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_821 (Dropout)           (None, 622, 6)       0           conv1d_801[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_634 (Concatenate)   (None, 622, 64)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "                                                                 dropout_820[0][0]                \n",
      "                                                                 dropout_821[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_822 (BatchN (None, 622, 64)      256         concatenate_634[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_822 (Activation)     (None, 622, 64)      0           batch_normalization_822[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_802 (Conv1D)             (None, 622, 6)       1920        activation_822[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_822 (Dropout)           (None, 622, 6)       0           conv1d_802[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_635 (Concatenate)   (None, 622, 70)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "                                                                 dropout_820[0][0]                \n",
      "                                                                 dropout_821[0][0]                \n",
      "                                                                 dropout_822[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_823 (BatchN (None, 622, 70)      280         concatenate_635[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_823 (Activation)     (None, 622, 70)      0           batch_normalization_823[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_803 (Conv1D)             (None, 622, 6)       2100        activation_823[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_823 (Dropout)           (None, 622, 6)       0           conv1d_803[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_636 (Concatenate)   (None, 622, 76)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "                                                                 dropout_820[0][0]                \n",
      "                                                                 dropout_821[0][0]                \n",
      "                                                                 dropout_822[0][0]                \n",
      "                                                                 dropout_823[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_824 (BatchN (None, 622, 76)      304         concatenate_636[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_824 (Activation)     (None, 622, 76)      0           batch_normalization_824[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_804 (Conv1D)             (None, 622, 6)       2280        activation_824[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_824 (Dropout)           (None, 622, 6)       0           conv1d_804[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_637 (Concatenate)   (None, 622, 82)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "                                                                 dropout_820[0][0]                \n",
      "                                                                 dropout_821[0][0]                \n",
      "                                                                 dropout_822[0][0]                \n",
      "                                                                 dropout_823[0][0]                \n",
      "                                                                 dropout_824[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_825 (BatchN (None, 622, 82)      328         concatenate_637[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_825 (Activation)     (None, 622, 82)      0           batch_normalization_825[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_805 (Conv1D)             (None, 622, 6)       2460        activation_825[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_825 (Dropout)           (None, 622, 6)       0           conv1d_805[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_638 (Concatenate)   (None, 622, 88)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "                                                                 dropout_820[0][0]                \n",
      "                                                                 dropout_821[0][0]                \n",
      "                                                                 dropout_822[0][0]                \n",
      "                                                                 dropout_823[0][0]                \n",
      "                                                                 dropout_824[0][0]                \n",
      "                                                                 dropout_825[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_826 (BatchN (None, 622, 88)      352         concatenate_638[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_826 (Activation)     (None, 622, 88)      0           batch_normalization_826[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_806 (Conv1D)             (None, 622, 6)       2640        activation_826[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_826 (Dropout)           (None, 622, 6)       0           conv1d_806[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_639 (Concatenate)   (None, 622, 94)      0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "                                                                 dropout_820[0][0]                \n",
      "                                                                 dropout_821[0][0]                \n",
      "                                                                 dropout_822[0][0]                \n",
      "                                                                 dropout_823[0][0]                \n",
      "                                                                 dropout_824[0][0]                \n",
      "                                                                 dropout_825[0][0]                \n",
      "                                                                 dropout_826[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_827 (BatchN (None, 622, 94)      376         concatenate_639[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_827 (Activation)     (None, 622, 94)      0           batch_normalization_827[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_807 (Conv1D)             (None, 622, 6)       2820        activation_827[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_827 (Dropout)           (None, 622, 6)       0           conv1d_807[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_640 (Concatenate)   (None, 622, 100)     0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "                                                                 dropout_820[0][0]                \n",
      "                                                                 dropout_821[0][0]                \n",
      "                                                                 dropout_822[0][0]                \n",
      "                                                                 dropout_823[0][0]                \n",
      "                                                                 dropout_824[0][0]                \n",
      "                                                                 dropout_825[0][0]                \n",
      "                                                                 dropout_826[0][0]                \n",
      "                                                                 dropout_827[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_828 (BatchN (None, 622, 100)     400         concatenate_640[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_828 (Activation)     (None, 622, 100)     0           batch_normalization_828[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_808 (Conv1D)             (None, 622, 6)       3000        activation_828[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_828 (Dropout)           (None, 622, 6)       0           conv1d_808[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_641 (Concatenate)   (None, 622, 106)     0           concatenate_626[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "                                                                 dropout_815[0][0]                \n",
      "                                                                 dropout_816[0][0]                \n",
      "                                                                 dropout_817[0][0]                \n",
      "                                                                 dropout_818[0][0]                \n",
      "                                                                 dropout_819[0][0]                \n",
      "                                                                 dropout_820[0][0]                \n",
      "                                                                 dropout_821[0][0]                \n",
      "                                                                 dropout_822[0][0]                \n",
      "                                                                 dropout_823[0][0]                \n",
      "                                                                 dropout_824[0][0]                \n",
      "                                                                 dropout_825[0][0]                \n",
      "                                                                 dropout_826[0][0]                \n",
      "                                                                 dropout_827[0][0]                \n",
      "                                                                 dropout_828[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_829 (BatchN (None, 622, 106)     424         concatenate_641[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_829 (Activation)     (None, 622, 106)     0           batch_normalization_829[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 65932)        0           activation_829[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 20)           1318640     flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_829 (Dropout)           (None, 20)           0           dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 2)            42          dropout_829[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,349,802\n",
      "Trainable params: 1,347,754\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 238s 191ms/step - loss: 0.8424 - acc: 0.8184 - val_loss: 0.6879 - val_acc: 0.6058\n",
      "6710/6710 [==============================] - 19s 3ms/step\n",
      "TN:40,FP:106,FN:7,TP:131,Macc:0.611623934375,F1:0.698661509044\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 88s 71ms/step - loss: 0.3502 - acc: 0.8378 - val_loss: 0.6722 - val_acc: 0.6234\n",
      "6710/6710 [==============================] - 4s 612us/step\n",
      "TN:60,FP:86,FN:20,TP:118,Macc:0.633015634379,F1:0.690053140015\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 82s 66ms/step - loss: 0.3259 - acc: 0.8437 - val_loss: 0.6407 - val_acc: 0.6310\n",
      "6710/6710 [==============================] - 4s 602us/step\n",
      "TN:52,FP:94,FN:10,TP:128,Macc:0.641850257339,F1:0.711105867137\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.3147 - acc: 0.8556 - val_loss: 0.6583 - val_acc: 0.5885\n",
      "6710/6710 [==============================] - 4s 609us/step\n",
      "TN:41,FP:105,FN:6,TP:132,Macc:0.618671779768,F1:0.703994842064\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2977 - acc: 0.8727 - val_loss: 0.6393 - val_acc: 0.6377\n",
      "6710/6710 [==============================] - 4s 596us/step\n",
      "TN:57,FP:89,FN:14,TP:124,Macc:0.644480791252,F1:0.70654741342\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2886 - acc: 0.8778 - val_loss: 0.6189 - val_acc: 0.6782\n",
      "6710/6710 [==============================] - 4s 618us/step\n",
      "TN:86,FP:60,FN:38,TP:100,Macc:0.656839337455,F1:0.671135420071\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2844 - acc: 0.8801 - val_loss: 2.1660 - val_acc: 0.5639\n",
      "6710/6710 [==============================] - 4s 612us/step\n",
      "TN:146,FP:0,FN:135,TP:3,Macc:0.51086952668,F1:0.0425527267274\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2797 - acc: 0.8821 - val_loss: 3.9392 - val_acc: 0.5592\n",
      "6710/6710 [==============================] - 4s 642us/step\n",
      "TN:146,FP:0,FN:135,TP:3,Macc:0.51086952668,F1:0.0425527267274\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 82s 66ms/step - loss: 0.2750 - acc: 0.8840 - val_loss: 2.0987 - val_acc: 0.5857\n",
      "6710/6710 [==============================] - 4s 605us/step\n",
      "TN:140,FP:6,FN:117,TP:21,Macc:0.555538969128,F1:0.254542409732\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 83s 67ms/step - loss: 0.2725 - acc: 0.8864 - val_loss: 2.0695 - val_acc: 0.5805\n",
      "6710/6710 [==============================] - 4s 617us/step\n",
      "TN:144,FP:2,FN:121,TP:17,Macc:0.554744845765,F1:0.216558139016\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 82s 66ms/step - loss: 0.2701 - acc: 0.8852 - val_loss: 3.5202 - val_acc: 0.5580\n",
      "6710/6710 [==============================] - 4s 634us/step\n",
      "TN:146,FP:0,FN:136,TP:2,Macc:0.507246338563,F1:0.0285711142874\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2668 - acc: 0.8891 - val_loss: 3.8236 - val_acc: 0.5557\n",
      "6710/6710 [==============================] - 4s 604us/step\n",
      "TN:146,FP:0,FN:137,TP:1,Macc:0.503623150446,F1:0.0143883297966\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 82s 66ms/step - loss: 0.2630 - acc: 0.8906 - val_loss: 2.5687 - val_acc: 0.5779\n",
      "6710/6710 [==============================] - 4s 603us/step\n",
      "TN:143,FP:3,FN:119,TP:19,Macc:0.558566564723,F1:0.237497358309\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2611 - acc: 0.8907 - val_loss: 2.5472 - val_acc: 0.5677\n",
      "6710/6710 [==============================] - 4s 632us/step\n",
      "TN:146,FP:0,FN:131,TP:7,Macc:0.525362279148,F1:0.0965506987013\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2606 - acc: 0.8903 - val_loss: 3.7644 - val_acc: 0.5627\n",
      "6710/6710 [==============================] - 4s 615us/step\n",
      "TN:146,FP:0,FN:133,TP:5,Macc:0.518115902914,F1:0.0699293168434\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2578 - acc: 0.8916 - val_loss: 2.4109 - val_acc: 0.5651\n",
      "6710/6710 [==============================] - 4s 616us/step\n",
      "TN:146,FP:0,FN:131,TP:7,Macc:0.525362279148,F1:0.0965506987013\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 82s 66ms/step - loss: 0.2565 - acc: 0.8920 - val_loss: 2.0674 - val_acc: 0.5678\n",
      "6710/6710 [==============================] - 4s 597us/step\n",
      "TN:146,FP:0,FN:133,TP:5,Macc:0.518115902914,F1:0.0699293168434\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2537 - acc: 0.8936 - val_loss: 1.6181 - val_acc: 0.5857\n",
      "6710/6710 [==============================] - 4s 599us/step\n",
      "TN:141,FP:5,FN:118,TP:20,Macc:0.555340438287,F1:0.245395883205\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 82s 66ms/step - loss: 0.2532 - acc: 0.8941 - val_loss: 1.6221 - val_acc: 0.5778\n",
      "6710/6710 [==============================] - 4s 619us/step\n",
      "TN:145,FP:1,FN:128,TP:10,Macc:0.532807186222,F1:0.13422666386\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 83s 66ms/step - loss: 0.2556 - acc: 0.8927 - val_loss: 1.5103 - val_acc: 0.5811\n",
      "6710/6710 [==============================] - 4s 635us/step\n",
      "TN:145,FP:1,FN:124,TP:14,Macc:0.54729993869,F1:0.183004564247\n",
      "Loss: 1\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, Trials, fmin, tpe\n",
    "trials = Trials()\n",
    "best = fmin(objective,\n",
    "    space=[hp.quniform('depth',1,30,1),\n",
    "           hp.choice('num_block',[1,2,3]),\n",
    "           hp.quniform('growth',1,20,1),\n",
    "#            hp.choice('kernel_size',[3,5,7]),\n",
    "#            hp.choice('num_filters',[8,16,32]),\n",
    "#            hp.normal('dropout',0.4,0.1)\n",
    "          ],\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 14.0, 'growth': 1.0, 'num_block': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "D =(params)\n",
    "\n",
    "plt.bar(range(len(D)), list(D.values()), align='center')\n",
    "plt.xticks(range(len(D)), list(D.keys()),rotation=90)\n",
    "plt.yscale('log',nonposy='clip')\n",
    "# # for python 2.x:\n",
    "# plt.bar(range(len(D)), D.values(), align='center')  # python 2.x\n",
    "# plt.xticks(range(len(D)), D.keys())  # in python 2.x\n",
    "\n",
    "plt.show()\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkOpt for Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "args [5, 4, 3]\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 69s 57ms/step - loss: 0.8349 - acc: 0.8521 - val_loss: 0.6600 - val_acc: 0.7734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/callbacks.py:435: RuntimeWarning: Can save best model only with val_F1 available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:124,FP:47,FN:37,TP:127,Macc:0.749768172073,F1:0.751473745882\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.3358 - acc: 0.8736 - val_loss: 0.5833 - val_acc: 0.7479\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:90,FP:81,FN:4,TP:160,Macc:0.750962723139,F1:0.790118112713\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.3019 - acc: 0.8814 - val_loss: 0.5093 - val_acc: 0.8058\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:104,FP:67,FN:5,TP:159,Macc:0.788849612737,F1:0.815379208424\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2828 - acc: 0.8861 - val_loss: 0.5635 - val_acc: 0.7538\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:89,FP:82,FN:5,TP:159,Macc:0.744989966435,F1:0.785179841377\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2727 - acc: 0.8902 - val_loss: 0.4954 - val_acc: 0.8347\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:129,FP:42,FN:15,TP:149,Macc:0.831461220406,F1:0.839431099548\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2632 - acc: 0.8923 - val_loss: 0.5121 - val_acc: 0.7734\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:95,FP:76,FN:4,TP:160,Macc:0.765582605239,F1:0.799994634235\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2606 - acc: 0.8915 - val_loss: 0.5312 - val_acc: 0.7667\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:94,FP:77,FN:7,TP:157,Macc:0.753512287969,F1:0.788939350179\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2522 - acc: 0.8962 - val_loss: 0.6579 - val_acc: 0.7858\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:121,FP:50,FN:25,TP:139,Macc:0.777581606212,F1:0.787529889308\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2490 - acc: 0.8972 - val_loss: 0.5313 - val_acc: 0.8042\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:127,FP:44,FN:19,TP:145,Macc:0.813418146433,F1:0.821524221465\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2451 - acc: 0.8972 - val_loss: 0.4822 - val_acc: 0.8227\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:128,FP:43,FN:14,TP:150,Macc:0.831586024269,F1:0.840330618997\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2425 - acc: 0.8993 - val_loss: 1.0356 - val_acc: 0.6734\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:156,FP:15,FN:92,TP:72,Macc:0.675652501933,F1:0.573700146644\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2401 - acc: 0.9002 - val_loss: 0.4804 - val_acc: 0.8265\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:137,FP:34,FN:26,TP:138,Macc:0.82131644865,F1:0.821423020799\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2369 - acc: 0.9003 - val_loss: 0.4960 - val_acc: 0.8395\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:140,FP:31,FN:18,TP:146,Macc:0.854478620177,F1:0.856299438121\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2341 - acc: 0.9021 - val_loss: 0.5785 - val_acc: 0.7935\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:155,FP:16,FN:49,TP:115,Macc:0.803826077696,F1:0.779655527668\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2333 - acc: 0.9025 - val_loss: 0.6990 - val_acc: 0.7607\n",
      "9639/9639 [==============================] - 3s 259us/step\n",
      "TN:166,FP:5,FN:74,TP:90,Macc:0.759770311234,F1:0.694975526342\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2308 - acc: 0.9035 - val_loss: 0.6222 - val_acc: 0.7471\n",
      "9639/9639 [==============================] - 2s 257us/step\n",
      "TN:158,FP:13,FN:70,TP:94,Macc:0.748573621006,F1:0.69372162431\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2300 - acc: 0.9041 - val_loss: 0.3685 - val_acc: 0.8554\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:150,FP:21,FN:23,TP:141,Macc:0.868474482961,F1:0.865025116712\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2273 - acc: 0.9046 - val_loss: 0.7015 - val_acc: 0.7483\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:156,FP:15,FN:65,TP:99,Macc:0.757969569583,F1:0.712224837419\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2245 - acc: 0.9054 - val_loss: 0.3206 - val_acc: 0.8770\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:144,FP:27,FN:9,TP:155,Macc:0.893613548407,F1:0.895948215176\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2223 - acc: 0.9068 - val_loss: 0.4471 - val_acc: 0.8078\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:148,FP:23,FN:37,TP:127,Macc:0.819943606155,F1:0.808911651747\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2208 - acc: 0.9081 - val_loss: 0.8339 - val_acc: 0.7000\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:162,FP:9,FN:81,TP:83,Macc:0.72673294357,F1:0.648432379373\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 58s 48ms/step - loss: 0.2204 - acc: 0.9076 - val_loss: 0.4992 - val_acc: 0.7999\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:153,FP:18,FN:41,TP:123,Macc:0.822368367122,F1:0.806551850185\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 58s 49ms/step - loss: 0.2178 - acc: 0.9090 - val_loss: 0.6025 - val_acc: 0.7586\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:159,FP:12,FN:71,TP:93,Macc:0.748448817143,F1:0.691444522199\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 58s 48ms/step - loss: 0.2175 - acc: 0.9092 - val_loss: 0.8011 - val_acc: 0.7064\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:158,FP:13,FN:84,TP:80,Macc:0.70589069704,F1:0.622562959905\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 58s 48ms/step - loss: 0.2147 - acc: 0.9110 - val_loss: 0.3683 - val_acc: 0.8495\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:146,FP:25,FN:20,TP:144,Macc:0.865924918131,F1:0.864859309002\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 58s 48ms/step - loss: 0.2139 - acc: 0.9096 - val_loss: 0.5836 - val_acc: 0.7734\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:161,FP:10,FN:66,TP:98,Macc:0.7695406714,F1:0.720582910181\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2142 - acc: 0.9107 - val_loss: 0.3327 - val_acc: 0.8726\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:142,FP:29,FN:12,TP:152,Macc:0.878619254717,F1:0.881153877488\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2111 - acc: 0.9118 - val_loss: 0.3284 - val_acc: 0.8670\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:142,FP:29,FN:10,TP:154,Macc:0.884716815284,F1:0.887602529413\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2090 - acc: 0.9119 - val_loss: 1.1164 - val_acc: 0.6552\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:158,FP:13,FN:98,TP:66,Macc:0.663207773074,F1:0.543205000365\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 58s 48ms/step - loss: 0.2085 - acc: 0.9136 - val_loss: 0.8660 - val_acc: 0.7187\n",
      "9639/9639 [==============================] - 3s 259us/step\n",
      "TN:153,FP:18,FN:70,TP:94,Macc:0.733953738906,F1:0.681154061268\n",
      "WARNING:tensorflow:Variable /= will be deprecated. Use variable.assign_div if you want assignment to the variable value or 'x = x / y' if you want a new python Tensor object.\n",
      "Loss: 0.849474\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 1845.4696\n",
      "Function value obtained: 0.8495\n",
      "Current minimum: 0.8495\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "args [4, 3, 5]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.8137 - acc: 0.8376 - val_loss: 0.6578 - val_acc: 0.7746\n",
      "9639/9639 [==============================] - 3s 319us/step\n",
      "TN:98,FP:73,FN:9,TP:155,Macc:0.759110633083,F1:0.79081092879\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3369 - acc: 0.8477 - val_loss: 0.7502 - val_acc: 0.7558\n",
      "9639/9639 [==============================] - 2s 221us/step\n",
      "TN:144,FP:27,FN:67,TP:97,Macc:0.716784291975,F1:0.673605665794\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3091 - acc: 0.8659 - val_loss: 0.5895 - val_acc: 0.7316\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:89,FP:82,FN:9,TP:155,Macc:0.732794845302,F1:0.773061971567\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2899 - acc: 0.8795 - val_loss: 0.4887 - val_acc: 0.8271\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:113,FP:58,FN:6,TP:158,Macc:0.812116620234,F1:0.831573502251\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2770 - acc: 0.8858 - val_loss: 0.4744 - val_acc: 0.8257\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:121,FP:50,FN:14,TP:150,Macc:0.811118189329,F1:0.824170328197\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2684 - acc: 0.8883 - val_loss: 0.4807 - val_acc: 0.8178\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:140,FP:31,FN:30,TP:134,Macc:0.817893256777,F1:0.81458411127\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2652 - acc: 0.8883 - val_loss: 0.4038 - val_acc: 0.8625\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:132,FP:39,FN:12,TP:152,Macc:0.849379490516,F1:0.85633250695\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2588 - acc: 0.8900 - val_loss: 1.7204 - val_acc: 0.5539\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:157,FP:14,FN:117,TP:47,Macc:0.602356971271,F1:0.417773389556\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2548 - acc: 0.8920 - val_loss: 0.6931 - val_acc: 0.6783\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:153,FP:18,FN:77,TP:87,Macc:0.712612276923,F1:0.646834860423\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2517 - acc: 0.8940 - val_loss: 0.6112 - val_acc: 0.6465\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:150,FP:21,FN:83,TP:81,Macc:0.685547665963,F1:0.609017304867\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2473 - acc: 0.8956 - val_loss: 0.4041 - val_acc: 0.8652\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:136,FP:35,FN:13,TP:151,Macc:0.858026615913,F1:0.862851610386\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2462 - acc: 0.8966 - val_loss: 0.4641 - val_acc: 0.7952\n",
      "9639/9639 [==============================] - 2s 222us/step\n",
      "TN:148,FP:23,FN:39,TP:125,Macc:0.813846045588,F1:0.801276509283\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2432 - acc: 0.8969 - val_loss: 1.3872 - val_acc: 0.5291\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:164,FP:7,FN:134,TP:30,Macc:0.570995541395,F1:0.298504125776\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2424 - acc: 0.8968 - val_loss: 1.1110 - val_acc: 0.5281\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:169,FP:2,FN:146,TP:18,Macc:0.549030060096,F1:0.195650019162\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2412 - acc: 0.8978 - val_loss: 1.9070 - val_acc: 0.5017\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:169,FP:2,FN:149,TP:15,Macc:0.539883719246,F1:0.165743964002\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2391 - acc: 0.8998 - val_loss: 0.7732 - val_acc: 0.6832\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:149,FP:22,FN:75,TP:89,Macc:0.707013931809,F1:0.647267379825\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2368 - acc: 0.8999 - val_loss: 0.4412 - val_acc: 0.8258\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:156,FP:15,FN:40,TP:124,Macc:0.834189076666,F1:0.818476326235\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2355 - acc: 0.9001 - val_loss: 0.4985 - val_acc: 0.7796\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:151,FP:20,FN:48,TP:116,Macc:0.795178952299,F1:0.773327824572\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2335 - acc: 0.9007 - val_loss: 0.9440 - val_acc: 0.5307\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:170,FP:1,FN:136,TP:28,Macc:0.582441839349,F1:0.290152598378\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2310 - acc: 0.9019 - val_loss: 1.9020 - val_acc: 0.5045\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:171,FP:0,FN:147,TP:17,Macc:0.551829232653,F1:0.187843408827\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2305 - acc: 0.9020 - val_loss: 1.4262 - val_acc: 0.5252\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:171,FP:0,FN:141,TP:23,Macc:0.570121914352,F1:0.245986902822\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2305 - acc: 0.9032 - val_loss: 2.0563 - val_acc: 0.5082\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:171,FP:0,FN:149,TP:15,Macc:0.545731672086,F1:0.167596055695\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2283 - acc: 0.9045 - val_loss: 0.7418 - val_acc: 0.6549\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:168,FP:3,FN:103,TP:61,Macc:0.677203635858,F1:0.535083225722\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2283 - acc: 0.9033 - val_loss: 0.9625 - val_acc: 0.5541\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:167,FP:4,FN:131,TP:33,Macc:0.588913811505,F1:0.328354868774\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2276 - acc: 0.9035 - val_loss: 1.4969 - val_acc: 0.5364\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:169,FP:2,FN:135,TP:29,Macc:0.582566643212,F1:0.297432922474\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2271 - acc: 0.9034 - val_loss: 0.4968 - val_acc: 0.7683\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:152,FP:19,FN:47,TP:117,Macc:0.801151709002,F1:0.779994490749\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2254 - acc: 0.9040 - val_loss: 1.7902 - val_acc: 0.5196\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:171,FP:0,FN:141,TP:23,Macc:0.570121914352,F1:0.245986902822\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2245 - acc: 0.9049 - val_loss: 1.8649 - val_acc: 0.5175\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:170,FP:1,FN:142,TP:22,Macc:0.564149157649,F1:0.235291716915\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2237 - acc: 0.9045 - val_loss: 2.0322 - val_acc: 0.4901\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:171,FP:0,FN:155,TP:9,Macc:0.527438990386,F1:0.104045144586\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2229 - acc: 0.9058 - val_loss: 2.7690 - val_acc: 0.5018\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:171,FP:0,FN:150,TP:14,Macc:0.542682891803,F1:0.157301757117\n",
      "Loss: 2.754800\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 1502.0534\n",
      "Function value obtained: 2.7548\n",
      "Current minimum: 0.8495\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "args [4, 3, 5]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.8170 - acc: 0.8376 - val_loss: 0.6515 - val_acc: 0.7655\n",
      "9639/9639 [==============================] - 3s 338us/step\n",
      "TN:99,FP:72,FN:9,TP:155,Macc:0.762034609503,F1:0.792833472894\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.3393 - acc: 0.8565 - val_loss: 0.6336 - val_acc: 0.6914\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:69,FP:102,FN:3,TP:161,Macc:0.6926079986,F1:0.754093117488\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3066 - acc: 0.8713 - val_loss: 0.5658 - val_acc: 0.7444\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:81,FP:90,FN:0,TP:164,Macc:0.736842056491,F1:0.784683708924\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2880 - acc: 0.8814 - val_loss: 0.6287 - val_acc: 0.6655\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:56,FP:115,FN:0,TP:164,Macc:0.663742645989,F1:0.740401154446\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2761 - acc: 0.8853 - val_loss: 0.6337 - val_acc: 0.6665\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:55,FP:116,FN:0,TP:164,Macc:0.660818669569,F1:0.738733577586\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2665 - acc: 0.8892 - val_loss: 0.5763 - val_acc: 0.7166\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:72,FP:99,FN:1,TP:163,Macc:0.707477488427,F1:0.765252967546\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2616 - acc: 0.8908 - val_loss: 0.5365 - val_acc: 0.7525\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:85,FP:86,FN:0,TP:164,Macc:0.748537962171,F1:0.792265226668\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2577 - acc: 0.8932 - val_loss: 0.4849 - val_acc: 0.7917\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:95,FP:76,FN:0,TP:164,Macc:0.777777726372,F1:0.81187583858\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2536 - acc: 0.8939 - val_loss: 0.4453 - val_acc: 0.8169\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:103,FP:68,FN:0,TP:164,Macc:0.801169537733,F1:0.828277444479\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2488 - acc: 0.8965 - val_loss: 0.4354 - val_acc: 0.8216\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:105,FP:66,FN:1,TP:163,Macc:0.80396871029,F1:0.829511143493\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2468 - acc: 0.8965 - val_loss: 0.4211 - val_acc: 0.8325\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:113,FP:58,FN:5,TP:159,Macc:0.815165400518,F1:0.834640227562\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2442 - acc: 0.8987 - val_loss: 0.5020 - val_acc: 0.7783\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:94,FP:77,FN:3,TP:161,Macc:0.765707409102,F1:0.800989667444\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2423 - acc: 0.8985 - val_loss: 0.5171 - val_acc: 0.7691\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:92,FP:79,FN:1,TP:163,Macc:0.765957016829,F1:0.802950324551\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2376 - acc: 0.9006 - val_loss: 0.3843 - val_acc: 0.8503\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:121,FP:50,FN:3,TP:161,Macc:0.844654772445,F1:0.858661202722\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2388 - acc: 0.8999 - val_loss: 0.3546 - val_acc: 0.8667\n",
      "9639/9639 [==============================] - 2s 228us/step\n",
      "TN:133,FP:38,FN:11,TP:153,Macc:0.85535224722,F1:0.861966309418\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2352 - acc: 0.9019 - val_loss: 0.4334 - val_acc: 0.8181\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:103,FP:68,FN:2,TP:162,Macc:0.795071977167,F1:0.822329633831\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2350 - acc: 0.9016 - val_loss: 0.4104 - val_acc: 0.8322\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:107,FP:64,FN:0,TP:164,Macc:0.812865443414,F1:0.836729293558\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2331 - acc: 0.9021 - val_loss: 0.3416 - val_acc: 0.8688\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:140,FP:31,FN:16,TP:148,Macc:0.860576180744,F1:0.862968216135\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2311 - acc: 0.9043 - val_loss: 0.3830 - val_acc: 0.8399\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:111,FP:60,FN:1,TP:163,Macc:0.821512568811,F1:0.842371840963\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2297 - acc: 0.9027 - val_loss: 0.3826 - val_acc: 0.8493\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:122,FP:49,FN:4,TP:160,Macc:0.844529968582,F1:0.85790337667\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2283 - acc: 0.9037 - val_loss: 0.3921 - val_acc: 0.8493\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:151,FP:20,FN:30,TP:134,Macc:0.850056997398,F1:0.842761742768\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2275 - acc: 0.9055 - val_loss: 0.3588 - val_acc: 0.8581\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:122,FP:49,FN:2,TP:162,Macc:0.850627529148,F1:0.863994535742\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2263 - acc: 0.9039 - val_loss: 0.3744 - val_acc: 0.8441\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:111,FP:60,FN:0,TP:164,Macc:0.824561349094,F1:0.845355408367\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2249 - acc: 0.9056 - val_loss: 0.3180 - val_acc: 0.8799\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:137,FP:34,FN:8,TP:156,Macc:0.87619449375,F1:0.881350407133\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2225 - acc: 0.9063 - val_loss: 0.3278 - val_acc: 0.8727\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:125,FP:46,FN:3,TP:161,Macc:0.856350678125,F1:0.867919050753\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2232 - acc: 0.9063 - val_loss: 0.3403 - val_acc: 0.8664\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:127,FP:44,FN:1,TP:163,Macc:0.868296191532,F1:0.878700721272\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2209 - acc: 0.9069 - val_loss: 0.3100 - val_acc: 0.8831\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:135,FP:36,FN:5,TP:159,Macc:0.87949288176,F1:0.885788358628\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2198 - acc: 0.9091 - val_loss: 0.3729 - val_acc: 0.8507\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:122,FP:49,FN:4,TP:160,Macc:0.844529968582,F1:0.85790337667\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2192 - acc: 0.9078 - val_loss: 0.4226 - val_acc: 0.8391\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:152,FP:19,FN:34,TP:130,Macc:0.840785852685,F1:0.8306653808\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2187 - acc: 0.9088 - val_loss: 0.3488 - val_acc: 0.8579\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:119,FP:52,FN:2,TP:162,Macc:0.841855599888,F1:0.857137403522\n",
      "Loss: 0.332842\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 1525.4376\n",
      "Function value obtained: 0.3328\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "args [4, 3, 5]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.8158 - acc: 0.8404 - val_loss: 0.6189 - val_acc: 0.8007\n",
      "9639/9639 [==============================] - 4s 376us/step\n",
      "TN:112,FP:59,FN:13,TP:151,Macc:0.787851181831,F1:0.807481166755\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.3328 - acc: 0.8621 - val_loss: 0.5601 - val_acc: 0.8108\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:114,FP:57,FN:7,TP:157,Macc:0.811991816371,F1:0.830682378608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3038 - acc: 0.8705 - val_loss: 0.5205 - val_acc: 0.7913\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:99,FP:72,FN:2,TP:162,Macc:0.783376071486,F1:0.814064976929\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2853 - acc: 0.8806 - val_loss: 0.5254 - val_acc: 0.7828\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:132,FP:39,FN:41,TP:123,Macc:0.7609648623,F1:0.754595676317\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2750 - acc: 0.8833 - val_loss: 0.4654 - val_acc: 0.8222\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:112,FP:59,FN:4,TP:160,Macc:0.815290204381,F1:0.835503703843\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2673 - acc: 0.8868 - val_loss: 0.4305 - val_acc: 0.8357\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:119,FP:52,FN:5,TP:159,Macc:0.832709259038,F1:0.847994536682\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2629 - acc: 0.8888 - val_loss: 0.4500 - val_acc: 0.8193\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:114,FP:57,FN:9,TP:155,Macc:0.805894255804,F1:0.824462626535\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2576 - acc: 0.8903 - val_loss: 0.4285 - val_acc: 0.8430\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:127,FP:44,FN:10,TP:154,Macc:0.840857168982,F1:0.850823226127\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2532 - acc: 0.8924 - val_loss: 0.4122 - val_acc: 0.8549\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:140,FP:31,FN:17,TP:147,Macc:0.85752740046,F1:0.85964357676\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2478 - acc: 0.8948 - val_loss: 0.4231 - val_acc: 0.8379\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:134,FP:37,FN:20,TP:144,Macc:0.83083720109,F1:0.834777068854\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2468 - acc: 0.8936 - val_loss: 0.4984 - val_acc: 0.8247\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:147,FP:24,FN:33,TP:131,Macc:0.829214750868,F1:0.821311062192\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2458 - acc: 0.8960 - val_loss: 1.0730 - val_acc: 0.6722\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:155,FP:16,FN:83,TP:81,Macc:0.700167548063,F1:0.620684465332\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2436 - acc: 0.8953 - val_loss: 1.5660 - val_acc: 0.5981\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:167,FP:4,FN:116,TP:48,Macc:0.634645515755,F1:0.444440377951\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2402 - acc: 0.8967 - val_loss: 0.9756 - val_acc: 0.6970\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:148,FP:23,FN:69,TP:95,Macc:0.722382637089,F1:0.673753459074\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2394 - acc: 0.8977 - val_loss: 1.1273 - val_acc: 0.6540\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:166,FP:5,FN:103,TP:61,Macc:0.671355683018,F1:0.530430230435\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2381 - acc: 0.8983 - val_loss: 0.6061 - val_acc: 0.7503\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:148,FP:23,FN:59,TP:105,Macc:0.752870439922,F1:0.719172611647\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2350 - acc: 0.8996 - val_loss: 1.4244 - val_acc: 0.5890\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:164,FP:7,FN:113,TP:51,Macc:0.635019927345,F1:0.459455167883\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2329 - acc: 0.9000 - val_loss: 0.8373 - val_acc: 0.7121\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:159,FP:12,FN:76,TP:88,Macc:0.733204915727,F1:0.666661434384\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2321 - acc: 0.8994 - val_loss: 0.9797 - val_acc: 0.6726\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:161,FP:10,FN:86,TP:78,Macc:0.708565065734,F1:0.619042565296\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2301 - acc: 0.9010 - val_loss: 0.5044 - val_acc: 0.7908\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:147,FP:24,FN:42,TP:122,Macc:0.801775728318,F1:0.787091236916\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2289 - acc: 0.9012 - val_loss: 0.4222 - val_acc: 0.8190\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:149,FP:22,FN:34,TP:130,Macc:0.832013923425,F1:0.822779260812\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2260 - acc: 0.9029 - val_loss: 0.7976 - val_acc: 0.7255\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:155,FP:16,FN:68,TP:96,Macc:0.745899252313,F1:0.695646813735\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2254 - acc: 0.9045 - val_loss: 0.3304 - val_acc: 0.8759\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:140,FP:31,FN:11,TP:153,Macc:0.87582008216,F1:0.879304807439\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2247 - acc: 0.9037 - val_loss: 1.1352 - val_acc: 0.6621\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:165,FP:6,FN:94,TP:70,Macc:0.695870729148,F1:0.583328519344\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2242 - acc: 0.9039 - val_loss: 0.9823 - val_acc: 0.6826\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:165,FP:6,FN:88,TP:76,Macc:0.714163410848,F1:0.617881234753\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2243 - acc: 0.9046 - val_loss: 0.3785 - val_acc: 0.8556\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:142,FP:29,FN:16,TP:148,Macc:0.866424133584,F1:0.868029642642\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2221 - acc: 0.9064 - val_loss: 0.5662 - val_acc: 0.7859\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:152,FP:19,FN:42,TP:122,Macc:0.816395610419,F1:0.799994473609\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2207 - acc: 0.9053 - val_loss: 0.3416 - val_acc: 0.8646\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:133,FP:38,FN:7,TP:157,Macc:0.867547368353,F1:0.87464629803\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2191 - acc: 0.9070 - val_loss: 0.4752 - val_acc: 0.7951\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:153,FP:18,FN:46,TP:118,Macc:0.807124465706,F1:0.786661156927\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2180 - acc: 0.9071 - val_loss: 1.0235 - val_acc: 0.6673\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:162,FP:9,FN:92,TP:72,Macc:0.693196360454,F1:0.587750180532\n",
      "Loss: 1.009483\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 1530.5320\n",
      "Function value obtained: 1.0095\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "args [4, 3, 5]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.8150 - acc: 0.8388 - val_loss: 0.6439 - val_acc: 0.7680\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:97,FP:74,FN:2,TP:162,Macc:0.777528118646,F1:0.809994633685\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.3374 - acc: 0.8488 - val_loss: 0.6206 - val_acc: 0.7429\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:83,FP:88,FN:4,TP:160,Macc:0.730494888198,F1:0.776693716314\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3114 - acc: 0.8552 - val_loss: 0.5431 - val_acc: 0.8037\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:111,FP:60,FN:6,TP:158,Macc:0.806268667394,F1:0.82721969319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2937 - acc: 0.8714 - val_loss: 0.6419 - val_acc: 0.6499\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:49,FP:122,FN:1,TP:163,Macc:0.640226030765,F1:0.726052770349\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2797 - acc: 0.8825 - val_loss: 0.5280 - val_acc: 0.7771\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:100,FP:71,FN:5,TP:159,Macc:0.777153707056,F1:0.807101208286\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2694 - acc: 0.8854 - val_loss: 0.4584 - val_acc: 0.8262\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:113,FP:58,FN:9,TP:155,Macc:0.802970279384,F1:0.822275712072\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2636 - acc: 0.8886 - val_loss: 0.4862 - val_acc: 0.8024\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:107,FP:64,FN:8,TP:156,Macc:0.788475201147,F1:0.812494570457\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2579 - acc: 0.8904 - val_loss: 0.6501 - val_acc: 0.6667\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:60,FP:111,FN:4,TP:160,Macc:0.663243430536,F1:0.735626979516\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2511 - acc: 0.8949 - val_loss: 0.5805 - val_acc: 0.7135\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:79,FP:92,FN:2,TP:162,Macc:0.724896543084,F1:0.775114331438\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2491 - acc: 0.8946 - val_loss: 0.6007 - val_acc: 0.7067\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:76,FP:95,FN:6,TP:158,Macc:0.703929492691,F1:0.757788475581\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2450 - acc: 0.8956 - val_loss: 0.5319 - val_acc: 0.8026\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:112,FP:59,FN:9,TP:155,Macc:0.800046302964,F1:0.820100368642\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2424 - acc: 0.8988 - val_loss: 0.4628 - val_acc: 0.7978\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:103,FP:68,FN:2,TP:162,Macc:0.795071977167,F1:0.822329633831\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2400 - acc: 0.8985 - val_loss: 0.4581 - val_acc: 0.8098\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:110,FP:61,FN:7,TP:157,Macc:0.800295910691,F1:0.821984091398\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2356 - acc: 0.9007 - val_loss: 0.4048 - val_acc: 0.8493\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:124,FP:47,FN:11,TP:153,Macc:0.829036459439,F1:0.840653843683\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2349 - acc: 0.9013 - val_loss: 0.4150 - val_acc: 0.8439\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:123,FP:48,FN:9,TP:155,Macc:0.832210043585,F1:0.844681160011\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2330 - acc: 0.9024 - val_loss: 0.4048 - val_acc: 0.8305\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:111,FP:60,FN:3,TP:161,Macc:0.815415008244,F1:0.836358209163\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2324 - acc: 0.9020 - val_loss: 0.3528 - val_acc: 0.8741\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:133,FP:38,FN:8,TP:156,Macc:0.86449858807,F1:0.871502864988\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2286 - acc: 0.9027 - val_loss: 0.5695 - val_acc: 0.7693\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:141,FP:30,FN:45,TP:119,Macc:0.775085528948,F1:0.760377845807\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2285 - acc: 0.9032 - val_loss: 0.5026 - val_acc: 0.8244\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:140,FP:31,FN:24,TP:140,Macc:0.836185938477,F1:0.83581534307\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2264 - acc: 0.9045 - val_loss: 0.5974 - val_acc: 0.7434\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:148,FP:23,FN:61,TP:103,Macc:0.746772879355,F1:0.710339368174\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2237 - acc: 0.9059 - val_loss: 1.1879 - val_acc: 0.6056\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:159,FP:12,FN:113,TP:51,Macc:0.620400045244,F1:0.449334752357\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2231 - acc: 0.9050 - val_loss: 0.3719 - val_acc: 0.8484\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:124,FP:47,FN:7,TP:157,Macc:0.841231580572,F1:0.853255383571\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2213 - acc: 0.9059 - val_loss: 0.4186 - val_acc: 0.8301\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:147,FP:24,FN:36,TP:128,Macc:0.820068410018,F1:0.810121033846\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2215 - acc: 0.9074 - val_loss: 1.4709 - val_acc: 0.5940\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:158,FP:13,FN:110,TP:54,Macc:0.626622409674,F1:0.467527892849\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2192 - acc: 0.9072 - val_loss: 0.4446 - val_acc: 0.8140\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:139,FP:32,FN:28,TP:136,Macc:0.821066840924,F1:0.81927155498\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2177 - acc: 0.9078 - val_loss: 1.5975 - val_acc: 0.6058\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:153,FP:18,FN:107,TP:57,Macc:0.621148868423,F1:0.476982666527\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2179 - acc: 0.9082 - val_loss: 1.8683 - val_acc: 0.5371\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:168,FP:3,FN:136,TP:28,Macc:0.576593886509,F1:0.287176513376\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2152 - acc: 0.9090 - val_loss: 1.5313 - val_acc: 0.5645\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:165,FP:6,FN:127,TP:37,Macc:0.595260979798,F1:0.357484264033\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2146 - acc: 0.9080 - val_loss: 2.3864 - val_acc: 0.5350\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:165,FP:6,FN:137,TP:27,Macc:0.564773176965,F1:0.274108576601\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2151 - acc: 0.9094 - val_loss: 0.7707 - val_acc: 0.6630\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:159,FP:12,FN:95,TP:69,Macc:0.675278090344,F1:0.563260386814\n",
      "Loss: 0.756330\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 1555.3040\n",
      "Function value obtained: 0.7563\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "args [4, 3, 5]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.8203 - acc: 0.8361 - val_loss: 0.6823 - val_acc: 0.6915\n",
      "9639/9639 [==============================] - 4s 435us/step\n",
      "TN:66,FP:105,FN:4,TP:160,Macc:0.680787289056,F1:0.745915512558\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.3419 - acc: 0.8504 - val_loss: 0.6252 - val_acc: 0.7097\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:65,FP:106,FN:1,TP:163,Macc:0.687009653486,F1:0.752881621229\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3164 - acc: 0.8541 - val_loss: 0.6272 - val_acc: 0.6786\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:64,FP:107,FN:4,TP:160,Macc:0.674939336216,F1:0.742454173\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.3010 - acc: 0.8624 - val_loss: 0.5272 - val_acc: 0.7842\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:100,FP:71,FN:6,TP:158,Macc:0.774104926773,F1:0.804065852297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2860 - acc: 0.8754 - val_loss: 0.5625 - val_acc: 0.7341\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:76,FP:95,FN:3,TP:161,Macc:0.713075833541,F1:0.766661390444\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2729 - acc: 0.8823 - val_loss: 0.4964 - val_acc: 0.7882\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:104,FP:67,FN:6,TP:158,Macc:0.785800832453,F1:0.812333920958\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2668 - acc: 0.8856 - val_loss: 0.5698 - val_acc: 0.7184\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:79,FP:92,FN:2,TP:162,Macc:0.724896543084,F1:0.775114331438\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2615 - acc: 0.8881 - val_loss: 0.4497 - val_acc: 0.8156\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:107,FP:64,FN:2,TP:162,Macc:0.806767882847,F1:0.830763822941\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2543 - acc: 0.8913 - val_loss: 0.4343 - val_acc: 0.8281\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:113,FP:58,FN:7,TP:157,Macc:0.809067839951,F1:0.828490593751\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2520 - acc: 0.8927 - val_loss: 0.3841 - val_acc: 0.8534\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:121,FP:50,FN:6,TP:158,Macc:0.835508431595,F1:0.849456892334\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2487 - acc: 0.8939 - val_loss: 0.3856 - val_acc: 0.8576\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:124,FP:47,FN:5,TP:159,Macc:0.847329141139,F1:0.85945397926\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2444 - acc: 0.8958 - val_loss: 0.4464 - val_acc: 0.8124\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:104,FP:67,FN:0,TP:164,Macc:0.804093514153,F1:0.830374358861\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2432 - acc: 0.8957 - val_loss: 0.3697 - val_acc: 0.8588\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:117,FP:54,FN:2,TP:162,Macc:0.836007647048,F1:0.85262613261\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2416 - acc: 0.8969 - val_loss: 0.3680 - val_acc: 0.8618\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:121,FP:50,FN:2,TP:162,Macc:0.847703552728,F1:0.861696666908\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2387 - acc: 0.8983 - val_loss: 0.3701 - val_acc: 0.8583\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:121,FP:50,FN:3,TP:161,Macc:0.844654772445,F1:0.858661202722\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2366 - acc: 0.8994 - val_loss: 0.6960 - val_acc: 0.6859\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:147,FP:24,FN:73,TP:91,Macc:0.707263539536,F1:0.652324367356\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2342 - acc: 0.9004 - val_loss: 0.3761 - val_acc: 0.8545\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:131,FP:40,FN:12,TP:152,Macc:0.846455514096,F1:0.853927065557\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2332 - acc: 0.9005 - val_loss: 0.3594 - val_acc: 0.8625\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:124,FP:47,FN:2,TP:162,Macc:0.856475481989,F1:0.868627236627\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2308 - acc: 0.9012 - val_loss: 0.5865 - val_acc: 0.7613\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:144,FP:27,FN:47,TP:117,Macc:0.777759897641,F1:0.759734728704\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2298 - acc: 0.9015 - val_loss: 1.7686 - val_acc: 0.5754\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:159,FP:12,FN:117,TP:47,Macc:0.608204924111,F1:0.421520341494\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2278 - acc: 0.9028 - val_loss: 0.3633 - val_acc: 0.8586\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:133,FP:38,FN:14,TP:150,Macc:0.84620590637,F1:0.852267199609\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2263 - acc: 0.9032 - val_loss: 0.3838 - val_acc: 0.8485\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:121,FP:50,FN:5,TP:159,Macc:0.838557211878,F1:0.852541446692\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2253 - acc: 0.9053 - val_loss: 0.7777 - val_acc: 0.6702\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:154,FP:17,FN:84,TP:80,Macc:0.69419479136,F1:0.61302163073\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2240 - acc: 0.9050 - val_loss: 0.3763 - val_acc: 0.8514\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:119,FP:52,FN:1,TP:163,Macc:0.844904380171,F1:0.860152861042\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2217 - acc: 0.9066 - val_loss: 0.8982 - val_acc: 0.6504\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:147,FP:24,FN:87,TP:77,Macc:0.664580615569,F1:0.581126838124\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2226 - acc: 0.9068 - val_loss: 0.3778 - val_acc: 0.8556\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:144,FP:27,FN:26,TP:138,Macc:0.841784283591,F1:0.838900219066\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2212 - acc: 0.9073 - val_loss: 0.8309 - val_acc: 0.6874\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:146,FP:25,FN:74,TP:90,Macc:0.701290782832,F1:0.645155909141\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2201 - acc: 0.9066 - val_loss: 0.3420 - val_acc: 0.8780\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:138,FP:33,FN:7,TP:157,Macc:0.882167250453,F1:0.887000124296\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2193 - acc: 0.9076 - val_loss: 0.4141 - val_acc: 0.8494\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:144,FP:27,FN:26,TP:138,Macc:0.841784283591,F1:0.838900219066\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2168 - acc: 0.9098 - val_loss: 0.3694 - val_acc: 0.8594\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:148,FP:23,FN:21,TP:143,Macc:0.868724090688,F1:0.866661109126\n",
      "Loss: 0.353257\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 1579.1254\n",
      "Function value obtained: 0.3533\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "args [4, 3, 5]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.8166 - acc: 0.8379 - val_loss: 1.1426 - val_acc: 0.5478\n",
      "9639/9639 [==============================] - 4s 459us/step\n",
      "TN:149,FP:22,FN:112,TP:52,Macc:0.594209061326,F1:0.436970036065\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3338 - acc: 0.8633 - val_loss: 1.2405 - val_acc: 0.5165\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:151,FP:20,FN:138,TP:26,Macc:0.520788726801,F1:0.247615258288\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3022 - acc: 0.8731 - val_loss: 1.4008 - val_acc: 0.5449\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:147,FP:24,FN:112,TP:52,Macc:0.588361108486,F1:0.433328533108\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2915 - acc: 0.8744 - val_loss: 0.7856 - val_acc: 0.6533\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:160,FP:11,FN:98,TP:66,Macc:0.669055725914,F1:0.547713009115\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2810 - acc: 0.8780 - val_loss: 0.8768 - val_acc: 0.6176\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:165,FP:6,FN:109,TP:55,Macc:0.650139024898,F1:0.488884493708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2733 - acc: 0.8818 - val_loss: 0.5321 - val_acc: 0.7328\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:162,FP:9,FN:77,TP:87,Macc:0.738928064704,F1:0.669225588856\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2658 - acc: 0.8865 - val_loss: 0.8932 - val_acc: 0.6383\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:171,FP:0,FN:110,TP:54,Macc:0.664634103135,F1:0.49540869442\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2624 - acc: 0.8877 - val_loss: 0.7194 - val_acc: 0.7181\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:159,FP:12,FN:76,TP:88,Macc:0.733204915727,F1:0.666661434384\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2556 - acc: 0.8900 - val_loss: 0.4236 - val_acc: 0.8384\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:146,FP:25,FN:34,TP:130,Macc:0.823241994165,F1:0.815041470148\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2526 - acc: 0.8913 - val_loss: 0.4642 - val_acc: 0.7914\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:134,FP:37,FN:30,TP:134,Macc:0.800349398257,F1:0.799994449902\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2502 - acc: 0.8919 - val_loss: 0.5167 - val_acc: 0.7838\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:149,FP:22,FN:46,TP:118,Macc:0.795428560025,F1:0.776310267611\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2480 - acc: 0.8940 - val_loss: 0.3654 - val_acc: 0.8627\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:145,FP:26,FN:21,TP:143,Macc:0.859952161428,F1:0.858853303393\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2447 - acc: 0.8941 - val_loss: 0.4420 - val_acc: 0.8142\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:136,FP:35,FN:23,TP:141,Macc:0.82753881308,F1:0.829406217926\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2416 - acc: 0.8974 - val_loss: 0.3616 - val_acc: 0.8637\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:131,FP:40,FN:9,TP:155,Macc:0.855601854946,F1:0.863504237432\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2400 - acc: 0.8974 - val_loss: 0.4463 - val_acc: 0.8002\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:142,FP:29,FN:37,TP:127,Macc:0.802399747634,F1:0.793744448905\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2379 - acc: 0.8980 - val_loss: 0.3784 - val_acc: 0.8504\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:136,FP:35,FN:16,TP:148,Macc:0.848880275063,F1:0.853020399042\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2370 - acc: 0.8981 - val_loss: 0.3448 - val_acc: 0.8706\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:130,FP:41,FN:7,TP:157,Macc:0.858775439093,F1:0.867397810754\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2357 - acc: 0.8981 - val_loss: 0.4107 - val_acc: 0.8099\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:142,FP:29,FN:32,TP:132,Macc:0.817643649051,F1:0.812302137827\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2317 - acc: 0.9005 - val_loss: 0.3466 - val_acc: 0.8726\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:133,FP:38,FN:12,TP:152,Macc:0.852303466936,F1:0.858751538482\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2324 - acc: 0.8994 - val_loss: 0.5798 - val_acc: 0.7517\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:141,FP:30,FN:53,TP:111,Macc:0.750695286681,F1:0.727863331275\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2283 - acc: 0.9022 - val_loss: 0.3809 - val_acc: 0.8446\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:135,FP:36,FN:14,TP:150,Macc:0.85205385921,F1:0.857137325031\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2273 - acc: 0.9030 - val_loss: 0.3215 - val_acc: 0.8763\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:134,FP:37,FN:6,TP:158,Macc:0.873520125056,F1:0.880217328329\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2265 - acc: 0.9029 - val_loss: 0.3254 - val_acc: 0.8762\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:133,FP:38,FN:5,TP:159,Macc:0.873644928919,F1:0.880880918903\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2241 - acc: 0.9038 - val_loss: 0.3092 - val_acc: 0.8866\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:131,FP:40,FN:2,TP:162,Macc:0.876943316929,F1:0.885240407749\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2234 - acc: 0.9043 - val_loss: 0.3644 - val_acc: 0.8516\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:141,FP:30,FN:15,TP:149,Macc:0.866548937447,F1:0.868799119551\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2221 - acc: 0.9054 - val_loss: 0.3328 - val_acc: 0.8700\n",
      "9639/9639 [==============================] - 2s 257us/step\n",
      "TN:133,FP:38,FN:5,TP:159,Macc:0.873644928919,F1:0.880880918903\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2224 - acc: 0.9049 - val_loss: 0.3941 - val_acc: 0.8406\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:136,FP:35,FN:22,TP:142,Macc:0.830587593363,F1:0.832839029078\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2193 - acc: 0.9062 - val_loss: 0.3921 - val_acc: 0.8513\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:134,FP:37,FN:14,TP:150,Macc:0.84912988279,F1:0.854695324781\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2184 - acc: 0.9073 - val_loss: 0.4259 - val_acc: 0.8347\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:138,FP:33,FN:22,TP:142,Macc:0.836435546204,F1:0.837752563554\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2178 - acc: 0.9065 - val_loss: 0.3624 - val_acc: 0.8579\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:138,FP:33,FN:19,TP:145,Macc:0.845581887054,F1:0.84794767108\n",
      "Loss: 0.347962\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 1611.9064\n",
      "Function value obtained: 0.3480\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "args [4, 3, 5]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.8140 - acc: 0.8495 - val_loss: 0.6090 - val_acc: 0.7268\n",
      "9639/9639 [==============================] - 5s 506us/step\n",
      "TN:143,FP:28,FN:63,TP:101,Macc:0.726055436688,F1:0.68941432198\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3307 - acc: 0.8678 - val_loss: 0.5466 - val_acc: 0.7899\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:153,FP:18,FN:57,TP:107,Macc:0.773587882589,F1:0.740478972896\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3004 - acc: 0.8769 - val_loss: 0.6656 - val_acc: 0.6530\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:167,FP:4,FN:100,TP:64,Macc:0.683426000288,F1:0.551719527386\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2849 - acc: 0.8823 - val_loss: 0.6647 - val_acc: 0.5547\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:169,FP:2,FN:134,TP:30,Macc:0.585615423495,F1:0.306119409233\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2757 - acc: 0.8851 - val_loss: 0.4780 - val_acc: 0.8365\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:155,FP:16,FN:45,TP:119,Macc:0.816021198829,F1:0.795981115282\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2663 - acc: 0.8880 - val_loss: 0.3906 - val_acc: 0.8733\n",
      "9639/9639 [==============================] - 2s 250us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:136,FP:35,FN:7,TP:157,Macc:0.876319297613,F1:0.88201695146\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2613 - acc: 0.8888 - val_loss: 0.4074 - val_acc: 0.8450\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:119,FP:52,FN:4,TP:160,Macc:0.835758039321,F1:0.851058369659\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2562 - acc: 0.8920 - val_loss: 0.4316 - val_acc: 0.8259\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:107,FP:64,FN:2,TP:162,Macc:0.806767882847,F1:0.830763822941\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2505 - acc: 0.8931 - val_loss: 0.4520 - val_acc: 0.8150\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:109,FP:62,FN:7,TP:157,Macc:0.797371934271,F1:0.819837908399\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2492 - acc: 0.8949 - val_loss: 0.4720 - val_acc: 0.7942\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:100,FP:71,FN:0,TP:164,Macc:0.792397608473,F1:0.822049766707\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2442 - acc: 0.8963 - val_loss: 0.3537 - val_acc: 0.8741\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:145,FP:26,FN:20,TP:144,Macc:0.863000941711,F1:0.862269894115\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2406 - acc: 0.8980 - val_loss: 0.3630 - val_acc: 0.8687\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:130,FP:41,FN:8,TP:156,Macc:0.855726658809,F1:0.864260421302\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2404 - acc: 0.8983 - val_loss: 0.5662 - val_acc: 0.7963\n",
      "9639/9639 [==============================] - 3s 277us/step\n",
      "TN:143,FP:28,FN:39,TP:125,Macc:0.799226163488,F1:0.788637985052\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2357 - acc: 0.9007 - val_loss: 0.7068 - val_acc: 0.7642\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:148,FP:23,FN:50,TP:114,Macc:0.780309462472,F1:0.757469571987\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2345 - acc: 0.9000 - val_loss: 0.7617 - val_acc: 0.7123\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:158,FP:13,FN:75,TP:89,Macc:0.73332971959,F1:0.669167675828\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2320 - acc: 0.9022 - val_loss: 1.1828 - val_acc: 0.6477\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:166,FP:5,FN:100,TP:64,Macc:0.680502023868,F1:0.549351585663\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2322 - acc: 0.9008 - val_loss: 0.3484 - val_acc: 0.8655\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:123,FP:48,FN:2,TP:162,Macc:0.853551505568,F1:0.866304692705\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2311 - acc: 0.9009 - val_loss: 0.6021 - val_acc: 0.7534\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:148,FP:23,FN:52,TP:112,Macc:0.774211901905,F1:0.749158376255\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2284 - acc: 0.9029 - val_loss: 1.6142 - val_acc: 0.5464\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:163,FP:8,FN:128,TP:36,Macc:0.586364246675,F1:0.346150140201\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2264 - acc: 0.9030 - val_loss: 1.2123 - val_acc: 0.6504\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:154,FP:17,FN:87,TP:77,Macc:0.68504845051,F1:0.596894078825\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2243 - acc: 0.9054 - val_loss: 1.0521 - val_acc: 0.6591\n",
      "9639/9639 [==============================] - 2s 257us/step\n",
      "TN:155,FP:16,FN:86,TP:78,Macc:0.691021207213,F1:0.604646016148\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2237 - acc: 0.9046 - val_loss: 1.2848 - val_acc: 0.5838\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:159,FP:12,FN:115,TP:49,Macc:0.614302484677,F1:0.435551165594\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2216 - acc: 0.9060 - val_loss: 0.8185 - val_acc: 0.6710\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:161,FP:10,FN:91,TP:73,Macc:0.693321164317,F1:0.59108815628\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2209 - acc: 0.9060 - val_loss: 1.6413 - val_acc: 0.5285\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:170,FP:1,FN:139,TP:25,Macc:0.573295498499,F1:0.263155265731\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2202 - acc: 0.9064 - val_loss: 1.6753 - val_acc: 0.5975\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:153,FP:18,FN:100,TP:64,Macc:0.642490330407,F1:0.520320267876\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2186 - acc: 0.9072 - val_loss: 2.3854 - val_acc: 0.5058\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:166,FP:5,FN:143,TP:21,Macc:0.549404471685,F1:0.221050007454\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2168 - acc: 0.9082 - val_loss: 0.6702 - val_acc: 0.7250\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:156,FP:15,FN:69,TP:95,Macc:0.74577444845,F1:0.693425314921\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2176 - acc: 0.9076 - val_loss: 1.2042 - val_acc: 0.6676\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:145,FP:26,FN:81,TP:83,Macc:0.677025344429,F1:0.608053282339\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2175 - acc: 0.9094 - val_loss: 1.5699 - val_acc: 0.5599\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:163,FP:8,FN:121,TP:43,Macc:0.607705708658,F1:0.399995978406\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2144 - acc: 0.9102 - val_loss: 2.7613 - val_acc: 0.5453\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:163,FP:8,FN:124,TP:40,Macc:0.598559367808,F1:0.377354598117\n",
      "Loss: 2.745432\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 1641.2186\n",
      "Function value obtained: 2.7454\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "args [4, 3, 5]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.8065 - acc: 0.8401 - val_loss: 1.0663 - val_acc: 0.5185\n",
      "9639/9639 [==============================] - 5s 545us/step\n",
      "TN:157,FP:14,FN:126,TP:38,Macc:0.574917948721,F1:0.351847794799\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3354 - acc: 0.8515 - val_loss: 0.5919 - val_acc: 0.7081\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:141,FP:30,FN:61,TP:103,Macc:0.726305044415,F1:0.693597202188\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3111 - acc: 0.8584 - val_loss: 0.6244 - val_acc: 0.6860\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:145,FP:26,FN:71,TP:93,Macc:0.707513147262,F1:0.657238404269\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2910 - acc: 0.8760 - val_loss: 0.4679 - val_acc: 0.8203\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:129,FP:42,FN:22,TP:142,Macc:0.810119758423,F1:0.816086420634\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2797 - acc: 0.8815 - val_loss: 0.5454 - val_acc: 0.7284\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:144,FP:27,FN:56,TP:108,Macc:0.750320875092,F1:0.722402525382\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2692 - acc: 0.8864 - val_loss: 0.4315 - val_acc: 0.8369\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:129,FP:42,FN:22,TP:142,Macc:0.810119758423,F1:0.816086420634\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2644 - acc: 0.8894 - val_loss: 0.4027 - val_acc: 0.8361\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:125,FP:46,FN:16,TP:148,Macc:0.816716534442,F1:0.826810130307\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2599 - acc: 0.8909 - val_loss: 0.3949 - val_acc: 0.8548\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:147,FP:24,FN:26,TP:138,Macc:0.850556212851,F1:0.84662020998\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2551 - acc: 0.8921 - val_loss: 0.3847 - val_acc: 0.8548\n",
      "9639/9639 [==============================] - 3s 269us/step\n",
      "TN:114,FP:57,FN:2,TP:162,Macc:0.827235717788,F1:0.845947567473\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2514 - acc: 0.8947 - val_loss: 0.4256 - val_acc: 0.8247\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:144,FP:27,FN:28,TP:136,Macc:0.835686723024,F1:0.831798725471\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2490 - acc: 0.8953 - val_loss: 0.3722 - val_acc: 0.8559\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:117,FP:54,FN:2,TP:162,Macc:0.836007647048,F1:0.85262613261\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2451 - acc: 0.8974 - val_loss: 0.3427 - val_acc: 0.8761\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:127,FP:44,FN:6,TP:158,Macc:0.853052290116,F1:0.863382485567\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2440 - acc: 0.8973 - val_loss: 0.3617 - val_acc: 0.8697\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:142,FP:29,FN:16,TP:148,Macc:0.866424133584,F1:0.868029642642\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2407 - acc: 0.9000 - val_loss: 0.3443 - val_acc: 0.8717\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:125,FP:46,FN:4,TP:160,Macc:0.853301897842,F1:0.864859384344\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2400 - acc: 0.8985 - val_loss: 0.4232 - val_acc: 0.8211\n",
      "9639/9639 [==============================] - 3s 273us/step\n",
      "TN:103,FP:68,FN:1,TP:163,Macc:0.79812075745,F1:0.825311068004\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2388 - acc: 0.8990 - val_loss: 0.4301 - val_acc: 0.8101\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:96,FP:75,FN:1,TP:163,Macc:0.777652922509,F1:0.810939915656\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2359 - acc: 0.9013 - val_loss: 0.3297 - val_acc: 0.8762\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:125,FP:46,FN:2,TP:162,Macc:0.859399458409,F1:0.870962267406\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2344 - acc: 0.9011 - val_loss: 0.3291 - val_acc: 0.8759\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:126,FP:45,FN:4,TP:160,Macc:0.856225874262,F1:0.867203188318\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2304 - acc: 0.9033 - val_loss: 0.5791 - val_acc: 0.7269\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:67,FP:104,FN:1,TP:163,Macc:0.692857606326,F1:0.756375285978\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2294 - acc: 0.9041 - val_loss: 0.3988 - val_acc: 0.8260\n",
      "9639/9639 [==============================] - 3s 269us/step\n",
      "TN:102,FP:69,FN:1,TP:163,Macc:0.79519678103,F1:0.823226939709\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2277 - acc: 0.9037 - val_loss: 0.3068 - val_acc: 0.8845\n",
      "9639/9639 [==============================] - 2s 257us/step\n",
      "TN:129,FP:42,FN:1,TP:163,Macc:0.874144144372,F1:0.88346334995\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2257 - acc: 0.9044 - val_loss: 0.3055 - val_acc: 0.8864\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:135,FP:36,FN:5,TP:159,Macc:0.87949288176,F1:0.885788358628\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2256 - acc: 0.9055 - val_loss: 0.3120 - val_acc: 0.8865\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:139,FP:32,FN:5,TP:159,Macc:0.89118878744,F1:0.895769124223\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2246 - acc: 0.9059 - val_loss: 0.3295 - val_acc: 0.8742\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:156,FP:15,FN:29,TP:135,Macc:0.867725659782,F1:0.859867062188\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2229 - acc: 0.9069 - val_loss: 0.3040 - val_acc: 0.8863\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:134,FP:37,FN:3,TP:161,Macc:0.882666465906,F1:0.889497256924\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2216 - acc: 0.9064 - val_loss: 0.3119 - val_acc: 0.8830\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:135,FP:36,FN:6,TP:158,Macc:0.876444101476,F1:0.882676048659\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2207 - acc: 0.9080 - val_loss: 0.9005 - val_acc: 0.6960\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:154,FP:17,FN:79,TP:85,Macc:0.709438692776,F1:0.639092490347\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2194 - acc: 0.9077 - val_loss: 0.2974 - val_acc: 0.8918\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:142,FP:29,FN:11,TP:153,Macc:0.881668035,F1:0.88438752227\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2179 - acc: 0.9078 - val_loss: 0.4013 - val_acc: 0.8367\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:146,FP:25,FN:24,TP:140,Macc:0.853729796998,F1:0.851058272964\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2165 - acc: 0.9097 - val_loss: 0.4366 - val_acc: 0.8252\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:149,FP:22,FN:34,TP:130,Macc:0.832013923425,F1:0.822779260812\n",
      "Loss: 0.421000\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 1683.9935\n",
      "Function value obtained: 0.4210\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "args [4, 3, 5]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.8127 - acc: 0.8379 - val_loss: 0.6847 - val_acc: 0.6915\n",
      "9639/9639 [==============================] - 6s 585us/step\n",
      "TN:64,FP:107,FN:0,TP:164,Macc:0.68713445735,F1:0.754017783182\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.3369 - acc: 0.8506 - val_loss: 0.5472 - val_acc: 0.8200\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:114,FP:57,FN:11,TP:153,Macc:0.799796695238,F1:0.818176353292\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3087 - acc: 0.8699 - val_loss: 0.5135 - val_acc: 0.8055\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:111,FP:60,FN:8,TP:156,Macc:0.800171106827,F1:0.821047187071\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2890 - acc: 0.8782 - val_loss: 0.7079 - val_acc: 0.5843\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:21,FP:150,FN:0,TP:164,Macc:0.561403471285,F1:0.686187478686\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2793 - acc: 0.8831 - val_loss: 0.5882 - val_acc: 0.7032\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:73,FP:98,FN:0,TP:164,Macc:0.71345024513,F1:0.769947802984\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2694 - acc: 0.8858 - val_loss: 0.6817 - val_acc: 0.6254\n",
      "9639/9639 [==============================] - 3s 274us/step\n",
      "TN:35,FP:136,FN:0,TP:164,Macc:0.602339141167,F1:0.706891490746\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2645 - acc: 0.8892 - val_loss: 0.6336 - val_acc: 0.6689\n",
      "9639/9639 [==============================] - 3s 273us/step\n",
      "TN:58,FP:113,FN:0,TP:164,Macc:0.669590598829,F1:0.74375899638\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2594 - acc: 0.8908 - val_loss: 0.6722 - val_acc: 0.6441\n",
      "9639/9639 [==============================] - 3s 275us/step\n",
      "TN:45,FP:126,FN:0,TP:164,Macc:0.631578905368,F1:0.722461849014\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2557 - acc: 0.8917 - val_loss: 0.5295 - val_acc: 0.7547\n",
      "9639/9639 [==============================] - 3s 282us/step\n",
      "TN:83,FP:88,FN:0,TP:164,Macc:0.742690009331,F1:0.788456242916\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2526 - acc: 0.8937 - val_loss: 0.5343 - val_acc: 0.7532\n",
      "9639/9639 [==============================] - 3s 275us/step\n",
      "TN:88,FP:83,FN:0,TP:164,Macc:0.757309891432,F1:0.7980482096\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2487 - acc: 0.8951 - val_loss: 0.4570 - val_acc: 0.8033\n",
      "9639/9639 [==============================] - 3s 275us/step\n",
      "TN:110,FP:61,FN:4,TP:160,Macc:0.809442251541,F1:0.831163404265\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2466 - acc: 0.8961 - val_loss: 0.6265 - val_acc: 0.6882\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:67,FP:104,FN:1,TP:163,Macc:0.692857606326,F1:0.756375285978\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2445 - acc: 0.8972 - val_loss: 0.7060 - val_acc: 0.6421\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:48,FP:123,FN:0,TP:164,Macc:0.640350834628,F1:0.727267600923\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2419 - acc: 0.8986 - val_loss: 0.6087 - val_acc: 0.6972\n",
      "9639/9639 [==============================] - 3s 275us/step\n",
      "TN:78,FP:93,FN:2,TP:162,Macc:0.721972566664,F1:0.7732644086\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2413 - acc: 0.8985 - val_loss: 0.5976 - val_acc: 0.7144\n",
      "9639/9639 [==============================] - 3s 271us/step\n",
      "TN:69,FP:102,FN:0,TP:164,Macc:0.70175433945,F1:0.762785468159\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2384 - acc: 0.8998 - val_loss: 0.7094 - val_acc: 0.6474\n",
      "9639/9639 [==============================] - 3s 273us/step\n",
      "TN:51,FP:120,FN:0,TP:164,Macc:0.649122763888,F1:0.732137715836\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2384 - acc: 0.8996 - val_loss: 0.5069 - val_acc: 0.7682\n",
      "9639/9639 [==============================] - 3s 273us/step\n",
      "TN:92,FP:79,FN:2,TP:162,Macc:0.762908236546,F1:0.799994655386\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2360 - acc: 0.8997 - val_loss: 0.5952 - val_acc: 0.7213\n",
      "9639/9639 [==============================] - 3s 269us/step\n",
      "TN:79,FP:92,FN:3,TP:161,Macc:0.721847762801,F1:0.77217696403\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2352 - acc: 0.9013 - val_loss: 0.5366 - val_acc: 0.7584\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:91,FP:80,FN:3,TP:161,Macc:0.756935479842,F1:0.79505638405\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2335 - acc: 0.9007 - val_loss: 0.4562 - val_acc: 0.7996\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:109,FP:62,FN:6,TP:158,Macc:0.800420714554,F1:0.822911236526\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2322 - acc: 0.9023 - val_loss: 0.5547 - val_acc: 0.7536\n",
      "9639/9639 [==============================] - 3s 274us/step\n",
      "TN:91,FP:80,FN:3,TP:161,Macc:0.756935479842,F1:0.79505638405\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2308 - acc: 0.9024 - val_loss: 0.4859 - val_acc: 0.7906\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:103,FP:68,FN:1,TP:163,Macc:0.79812075745,F1:0.825311068004\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2304 - acc: 0.9037 - val_loss: 0.3956 - val_acc: 0.8560\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:136,FP:35,FN:14,TP:150,Macc:0.85497783563,F1:0.85959331963\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2290 - acc: 0.9030 - val_loss: 0.4310 - val_acc: 0.8189\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:110,FP:61,FN:3,TP:161,Macc:0.812491031824,F1:0.834191467859\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2262 - acc: 0.9053 - val_loss: 0.3569 - val_acc: 0.8687\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:139,FP:32,FN:11,TP:153,Macc:0.87289610574,F1:0.876785295623\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2260 - acc: 0.9045 - val_loss: 0.3569 - val_acc: 0.8716\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:144,FP:27,FN:17,TP:147,Macc:0.869223306141,F1:0.86981693344\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2258 - acc: 0.9049 - val_loss: 0.3741 - val_acc: 0.8537\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:124,FP:47,FN:3,TP:161,Macc:0.853426701705,F1:0.865585923638\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2235 - acc: 0.9050 - val_loss: 0.3712 - val_acc: 0.8682\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:134,FP:37,FN:8,TP:156,Macc:0.86742256449,F1:0.873944062302\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2238 - acc: 0.9055 - val_loss: 0.3347 - val_acc: 0.8766\n",
      "9639/9639 [==============================] - 3s 278us/step\n",
      "TN:139,FP:32,FN:7,TP:157,Macc:0.885091226873,F1:0.88951288578\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2230 - acc: 0.9062 - val_loss: 0.3932 - val_acc: 0.8439\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:122,FP:49,FN:5,TP:159,Macc:0.841481188298,F1:0.854833236102\n",
      "Loss: 0.377926\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 1704.1552\n",
      "Function value obtained: 0.3779\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.7683 - acc: 0.8332 - val_loss: 0.5839 - val_acc: 0.7753\n",
      "9639/9639 [==============================] - 5s 470us/step\n",
      "TN:109,FP:62,FN:14,TP:150,Macc:0.776030472287,F1:0.797866883412\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.3126 - acc: 0.8547 - val_loss: 0.5071 - val_acc: 0.8035\n",
      "9639/9639 [==============================] - 2s 158us/step\n",
      "TN:120,FP:51,FN:17,TP:147,Macc:0.799047872058,F1:0.81214919533\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.3020 - acc: 0.8615 - val_loss: 0.5240 - val_acc: 0.7417\n",
      "9639/9639 [==============================] - 2s 159us/step\n",
      "TN:146,FP:25,FN:55,TP:109,Macc:0.759217608215,F1:0.731538125936\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2885 - acc: 0.8673 - val_loss: 0.5347 - val_acc: 0.7222\n",
      "9639/9639 [==============================] - 2s 161us/step\n",
      "TN:158,FP:13,FN:83,TP:81,Macc:0.708939477323,F1:0.627901828117\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2862 - acc: 0.8697 - val_loss: 0.6989 - val_acc: 0.6500\n",
      "9639/9639 [==============================] - 2s 163us/step\n",
      "TN:145,FP:26,FN:79,TP:85,Macc:0.683122904995,F1:0.618176473064\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2770 - acc: 0.8739 - val_loss: 0.7267 - val_acc: 0.6389\n",
      "9639/9639 [==============================] - 2s 162us/step\n",
      "TN:142,FP:29,FN:81,TP:83,Macc:0.668253415169,F1:0.6014439227\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2750 - acc: 0.8757 - val_loss: 0.5508 - val_acc: 0.7426\n",
      "9639/9639 [==============================] - 2s 165us/step\n",
      "TN:132,FP:39,FN:44,TP:120,Macc:0.75181852145,F1:0.743028506477\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2765 - acc: 0.8748 - val_loss: 0.7027 - val_acc: 0.6445\n",
      "9639/9639 [==============================] - 2s 166us/step\n",
      "TN:147,FP:24,FN:79,TP:85,Macc:0.688970857836,F1:0.622705295809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2701 - acc: 0.8778 - val_loss: 0.5384 - val_acc: 0.7384\n",
      "9639/9639 [==============================] - 2s 205us/step\n",
      "TN:152,FP:19,FN:64,TP:100,Macc:0.749322444186,F1:0.706708365085\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2686 - acc: 0.8784 - val_loss: 0.7479 - val_acc: 0.6504\n",
      "9639/9639 [==============================] - 2s 171us/step\n",
      "TN:144,FP:27,FN:79,TP:85,Macc:0.680198928575,F1:0.615936675166\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2689 - acc: 0.8776 - val_loss: 0.5369 - val_acc: 0.7368\n",
      "9639/9639 [==============================] - 2s 175us/step\n",
      "TN:152,FP:19,FN:64,TP:100,Macc:0.749322444186,F1:0.706708365085\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2666 - acc: 0.8789 - val_loss: 0.5658 - val_acc: 0.7346\n",
      "9639/9639 [==============================] - 2s 199us/step\n",
      "TN:149,FP:22,FN:57,TP:107,Macc:0.761891976909,F1:0.730369950302\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2646 - acc: 0.8807 - val_loss: 0.4574 - val_acc: 0.7910\n",
      "9639/9639 [==============================] - 2s 186us/step\n",
      "TN:124,FP:47,FN:28,TP:136,Macc:0.777207194622,F1:0.7838561383\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2636 - acc: 0.8800 - val_loss: 0.6936 - val_acc: 0.6739\n",
      "9639/9639 [==============================] - 2s 163us/step\n",
      "TN:154,FP:17,FN:81,TP:83,Macc:0.70334113221,F1:0.628782649664\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2628 - acc: 0.8809 - val_loss: 0.6101 - val_acc: 0.6926\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:147,FP:24,FN:70,TP:94,Macc:0.716409880385,F1:0.666661261046\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2612 - acc: 0.8813 - val_loss: 0.5479 - val_acc: 0.7269\n",
      "9639/9639 [==============================] - 2s 178us/step\n",
      "TN:150,FP:21,FN:60,TP:104,Macc:0.755669612479,F1:0.719717728803\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2621 - acc: 0.8808 - val_loss: 0.4629 - val_acc: 0.7890\n",
      "9639/9639 [==============================] - 1s 156us/step\n",
      "TN:129,FP:42,FN:30,TP:134,Macc:0.785729516156,F1:0.788229750004\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2596 - acc: 0.8828 - val_loss: 0.6199 - val_acc: 0.7145\n",
      "9639/9639 [==============================] - 2s 171us/step\n",
      "TN:149,FP:22,FN:67,TP:97,Macc:0.731404174076,F1:0.685506953306\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2578 - acc: 0.8837 - val_loss: 0.5021 - val_acc: 0.7634\n",
      "9639/9639 [==============================] - 2s 175us/step\n",
      "TN:148,FP:23,FN:47,TP:117,Macc:0.789455803322,F1:0.769731320719\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2563 - acc: 0.8833 - val_loss: 0.7807 - val_acc: 0.6384\n",
      "9639/9639 [==============================] - 2s 163us/step\n",
      "TN:153,FP:18,FN:92,TP:72,Macc:0.666880572673,F1:0.566924051629\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2544 - acc: 0.8841 - val_loss: 0.5862 - val_acc: 0.7158\n",
      "9639/9639 [==============================] - 2s 161us/step\n",
      "TN:146,FP:25,FN:64,TP:100,Macc:0.731778585665,F1:0.692036070013\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2533 - acc: 0.8850 - val_loss: 0.4847 - val_acc: 0.7823\n",
      "9639/9639 [==============================] - 2s 172us/step\n",
      "TN:146,FP:25,FN:47,TP:117,Macc:0.783607850482,F1:0.764700355842\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2539 - acc: 0.8847 - val_loss: 0.5010 - val_acc: 0.7741\n",
      "9639/9639 [==============================] - 2s 160us/step\n",
      "TN:149,FP:22,FN:47,TP:117,Macc:0.792379779742,F1:0.77227170913\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2531 - acc: 0.8851 - val_loss: 0.5004 - val_acc: 0.7672\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:143,FP:28,FN:48,TP:116,Macc:0.771787140938,F1:0.753241222674\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2527 - acc: 0.8855 - val_loss: 0.6527 - val_acc: 0.7011\n",
      "9639/9639 [==============================] - 2s 181us/step\n",
      "TN:145,FP:26,FN:67,TP:97,Macc:0.719708268395,F1:0.675952748626\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2524 - acc: 0.8851 - val_loss: 0.4593 - val_acc: 0.7842\n",
      "9639/9639 [==============================] - 2s 170us/step\n",
      "TN:138,FP:33,FN:38,TP:126,Macc:0.787655061671,F1:0.780180206731\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2508 - acc: 0.8859 - val_loss: 0.5111 - val_acc: 0.7638\n",
      "9639/9639 [==============================] - 2s 179us/step\n",
      "TN:145,FP:26,FN:49,TP:115,Macc:0.774586313495,F1:0.754092837578\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 41s 35ms/step - loss: 0.2500 - acc: 0.8878 - val_loss: 0.5092 - val_acc: 0.7593\n",
      "9639/9639 [==============================] - 2s 162us/step\n",
      "TN:151,FP:20,FN:54,TP:110,Macc:0.776886270599,F1:0.748293837329\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2495 - acc: 0.8861 - val_loss: 0.4692 - val_acc: 0.7751A: 1s - loss:\n",
      "9639/9639 [==============================] - 2s 164us/step\n",
      "TN:144,FP:27,FN:44,TP:120,Macc:0.786906238491,F1:0.771698641947\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2498 - acc: 0.8859 - val_loss: 0.5036 - val_acc: 0.7636\n",
      "9639/9639 [==============================] - 2s 165us/step\n",
      "TN:144,FP:27,FN:46,TP:118,Macc:0.780808677925,F1:0.763748511764\n",
      "Loss: 0.497485\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 1474.9489\n",
      "Function value obtained: 0.4975\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/skopt/optimizer/optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.7654 - acc: 0.8310 - val_loss: 0.6170 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 5s 507us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.3129 - acc: 0.8344 - val_loss: 0.5692 - val_acc: 0.8051\n",
      "9639/9639 [==============================] - 2s 175us/step\n",
      "TN:99,FP:72,FN:0,TP:164,Macc:0.789473632053,F1:0.819994633135\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.3012 - acc: 0.8420 - val_loss: 0.6308 - val_acc: 0.7063\n",
      "9639/9639 [==============================] - 2s 166us/step\n",
      "TN:64,FP:107,FN:0,TP:164,Macc:0.68713445735,F1:0.754017783182\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2935 - acc: 0.8478 - val_loss: 0.6055 - val_acc: 0.7264\n",
      "9639/9639 [==============================] - 2s 176us/step\n",
      "TN:70,FP:101,FN:3,TP:161,Macc:0.69553197502,F1:0.75586329667\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2916 - acc: 0.8480 - val_loss: 0.6086 - val_acc: 0.7237\n",
      "9639/9639 [==============================] - 2s 175us/step\n",
      "TN:71,FP:100,FN:0,TP:164,Macc:0.70760229229,F1:0.766349901075\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2863 - acc: 0.8521 - val_loss: 0.5596 - val_acc: 0.7844\n",
      "9639/9639 [==============================] - 2s 164us/step\n",
      "TN:117,FP:54,FN:23,TP:141,Macc:0.771983261098,F1:0.785509813245\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 41s 35ms/step - loss: 0.2848 - acc: 0.8527 - val_loss: 0.6489 - val_acc: 0.6542\n",
      "9639/9639 [==============================] - 2s 173us/step\n",
      "TN:43,FP:128,FN:0,TP:164,Macc:0.625730952528,F1:0.719293144311\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2858 - acc: 0.8519 - val_loss: 0.6171 - val_acc: 0.7056\n",
      "9639/9639 [==============================] - 2s 162us/step\n",
      "TN:66,FP:105,FN:0,TP:164,Macc:0.69298241019,F1:0.757500558639\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 41s 35ms/step - loss: 0.2808 - acc: 0.8548 - val_loss: 0.6095 - val_acc: 0.7194\n",
      "9639/9639 [==============================] - 2s 185us/step\n",
      "TN:72,FP:99,FN:0,TP:164,Macc:0.71052626871,F1:0.768144639012\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 41s 35ms/step - loss: 0.2805 - acc: 0.8531 - val_loss: 0.5426 - val_acc: 0.8097\n",
      "9639/9639 [==============================] - 2s 175us/step\n",
      "TN:112,FP:59,FN:8,TP:156,Macc:0.803095083248,F1:0.823213549203\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2799 - acc: 0.8535 - val_loss: 0.6389 - val_acc: 0.6736\n",
      "9639/9639 [==============================] - 2s 165us/step\n",
      "TN:53,FP:118,FN:0,TP:164,Macc:0.654970716728,F1:0.735420857724\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2785 - acc: 0.8543 - val_loss: 0.5457 - val_acc: 0.8083\n",
      "9639/9639 [==============================] - 2s 162us/step\n",
      "TN:111,FP:60,FN:12,TP:152,Macc:0.787975985694,F1:0.808505180661\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2784 - acc: 0.8568 - val_loss: 0.5808 - val_acc: 0.7611\n",
      "9639/9639 [==============================] - 2s 179us/step\n",
      "TN:94,FP:77,FN:5,TP:159,Macc:0.759609848536,F1:0.794994634511\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2782 - acc: 0.8556 - val_loss: 0.6083 - val_acc: 0.7224\n",
      "9639/9639 [==============================] - 2s 188us/step\n",
      "TN:79,FP:92,FN:5,TP:159,Macc:0.715750202234,F1:0.766259761372\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2784 - acc: 0.8558 - val_loss: 0.5579 - val_acc: 0.7907\n",
      "9639/9639 [==============================] - 2s 172us/step\n",
      "TN:107,FP:64,FN:8,TP:156,Macc:0.788475201147,F1:0.812494570457\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2758 - acc: 0.8638 - val_loss: 0.6208 - val_acc: 0.6937\n",
      "9639/9639 [==============================] - 2s 170us/step\n",
      "TN:62,FP:109,FN:6,TP:158,Macc:0.662993822809,F1:0.733173431015\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2698 - acc: 0.8735 - val_loss: 0.5678 - val_acc: 0.7404\n",
      "9639/9639 [==============================] - 2s 175us/step\n",
      "TN:81,FP:90,FN:0,TP:164,Macc:0.736842056491,F1:0.784683708924\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2701 - acc: 0.8730 - val_loss: 0.5262 - val_acc: 0.7839\n",
      "9639/9639 [==============================] - 2s 165us/step\n",
      "TN:107,FP:64,FN:9,TP:155,Macc:0.785426420864,F1:0.80939404477\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2682 - acc: 0.8734 - val_loss: 0.4932 - val_acc: 0.8139: 0s - loss: 0.2685 - acc: \n",
      "9639/9639 [==============================] - 2s 166us/step\n",
      "TN:120,FP:51,FN:16,TP:148,Macc:0.802096652342,F1:0.815421498993\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2689 - acc: 0.8752 - val_loss: 0.5593 - val_acc: 0.7427\n",
      "9639/9639 [==============================] - 2s 162us/step\n",
      "TN:87,FP:84,FN:2,TP:162,Macc:0.748288354445,F1:0.790238580071\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2672 - acc: 0.8770 - val_loss: 0.5097 - val_acc: 0.7855\n",
      "9639/9639 [==============================] - 2s 166us/step\n",
      "TN:109,FP:62,FN:13,TP:151,Macc:0.779079252571,F1:0.80105555416\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 41s 35ms/step - loss: 0.2656 - acc: 0.8764 - val_loss: 0.5286 - val_acc: 0.7728\n",
      "9639/9639 [==============================] - 2s 187us/step\n",
      "TN:103,FP:68,FN:10,TP:154,Macc:0.7706817349,F1:0.797922039876\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2654 - acc: 0.8774 - val_loss: 0.5208 - val_acc: 0.7758\n",
      "9639/9639 [==============================] - 2s 163us/step\n",
      "TN:101,FP:70,FN:6,TP:158,Macc:0.777028903193,F1:0.806117050379\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2650 - acc: 0.8776 - val_loss: 0.5369 - val_acc: 0.7570\n",
      "9639/9639 [==============================] - 2s 171us/step\n",
      "TN:91,FP:80,FN:7,TP:157,Macc:0.744740358709,F1:0.783037033363\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2653 - acc: 0.8768 - val_loss: 0.5029 - val_acc: 0.7921\n",
      "9639/9639 [==============================] - 2s 169us/step\n",
      "TN:108,FP:63,FN:10,TP:154,Macc:0.785301617001,F1:0.808393509918\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2656 - acc: 0.8784 - val_loss: 0.5482 - val_acc: 0.7443\n",
      "9639/9639 [==============================] - 2s 177us/step\n",
      "TN:90,FP:81,FN:7,TP:157,Macc:0.741816382289,F1:0.781089171022\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2625 - acc: 0.8773 - val_loss: 0.5653 - val_acc: 0.7240\n",
      "9639/9639 [==============================] - 2s 164us/step\n",
      "TN:75,FP:96,FN:0,TP:164,Macc:0.719298197971,F1:0.773579647508\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2619 - acc: 0.8785 - val_loss: 0.5110 - val_acc: 0.7765\n",
      "9639/9639 [==============================] - 2s 176us/step\n",
      "TN:114,FP:57,FN:18,TP:146,Macc:0.778455233255,F1:0.795634841427\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2628 - acc: 0.8796 - val_loss: 0.5298 - val_acc: 0.7583\n",
      "9639/9639 [==============================] - 2s 169us/step\n",
      "TN:93,FP:78,FN:4,TP:160,Macc:0.759734652399,F1:0.796014543339\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 41s 34ms/step - loss: 0.2619 - acc: 0.8788 - val_loss: 0.5132 - val_acc: 0.7702\n",
      "9639/9639 [==============================] - 2s 164us/step\n",
      "TN:98,FP:73,FN:8,TP:156,Macc:0.762159413366,F1:0.793887735819\n",
      "Loss: 0.507643\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 1474.2957\n",
      "Function value obtained: 0.5076\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.7576 - acc: 0.8353 - val_loss: 0.7069 - val_acc: 0.5990\n",
      "9639/9639 [==============================] - 5s 515us/step\n",
      "TN:21,FP:150,FN:0,TP:164,Macc:0.561403471285,F1:0.686187478686\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.3104 - acc: 0.8491 - val_loss: 0.5457 - val_acc: 0.8031\n",
      "9639/9639 [==============================] - 2s 190us/step\n",
      "TN:98,FP:73,FN:0,TP:164,Macc:0.786549655633,F1:0.817949749651\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2957 - acc: 0.8609 - val_loss: 0.5370 - val_acc: 0.7760\n",
      "9639/9639 [==============================] - 2s 169us/step\n",
      "TN:91,FP:80,FN:2,TP:162,Macc:0.759984260125,F1:0.798024216444\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2885 - acc: 0.8655 - val_loss: 0.5892 - val_acc: 0.7118\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:68,FP:103,FN:0,TP:164,Macc:0.69883036303,F1:0.761015656971\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2855 - acc: 0.8706 - val_loss: 0.6522 - val_acc: 0.6287\n",
      "9639/9639 [==============================] - 2s 171us/step\n",
      "TN:33,FP:138,FN:0,TP:164,Macc:0.596491188327,F1:0.703857610086\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2778 - acc: 0.8733 - val_loss: 0.5977 - val_acc: 0.6855\n",
      "9639/9639 [==============================] - 2s 168us/step\n",
      "TN:62,FP:109,FN:3,TP:161,Macc:0.672140163659,F1:0.741930274389\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2765 - acc: 0.8755 - val_loss: 0.5703 - val_acc: 0.7077\n",
      "9639/9639 [==============================] - 2s 173us/step\n",
      "TN:65,FP:106,FN:1,TP:163,Macc:0.687009653486,F1:0.752881621229\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2754 - acc: 0.8762 - val_loss: 0.6266 - val_acc: 0.6584\n",
      "9639/9639 [==============================] - 2s 178us/step\n",
      "TN:48,FP:123,FN:4,TP:160,Macc:0.628155713495,F1:0.715878523506\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2717 - acc: 0.8776 - val_loss: 0.5315 - val_acc: 0.7419\n",
      "9639/9639 [==============================] - 2s 168us/step\n",
      "TN:82,FP:89,FN:2,TP:162,Macc:0.733668472345,F1:0.78071759193\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2703 - acc: 0.8781 - val_loss: 0.5717 - val_acc: 0.7064\n",
      "9639/9639 [==============================] - 2s 170us/step\n",
      "TN:68,FP:103,FN:3,TP:161,Macc:0.68968402218,F1:0.752331210207\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2694 - acc: 0.8784 - val_loss: 0.6629 - val_acc: 0.6269\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:35,FP:136,FN:0,TP:164,Macc:0.602339141167,F1:0.706891490746\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2684 - acc: 0.8790 - val_loss: 0.6286 - val_acc: 0.6614\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:51,FP:120,FN:0,TP:164,Macc:0.649122763888,F1:0.732137715836\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2667 - acc: 0.8792 - val_loss: 0.4803 - val_acc: 0.7866\n",
      "9639/9639 [==============================] - 2s 166us/step\n",
      "TN:98,FP:73,FN:5,TP:159,Macc:0.771305754216,F1:0.80302492063\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2660 - acc: 0.8789 - val_loss: 0.6104 - val_acc: 0.6767\n",
      "9639/9639 [==============================] - 2s 174us/step\n",
      "TN:49,FP:122,FN:1,TP:163,Macc:0.640226030765,F1:0.726052770349\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2673 - acc: 0.8791 - val_loss: 0.5749 - val_acc: 0.7075\n",
      "9639/9639 [==============================] - 2s 169us/step\n",
      "TN:65,FP:106,FN:1,TP:163,Macc:0.687009653486,F1:0.752881621229\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2655 - acc: 0.8798 - val_loss: 0.5735 - val_acc: 0.7076\n",
      "9639/9639 [==============================] - 2s 174us/step\n",
      "TN:65,FP:106,FN:1,TP:163,Macc:0.687009653486,F1:0.752881621229\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2643 - acc: 0.8803 - val_loss: 0.5539 - val_acc: 0.7231\n",
      "9639/9639 [==============================] - 2s 188us/step\n",
      "TN:69,FP:102,FN:0,TP:164,Macc:0.70175433945,F1:0.762785468159\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2630 - acc: 0.8809 - val_loss: 0.5792 - val_acc: 0.7082\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:63,FP:108,FN:1,TP:163,Macc:0.681161700646,F1:0.749420082266\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2621 - acc: 0.8803 - val_loss: 0.4941 - val_acc: 0.7724\n",
      "9639/9639 [==============================] - 2s 178us/step\n",
      "TN:91,FP:80,FN:1,TP:163,Macc:0.763033040409,F1:0.80097746494\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2605 - acc: 0.8792 - val_loss: 0.5471 - val_acc: 0.7279\n",
      "9639/9639 [==============================] - 2s 173us/step\n",
      "TN:68,FP:103,FN:3,TP:161,Macc:0.68968402218,F1:0.752331210207\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2595 - acc: 0.8823 - val_loss: 0.5119 - val_acc: 0.7572\n",
      "9639/9639 [==============================] - 2s 165us/step\n",
      "TN:80,FP:91,FN:1,TP:163,Macc:0.730869299788,F1:0.779899020181\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2597 - acc: 0.8817 - val_loss: 0.4971 - val_acc: 0.7691\n",
      "9639/9639 [==============================] - 2s 171us/step\n",
      "TN:85,FP:86,FN:1,TP:163,Macc:0.745489181888,F1:0.789340937931\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2585 - acc: 0.8827 - val_loss: 0.5495 - val_acc: 0.7290\n",
      "9639/9639 [==============================] - 2s 181us/step\n",
      "TN:70,FP:101,FN:1,TP:163,Macc:0.701629535587,F1:0.761677004119\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2568 - acc: 0.8827 - val_loss: 0.5023 - val_acc: 0.7624\n",
      "9639/9639 [==============================] - 2s 172us/step\n",
      "TN:83,FP:88,FN:4,TP:160,Macc:0.730494888198,F1:0.776693716314\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2576 - acc: 0.8823 - val_loss: 0.6041 - val_acc: 0.6917\n",
      "9639/9639 [==============================] - 2s 171us/step\n",
      "TN:58,FP:113,FN:1,TP:163,Macc:0.666541818546,F1:0.740903910263\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2589 - acc: 0.8826 - val_loss: 0.5136 - val_acc: 0.7569\n",
      "9639/9639 [==============================] - 2s 183us/step\n",
      "TN:81,FP:90,FN:2,TP:162,Macc:0.730744495924,F1:0.778840858809\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2565 - acc: 0.8828 - val_loss: 0.6032 - val_acc: 0.6917\n",
      "9639/9639 [==============================] - 2s 169us/step\n",
      "TN:52,FP:119,FN:0,TP:164,Macc:0.652046740308,F1:0.733775614347\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2556 - acc: 0.8821 - val_loss: 0.5642 - val_acc: 0.7200\n",
      "9639/9639 [==============================] - 2s 178us/step\n",
      "TN:70,FP:101,FN:1,TP:163,Macc:0.701629535587,F1:0.761677004119\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2550 - acc: 0.8841 - val_loss: 0.5034 - val_acc: 0.7648\n",
      "9639/9639 [==============================] - 2s 167us/step\n",
      "TN:81,FP:90,FN:1,TP:163,Macc:0.733793276208,F1:0.781769289663\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 42s 35ms/step - loss: 0.2555 - acc: 0.8833 - val_loss: 0.5029 - val_acc: 0.7641\n",
      "9639/9639 [==============================] - 2s 192us/step\n",
      "TN:83,FP:88,FN:2,TP:162,Macc:0.736592448765,F1:0.782603391433\n",
      "Loss: 0.496256\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 1506.9688\n",
      "Function value obtained: 0.4963\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "args [5, 4, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 108s 90ms/step - loss: 1.0222 - acc: 0.8413 - val_loss: 0.8128 - val_acc: 0.5993\n",
      "9639/9639 [==============================] - 10s 1ms/step\n",
      "TN:147,FP:24,FN:94,TP:70,Macc:0.643239153586,F1:0.542630517565\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.3612 - acc: 0.8561 - val_loss: 0.7755 - val_acc: 0.6224\n",
      "9639/9639 [==============================] - 6s 623us/step\n",
      "TN:83,FP:88,FN:46,TP:118,Macc:0.602446116299,F1:0.637832370828\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.3179 - acc: 0.8706 - val_loss: 1.0030 - val_acc: 0.5890\n",
      "9639/9639 [==============================] - 6s 649us/step\n",
      "TN:148,FP:23,FN:100,TP:64,Macc:0.627870448306,F1:0.509955132316\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.3013 - acc: 0.8754 - val_loss: 0.9475 - val_acc: 0.6105\n",
      "9639/9639 [==============================] - 6s 642us/step\n",
      "TN:148,FP:23,FN:95,TP:69,Macc:0.643114349723,F1:0.53905738878\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2924 - acc: 0.8766 - val_loss: 3.1076 - val_acc: 0.5185\n",
      "9639/9639 [==============================] - 6s 616us/step\n",
      "TN:152,FP:19,FN:127,TP:37,Macc:0.557249286337,F1:0.336359428233\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2830 - acc: 0.8813 - val_loss: 2.0392 - val_acc: 0.5580\n",
      "9639/9639 [==============================] - 6s 619us/step\n",
      "TN:151,FP:20,FN:114,TP:50,Macc:0.5939594536,F1:0.427345774759\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 99s 83ms/step - loss: 0.2775 - acc: 0.8831 - val_loss: 1.4324 - val_acc: 0.6154\n",
      "9639/9639 [==============================] - 6s 626us/step\n",
      "TN:160,FP:11,FN:105,TP:59,Macc:0.647714263931,F1:0.504268844443\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 99s 83ms/step - loss: 0.2718 - acc: 0.8850 - val_loss: 1.8591 - val_acc: 0.5707\n",
      "9639/9639 [==============================] - 6s 602us/step\n",
      "TN:151,FP:20,FN:110,TP:54,Macc:0.606154574733,F1:0.453776757199\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2684 - acc: 0.8869 - val_loss: 1.4990 - val_acc: 0.6074\n",
      "9639/9639 [==============================] - 6s 626us/step\n",
      "TN:150,FP:21,FN:95,TP:69,Macc:0.648962302563,F1:0.543302006433\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2657 - acc: 0.8874 - val_loss: 1.0368 - val_acc: 0.6230\n",
      "9639/9639 [==============================] - 6s 645us/step\n",
      "TN:156,FP:15,FN:99,TP:65,Macc:0.65431103995,F1:0.532781989093\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2637 - acc: 0.8889 - val_loss: 1.7213 - val_acc: 0.5632\n",
      "9639/9639 [==============================] - 6s 612us/step\n",
      "TN:162,FP:9,FN:121,TP:43,Macc:0.604781732238,F1:0.398144086374\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2585 - acc: 0.8906 - val_loss: 1.5144 - val_acc: 0.5841\n",
      "9639/9639 [==============================] - 6s 626us/step\n",
      "TN:155,FP:16,FN:111,TP:53,Macc:0.61480170013,F1:0.454930993728\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2541 - acc: 0.8939 - val_loss: 2.4041 - val_acc: 0.5614\n",
      "9639/9639 [==============================] - 6s 623us/step\n",
      "TN:159,FP:12,FN:112,TP:52,Macc:0.623448825527,F1:0.456135864925\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 98s 82ms/step - loss: 0.2535 - acc: 0.8938 - val_loss: 2.0075 - val_acc: 0.5628\n",
      "9639/9639 [==============================] - 6s 623us/step\n",
      "TN:162,FP:9,FN:125,TP:39,Macc:0.592586611105,F1:0.367920636832\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 98s 82ms/step - loss: 0.2504 - acc: 0.8947 - val_loss: 1.5647 - val_acc: 0.6064\n",
      "9639/9639 [==============================] - 6s 653us/step\n",
      "TN:158,FP:13,FN:104,TP:60,Macc:0.644915091374,F1:0.506324377832\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2459 - acc: 0.8981 - val_loss: 2.1853 - val_acc: 0.5486\n",
      "9639/9639 [==============================] - 6s 637us/step\n",
      "TN:163,FP:8,FN:124,TP:40,Macc:0.598559367808,F1:0.377354598117\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 99s 83ms/step - loss: 0.2461 - acc: 0.8965 - val_loss: 1.6274 - val_acc: 0.6315\n",
      "9639/9639 [==============================] - 6s 608us/step\n",
      "TN:155,FP:16,FN:93,TP:71,Macc:0.66967974523,F1:0.565732019853\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 99s 83ms/step - loss: 0.2450 - acc: 0.8972 - val_loss: 1.8766 - val_acc: 0.6180\n",
      "9639/9639 [==============================] - 6s 604us/step\n",
      "TN:155,FP:16,FN:98,TP:66,Macc:0.654435843813,F1:0.536580429022\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2417 - acc: 0.9001 - val_loss: 1.3049 - val_acc: 0.6943\n",
      "9639/9639 [==============================] - 6s 619us/step\n",
      "TN:150,FP:21,FN:71,TP:93,Macc:0.722133029362,F1:0.669059373211\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2406 - acc: 0.8993 - val_loss: 1.6853 - val_acc: 0.5843\n",
      "9639/9639 [==============================] - 6s 629us/step\n",
      "TN:160,FP:11,FN:110,TP:54,Macc:0.632470362514,F1:0.471611203185\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 98s 82ms/step - loss: 0.2374 - acc: 0.9014 - val_loss: 1.4903 - val_acc: 0.6187\n",
      "9639/9639 [==============================] - 6s 624us/step\n",
      "TN:158,FP:13,FN:98,TP:66,Macc:0.663207773074,F1:0.543205000365\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2384 - acc: 0.9004 - val_loss: 1.8119 - val_acc: 0.6056\n",
      "9639/9639 [==============================] - 6s 639us/step\n",
      "TN:155,FP:16,FN:104,TP:60,Macc:0.636143162114,F1:0.499995193656\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2358 - acc: 0.9009 - val_loss: 2.2934 - val_acc: 0.5689\n",
      "9639/9639 [==============================] - 6s 618us/step\n",
      "TN:163,FP:8,FN:111,TP:53,Macc:0.638193511491,F1:0.47110671767\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 99s 83ms/step - loss: 0.2336 - acc: 0.9031 - val_loss: 2.2561 - val_acc: 0.5792\n",
      "9639/9639 [==============================] - 6s 621us/step\n",
      "TN:157,FP:14,FN:112,TP:52,Macc:0.617600872687,F1:0.452169368362\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2326 - acc: 0.9030 - val_loss: 1.0299 - val_acc: 0.6858\n",
      "9639/9639 [==============================] - 6s 649us/step\n",
      "TN:156,FP:15,FN:79,TP:85,Macc:0.715286645616,F1:0.643934163552\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2306 - acc: 0.9034 - val_loss: 1.4047 - val_acc: 0.6583\n",
      "9639/9639 [==============================] - 6s 608us/step\n",
      "TN:158,FP:13,FN:89,TP:75,Macc:0.690646795624,F1:0.595233043567\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2302 - acc: 0.9046 - val_loss: 1.9645 - val_acc: 0.5541\n",
      "9639/9639 [==============================] - 6s 640us/step\n",
      "TN:164,FP:7,FN:125,TP:39,Macc:0.598434563945,F1:0.371424769108\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2273 - acc: 0.9064 - val_loss: 1.5151 - val_acc: 0.6218\n",
      "9639/9639 [==============================] - 6s 623us/step\n",
      "TN:157,FP:14,FN:98,TP:66,Macc:0.660283796654,F1:0.540978709664\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2253 - acc: 0.9068 - val_loss: 1.8716 - val_acc: 0.6144\n",
      "9639/9639 [==============================] - 6s 618us/step\n",
      "TN:160,FP:11,FN:105,TP:59,Macc:0.647714263931,F1:0.504268844443\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 99s 82ms/step - loss: 0.2248 - acc: 0.9064 - val_loss: 1.4805 - val_acc: 0.6719\n",
      "9639/9639 [==============================] - 6s 622us/step\n",
      "TN:151,FP:20,FN:80,TP:84,Macc:0.697617983233,F1:0.6268603959\n",
      "Loss: 1.462774\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 3413.3111\n",
      "Function value obtained: 1.4628\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "args [1, 4, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.7653 - acc: 0.8470 - val_loss: 0.5542 - val_acc: 0.7921\n",
      "9639/9639 [==============================] - 6s 620us/step\n",
      "TN:128,FP:43,FN:23,TP:141,Macc:0.804147001719,F1:0.810339294561\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.3058 - acc: 0.8610 - val_loss: 0.6657 - val_acc: 0.5678\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:151,FP:20,FN:116,TP:48,Macc:0.587861893033,F1:0.413788505995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2915 - acc: 0.8660 - val_loss: 0.5749 - val_acc: 0.6910\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:148,FP:23,FN:69,TP:95,Macc:0.722382637089,F1:0.673753459074\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2832 - acc: 0.8683 - val_loss: 0.6580 - val_acc: 0.6730\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:148,FP:23,FN:83,TP:81,Macc:0.679699713122,F1:0.604472338038\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2749 - acc: 0.8724 - val_loss: 0.4816 - val_acc: 0.7834\n",
      "9639/9639 [==============================] - 2s 191us/step\n",
      "TN:144,FP:27,FN:39,TP:125,Macc:0.802150139908,F1:0.791133693397\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2671 - acc: 0.8771 - val_loss: 1.7512 - val_acc: 0.5080\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:153,FP:18,FN:136,TP:28,Macc:0.532734240207,F1:0.266662875336\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2621 - acc: 0.8795 - val_loss: 0.8186 - val_acc: 0.6407\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:144,FP:27,FN:88,TP:76,Macc:0.652759906026,F1:0.569283129731\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2588 - acc: 0.8827 - val_loss: 0.5180 - val_acc: 0.7686\n",
      "9639/9639 [==============================] - 2s 201us/step\n",
      "TN:141,FP:30,FN:45,TP:119,Macc:0.775085528948,F1:0.760377845807\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2505 - acc: 0.8874 - val_loss: 0.4743 - val_acc: 0.7937\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:139,FP:32,FN:41,TP:123,Macc:0.781432697241,F1:0.771154325842\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2488 - acc: 0.8895 - val_loss: 0.8342 - val_acc: 0.6175\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:144,FP:27,FN:88,TP:76,Macc:0.652759906026,F1:0.569283129731\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2478 - acc: 0.8888 - val_loss: 0.8736 - val_acc: 0.6186\n",
      "9639/9639 [==============================] - 2s 204us/step\n",
      "TN:146,FP:25,FN:94,TP:70,Macc:0.640315177166,F1:0.540535385031\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2445 - acc: 0.8918 - val_loss: 0.6588 - val_acc: 0.6972\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:148,FP:23,FN:71,TP:93,Macc:0.716285076522,F1:0.664280323767\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2417 - acc: 0.8928 - val_loss: 0.5944 - val_acc: 0.7305\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:152,FP:19,FN:68,TP:96,Macc:0.737127323053,F1:0.688166658435\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2398 - acc: 0.8944 - val_loss: 1.3877 - val_acc: 0.5584\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:155,FP:16,FN:122,TP:42,Macc:0.581265117014,F1:0.378374094845\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2384 - acc: 0.8956 - val_loss: 0.4732 - val_acc: 0.7955\n",
      "9639/9639 [==============================] - 2s 199us/step\n",
      "TN:136,FP:35,FN:33,TP:131,Macc:0.797051010247,F1:0.79393384125\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2367 - acc: 0.8961 - val_loss: 0.6929 - val_acc: 0.7110\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:152,FP:19,FN:66,TP:98,Macc:0.743224883619,F1:0.697503496096\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2350 - acc: 0.8973 - val_loss: 1.0047 - val_acc: 0.5744\n",
      "9639/9639 [==============================] - 2s 202us/step\n",
      "TN:155,FP:16,FN:114,TP:50,Macc:0.60565535928,F1:0.43477806568\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2344 - acc: 0.8984 - val_loss: 0.7447 - val_acc: 0.6323\n",
      "9639/9639 [==============================] - 2s 199us/step\n",
      "TN:148,FP:23,FN:92,TP:72,Macc:0.652260690573,F1:0.555979399161\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2334 - acc: 0.8990 - val_loss: 1.4934 - val_acc: 0.5298\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:167,FP:4,FN:133,TP:31,Macc:0.582816250939,F1:0.311554565725\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2321 - acc: 0.8982 - val_loss: 2.4004 - val_acc: 0.4978\n",
      "9639/9639 [==============================] - 2s 203us/step\n",
      "TN:170,FP:1,FN:151,TP:13,Macc:0.536710135099,F1:0.14606580345\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2292 - acc: 0.9003 - val_loss: 2.8956 - val_acc: 0.4999\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:169,FP:2,FN:146,TP:18,Macc:0.549030060096,F1:0.195650019162\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2264 - acc: 0.9008 - val_loss: 2.7047 - val_acc: 0.4921\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:169,FP:2,FN:150,TP:14,Macc:0.536834938962,F1:0.155553754834\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2276 - acc: 0.9015 - val_loss: 3.4670 - val_acc: 0.4985\n",
      "9639/9639 [==============================] - 2s 201us/step\n",
      "TN:167,FP:4,FN:143,TP:21,Macc:0.552328448105,F1:0.222219671258\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2262 - acc: 0.9014 - val_loss: 2.3126 - val_acc: 0.5097\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:160,FP:11,FN:137,TP:27,Macc:0.550153294865,F1:0.267323343538\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2271 - acc: 0.9007 - val_loss: 2.4397 - val_acc: 0.4999\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:167,FP:4,FN:145,TP:19,Macc:0.546230887539,F1:0.203206159197\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2255 - acc: 0.9029 - val_loss: 2.4202 - val_acc: 0.5003\n",
      "9639/9639 [==============================] - 2s 202us/step\n",
      "TN:166,FP:5,FN:144,TP:20,Macc:0.546355691402,F1:0.211637661909\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2234 - acc: 0.9039 - val_loss: 2.7314 - val_acc: 0.5041\n",
      "9639/9639 [==============================] - 2s 200us/step\n",
      "TN:167,FP:4,FN:143,TP:21,Macc:0.552328448105,F1:0.222219671258\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2231 - acc: 0.9030 - val_loss: 2.6310 - val_acc: 0.5006\n",
      "9639/9639 [==============================] - 2s 199us/step\n",
      "TN:166,FP:5,FN:144,TP:20,Macc:0.546355691402,F1:0.211637661909\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2211 - acc: 0.9043 - val_loss: 2.8085 - val_acc: 0.5023\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:167,FP:4,FN:141,TP:23,Macc:0.558426008672,F1:0.240834998302\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2227 - acc: 0.9029 - val_loss: 2.6948 - val_acc: 0.5012\n",
      "9639/9639 [==============================] - 2s 205us/step\n",
      "TN:166,FP:5,FN:141,TP:23,Macc:0.555502032252,F1:0.23958056546\n",
      "Loss: 2.683658\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 1628.7967\n",
      "Function value obtained: 2.6837\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.7548 - acc: 0.8368 - val_loss: 0.6232 - val_acc: 0.7358\n",
      "9639/9639 [==============================] - 6s 658us/step\n",
      "TN:80,FP:91,FN:0,TP:164,Macc:0.733918080071,F1:0.782810947478\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.3068 - acc: 0.8433 - val_loss: 0.5377 - val_acc: 0.8222\n",
      "9639/9639 [==============================] - 2s 212us/step\n",
      "TN:113,FP:58,FN:5,TP:159,Macc:0.815165400518,F1:0.834640227562\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2936 - acc: 0.8489 - val_loss: 0.5303 - val_acc: 0.8189\n",
      "9639/9639 [==============================] - 2s 207us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:112,FP:59,FN:8,TP:156,Macc:0.803095083248,F1:0.823213549203\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2867 - acc: 0.8588 - val_loss: 0.5331 - val_acc: 0.7981\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:129,FP:42,FN:31,TP:133,Macc:0.782680735873,F1:0.784655221869\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2801 - acc: 0.8731 - val_loss: 0.5589 - val_acc: 0.7359\n",
      "9639/9639 [==============================] - 2s 207us/step\n",
      "TN:83,FP:88,FN:2,TP:162,Macc:0.736592448765,F1:0.782603391433\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2718 - acc: 0.8791 - val_loss: 0.5167 - val_acc: 0.7752\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:99,FP:72,FN:3,TP:161,Macc:0.780327291203,F1:0.811077744656\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2714 - acc: 0.8782 - val_loss: 0.5110 - val_acc: 0.7816\n",
      "9639/9639 [==============================] - 2s 212us/step\n",
      "TN:97,FP:74,FN:3,TP:161,Macc:0.774479338363,F1:0.807012173552\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2683 - acc: 0.8805 - val_loss: 0.5284 - val_acc: 0.7619\n",
      "9639/9639 [==============================] - 2s 214us/step\n",
      "TN:89,FP:82,FN:1,TP:163,Macc:0.757185087569,F1:0.797060687549\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2667 - acc: 0.8797 - val_loss: 0.5186 - val_acc: 0.7707\n",
      "9639/9639 [==============================] - 2s 212us/step\n",
      "TN:93,FP:78,FN:3,TP:161,Macc:0.762783432682,F1:0.799002091077\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2643 - acc: 0.8826 - val_loss: 0.4812 - val_acc: 0.7978\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:105,FP:66,FN:6,TP:158,Macc:0.788724808874,F1:0.81442757507\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2640 - acc: 0.8821 - val_loss: 0.4861 - val_acc: 0.7972\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:121,FP:50,FN:17,TP:147,Macc:0.801971848479,F1:0.814398928498\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2642 - acc: 0.8822 - val_loss: 0.4786 - val_acc: 0.8044\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:122,FP:49,FN:15,TP:149,Macc:0.810993385465,F1:0.823198918415\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2638 - acc: 0.8816 - val_loss: 0.4988 - val_acc: 0.7850\n",
      "9639/9639 [==============================] - 2s 222us/step\n",
      "TN:106,FP:65,FN:9,TP:155,Macc:0.782502444444,F1:0.807286237422\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2617 - acc: 0.8825 - val_loss: 0.4726 - val_acc: 0.8014\n",
      "9639/9639 [==============================] - 2s 213us/step\n",
      "TN:117,FP:54,FN:13,TP:151,Macc:0.802471063931,F1:0.818422703424\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2615 - acc: 0.8819 - val_loss: 0.4835 - val_acc: 0.7924\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:105,FP:66,FN:10,TP:154,Macc:0.77652968774,F1:0.802077904387\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2620 - acc: 0.8826 - val_loss: 0.4730 - val_acc: 0.8094\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:122,FP:49,FN:16,TP:148,Macc:0.807944605182,F1:0.819939094365\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2598 - acc: 0.8837 - val_loss: 0.4882 - val_acc: 0.7986\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:117,FP:54,FN:18,TP:146,Macc:0.787227162515,F1:0.802192307548\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2597 - acc: 0.8835 - val_loss: 0.4591 - val_acc: 0.8163\n",
      "9639/9639 [==============================] - 2s 222us/step\n",
      "TN:129,FP:42,FN:24,TP:140,Macc:0.804022197856,F1:0.809243018381\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2595 - acc: 0.8834 - val_loss: 0.4710 - val_acc: 0.8011\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:105,FP:66,FN:9,TP:155,Macc:0.779578468024,F1:0.805189379776\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2580 - acc: 0.8833 - val_loss: 0.4687 - val_acc: 0.8068\n",
      "9639/9639 [==============================] - 2s 221us/step\n",
      "TN:112,FP:59,FN:11,TP:153,Macc:0.793948742398,F1:0.813824329286\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2562 - acc: 0.8842 - val_loss: 0.4552 - val_acc: 0.8248\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:135,FP:36,FN:24,TP:140,Macc:0.821566056377,F1:0.823523865365\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2554 - acc: 0.8862 - val_loss: 0.4919 - val_acc: 0.8013\n",
      "9639/9639 [==============================] - 2s 208us/step\n",
      "TN:144,FP:27,FN:44,TP:120,Macc:0.786906238491,F1:0.771698641947\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2565 - acc: 0.8853 - val_loss: 0.4446 - val_acc: 0.8166\n",
      "9639/9639 [==============================] - 2s 212us/step\n",
      "TN:112,FP:59,FN:8,TP:156,Macc:0.803095083248,F1:0.823213549203\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2556 - acc: 0.8860 - val_loss: 0.4606 - val_acc: 0.8150\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:123,FP:48,FN:18,TP:146,Macc:0.804771021035,F1:0.815636946636\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2546 - acc: 0.8856 - val_loss: 0.4582 - val_acc: 0.8096\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:112,FP:59,FN:11,TP:153,Macc:0.793948742398,F1:0.813824329286\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2557 - acc: 0.8857 - val_loss: 0.4497 - val_acc: 0.8185\n",
      "9639/9639 [==============================] - 2s 214us/step\n",
      "TN:119,FP:52,FN:14,TP:150,Macc:0.805270236488,F1:0.819666641202\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2554 - acc: 0.8853 - val_loss: 0.4416 - val_acc: 0.8206\n",
      "9639/9639 [==============================] - 2s 209us/step\n",
      "TN:115,FP:56,FN:6,TP:158,Macc:0.817964573074,F1:0.835973383591\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2544 - acc: 0.8858 - val_loss: 0.4440 - val_acc: 0.8219\n",
      "9639/9639 [==============================] - 2s 208us/step\n",
      "TN:119,FP:52,FN:16,TP:148,Macc:0.799172675922,F1:0.813181317872\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2533 - acc: 0.8848 - val_loss: 0.4590 - val_acc: 0.8193\n",
      "9639/9639 [==============================] - 2s 209us/step\n",
      "TN:140,FP:31,FN:33,TP:131,Macc:0.808746915927,F1:0.803675427604\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2552 - acc: 0.8849 - val_loss: 0.4557 - val_acc: 0.8096\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:115,FP:56,FN:9,TP:155,Macc:0.808818232225,F1:0.826661204601\n",
      "Loss: 0.449239\n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 1642.1273\n",
      "Function value obtained: 0.4492\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.7618 - acc: 0.8489 - val_loss: 0.6974 - val_acc: 0.5847\n",
      "9639/9639 [==============================] - 7s 687us/step\n",
      "TN:17,FP:154,FN:2,TP:162,Macc:0.543610005038,F1:0.674995020627\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.3024 - acc: 0.8649 - val_loss: 0.5162 - val_acc: 0.7857\n",
      "9639/9639 [==============================] - 2s 213us/step\n",
      "TN:107,FP:64,FN:7,TP:157,Macc:0.79152398143,F1:0.815578989572\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2888 - acc: 0.8689 - val_loss: 0.6066 - val_acc: 0.6822\n",
      "9639/9639 [==============================] - 2s 213us/step\n",
      "TN:65,FP:106,FN:1,TP:163,Macc:0.687009653486,F1:0.752881621229\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2808 - acc: 0.8716 - val_loss: 0.5473 - val_acc: 0.7376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:81,FP:90,FN:2,TP:162,Macc:0.730744495924,F1:0.778840858809\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2766 - acc: 0.8752 - val_loss: 0.5327 - val_acc: 0.7482\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:87,FP:84,FN:0,TP:164,Macc:0.754385915012,F1:0.796111191005\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2687 - acc: 0.8806 - val_loss: 0.5692 - val_acc: 0.7102\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:70,FP:101,FN:2,TP:162,Macc:0.698580755303,F1:0.758776957996\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2661 - acc: 0.8812 - val_loss: 0.5008 - val_acc: 0.7727\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:95,FP:76,FN:2,TP:162,Macc:0.771680165806,F1:0.80596479155\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2627 - acc: 0.8833 - val_loss: 0.4748 - val_acc: 0.7983\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:111,FP:60,FN:7,TP:157,Macc:0.803219887111,F1:0.824141540504\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2597 - acc: 0.8855 - val_loss: 0.4850 - val_acc: 0.7807\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:98,FP:73,FN:2,TP:162,Macc:0.780452095066,F1:0.812024704603\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2574 - acc: 0.8843 - val_loss: 0.4656 - val_acc: 0.7960\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:107,FP:64,FN:3,TP:161,Macc:0.803719102564,F1:0.82775808461\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2574 - acc: 0.8853 - val_loss: 0.4757 - val_acc: 0.7879\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:97,FP:74,FN:1,TP:163,Macc:0.780576898929,F1:0.812962218752\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2542 - acc: 0.8872 - val_loss: 0.4118 - val_acc: 0.8280\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:111,FP:60,FN:1,TP:163,Macc:0.821512568811,F1:0.842371840963\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2541 - acc: 0.8874 - val_loss: 0.4424 - val_acc: 0.8080\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:105,FP:66,FN:3,TP:161,Macc:0.797871149723,F1:0.82352400825\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2522 - acc: 0.8881 - val_loss: 0.4326 - val_acc: 0.8133\n",
      "9639/9639 [==============================] - 2s 221us/step\n",
      "TN:109,FP:62,FN:3,TP:161,Macc:0.809567055404,F1:0.832035924238\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2525 - acc: 0.8866 - val_loss: 0.4650 - val_acc: 0.7934\n",
      "9639/9639 [==============================] - 2s 213us/step\n",
      "TN:104,FP:67,FN:5,TP:159,Macc:0.788849612737,F1:0.815379208424\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2503 - acc: 0.8882 - val_loss: 0.4058 - val_acc: 0.8328\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:116,FP:55,FN:8,TP:156,Macc:0.814790988928,F1:0.831994537621\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2494 - acc: 0.8892 - val_loss: 0.4044 - val_acc: 0.8320\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:119,FP:52,FN:9,TP:155,Macc:0.820514137905,F1:0.835574039195\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2469 - acc: 0.8900 - val_loss: 0.4424 - val_acc: 0.8059\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:107,FP:64,FN:3,TP:161,Macc:0.803719102564,F1:0.82775808461\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2473 - acc: 0.8897 - val_loss: 0.4176 - val_acc: 0.8174\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:109,FP:62,FN:5,TP:159,Macc:0.803469494837,F1:0.825968599367\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2452 - acc: 0.8906 - val_loss: 0.4137 - val_acc: 0.8211\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:111,FP:60,FN:5,TP:159,Macc:0.809317447677,F1:0.830281772029\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2451 - acc: 0.8912 - val_loss: 0.4135 - val_acc: 0.8190\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:110,FP:61,FN:7,TP:157,Macc:0.800295910691,F1:0.821984091398\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2441 - acc: 0.8915 - val_loss: 0.3769 - val_acc: 0.8469\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:119,FP:52,FN:2,TP:162,Macc:0.841855599888,F1:0.857137403522\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2454 - acc: 0.8897 - val_loss: 0.4136 - val_acc: 0.8201\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:108,FP:63,FN:2,TP:162,Macc:0.809691859267,F1:0.832899472494\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2436 - acc: 0.8908 - val_loss: 0.4288 - val_acc: 0.8094\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:109,FP:62,FN:1,TP:163,Macc:0.81566461597,F1:0.838040860378\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2438 - acc: 0.8903 - val_loss: 0.4287 - val_acc: 0.8101\n",
      "9639/9639 [==============================] - 2s 209us/step\n",
      "TN:110,FP:61,FN:3,TP:161,Macc:0.812491031824,F1:0.834191467859\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2435 - acc: 0.8913 - val_loss: 0.4053 - val_acc: 0.8206\n",
      "9639/9639 [==============================] - 2s 209us/step\n",
      "TN:112,FP:59,FN:6,TP:158,Macc:0.809192643814,F1:0.829390884033\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2437 - acc: 0.8915 - val_loss: 0.4116 - val_acc: 0.8214\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:113,FP:58,FN:10,TP:154,Macc:0.799921499101,F1:0.81914347791\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2411 - acc: 0.8916 - val_loss: 0.4154 - val_acc: 0.8172\n",
      "9639/9639 [==============================] - 2s 212us/step\n",
      "TN:106,FP:65,FN:3,TP:161,Macc:0.800795126144,F1:0.825635618102\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2410 - acc: 0.8939 - val_loss: 0.4186 - val_acc: 0.8164\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:111,FP:60,FN:7,TP:157,Macc:0.803219887111,F1:0.824141540504\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2424 - acc: 0.8913 - val_loss: 0.3968 - val_acc: 0.8265\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:110,FP:61,FN:3,TP:161,Macc:0.812491031824,F1:0.834191467859\n",
      "Loss: 0.389622\n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 1659.6539\n",
      "Function value obtained: 0.3896\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.7584 - acc: 0.8365 - val_loss: 0.6409 - val_acc: 0.7073\n",
      "9639/9639 [==============================] - 7s 719us/step\n",
      "TN:73,FP:98,FN:1,TP:163,Macc:0.710401464847,F1:0.767053570361\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.3077 - acc: 0.8426 - val_loss: 0.6217 - val_acc: 0.7238\n",
      "9639/9639 [==============================] - 2s 222us/step\n",
      "TN:71,FP:100,FN:0,TP:164,Macc:0.70760229229,F1:0.766349901075\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2965 - acc: 0.8468 - val_loss: 0.5979 - val_acc: 0.7277\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:79,FP:92,FN:1,TP:163,Macc:0.727945323368,F1:0.778037678039\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2919 - acc: 0.8455 - val_loss: 0.6347 - val_acc: 0.6645\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:54,FP:117,FN:1,TP:163,Macc:0.654845912865,F1:0.734229073305\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2882 - acc: 0.8555 - val_loss: 0.6218 - val_acc: 0.6872\n",
      "9639/9639 [==============================] - 2s 221us/step\n",
      "TN:63,FP:108,FN:5,TP:159,Macc:0.668966579513,F1:0.737813802008\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2799 - acc: 0.8668 - val_loss: 0.5967 - val_acc: 0.7007\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:67,FP:104,FN:2,TP:162,Macc:0.689808826043,F1:0.753483143054\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2783 - acc: 0.8718 - val_loss: 0.6324 - val_acc: 0.6518\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:54,FP:117,FN:0,TP:164,Macc:0.657894693149,F1:0.737073495483\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2748 - acc: 0.8755 - val_loss: 0.6319 - val_acc: 0.6544\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:51,FP:120,FN:2,TP:162,Macc:0.643025203322,F1:0.726452248302\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2725 - acc: 0.8755 - val_loss: 0.5557 - val_acc: 0.7568\n",
      "9639/9639 [==============================] - 2s 221us/step\n",
      "TN:95,FP:76,FN:6,TP:158,Macc:0.759485044673,F1:0.793964475529\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2704 - acc: 0.8776 - val_loss: 0.5592 - val_acc: 0.7445\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:88,FP:83,FN:5,TP:159,Macc:0.742065990015,F1:0.783245892122\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2690 - acc: 0.8788 - val_loss: 0.5016 - val_acc: 0.8034\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:109,FP:62,FN:6,TP:158,Macc:0.800420714554,F1:0.822911236526\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2666 - acc: 0.8791 - val_loss: 0.4771 - val_acc: 0.8271\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:119,FP:52,FN:13,TP:151,Macc:0.808319016772,F1:0.822882796196\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2660 - acc: 0.8808 - val_loss: 0.5318 - val_acc: 0.6966\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:154,FP:17,FN:78,TP:86,Macc:0.71248747306,F1:0.644189490595\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2646 - acc: 0.8808 - val_loss: 0.5396 - val_acc: 0.6855\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:158,FP:13,FN:78,TP:86,Macc:0.72418337874,F1:0.653987176724\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2646 - acc: 0.8815 - val_loss: 0.7158 - val_acc: 0.5384\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:165,FP:6,FN:133,TP:31,Macc:0.576968298098,F1:0.308454373442\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2626 - acc: 0.8814 - val_loss: 0.8948 - val_acc: 0.5367\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:164,FP:7,FN:128,TP:36,Macc:0.589288223095,F1:0.347822429312\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2595 - acc: 0.8840 - val_loss: 0.8113 - val_acc: 0.5561\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:158,FP:13,FN:116,TP:48,Macc:0.608329727974,F1:0.426662277575\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2608 - acc: 0.8827 - val_loss: 1.0338 - val_acc: 0.5353\n",
      "9639/9639 [==============================] - 2s 228us/step\n",
      "TN:165,FP:6,FN:128,TP:36,Macc:0.592212199515,F1:0.349510954886\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2586 - acc: 0.8844 - val_loss: 0.8008 - val_acc: 0.5606\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:157,FP:14,FN:114,TP:50,Macc:0.611503312121,F1:0.43859200697\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2584 - acc: 0.8843 - val_loss: 0.9045 - val_acc: 0.5493\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:159,FP:12,FN:116,TP:48,Macc:0.611253704394,F1:0.42856707211\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2596 - acc: 0.8827 - val_loss: 1.1664 - val_acc: 0.5389\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:162,FP:9,FN:125,TP:39,Macc:0.592586611105,F1:0.367920636832\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2570 - acc: 0.8846 - val_loss: 1.5176 - val_acc: 0.5271\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:166,FP:5,FN:129,TP:35,Macc:0.592087395652,F1:0.343133750035\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2560 - acc: 0.8844 - val_loss: 1.3895 - val_acc: 0.5311\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:163,FP:8,FN:128,TP:36,Macc:0.586364246675,F1:0.346150140201\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2571 - acc: 0.8847 - val_loss: 0.8773 - val_acc: 0.5500\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:158,FP:13,FN:115,TP:49,Macc:0.611378508257,F1:0.433623896747\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2557 - acc: 0.8851 - val_loss: 1.2358 - val_acc: 0.5436\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:157,FP:14,FN:120,TP:44,Macc:0.593210630421,F1:0.396392111075\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2547 - acc: 0.8862 - val_loss: 1.3737 - val_acc: 0.5367\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:163,FP:8,FN:126,TP:38,Macc:0.592461807241,F1:0.361900960583\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2547 - acc: 0.8858 - val_loss: 1.1991 - val_acc: 0.5450\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:157,FP:14,FN:120,TP:44,Macc:0.593210630421,F1:0.396392111075\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2533 - acc: 0.8875 - val_loss: 1.5844 - val_acc: 0.5244\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:166,FP:5,FN:130,TP:34,Macc:0.589038615368,F1:0.334971918596\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2536 - acc: 0.8863 - val_loss: 1.5193 - val_acc: 0.5324\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:164,FP:7,FN:128,TP:36,Macc:0.589288223095,F1:0.347822429312\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2545 - acc: 0.8859 - val_loss: 1.4370 - val_acc: 0.5351\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:162,FP:9,FN:127,TP:37,Macc:0.586489050538,F1:0.352377152058\n",
      "Loss: 1.430787\n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 1690.1551\n",
      "Function value obtained: 1.4308\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.7655 - acc: 0.8323 - val_loss: 0.6169 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 7s 716us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.3206 - acc: 0.8326 - val_loss: 0.5790 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 214us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.3084 - acc: 0.8326 - val_loss: 0.5660 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.3050 - acc: 0.8328 - val_loss: 0.5919 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2998 - acc: 0.8321 - val_loss: 0.5728 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 185us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2963 - acc: 0.8316 - val_loss: 0.5942 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2943 - acc: 0.8337 - val_loss: 0.5880 - val_acc: 0.7528\n",
      "9639/9639 [==============================] - 2s 200us/step\n",
      "TN:79,FP:92,FN:0,TP:164,Macc:0.730994103651,F1:0.780947103981\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2931 - acc: 0.8368 - val_loss: 0.5398 - val_acc: 0.8157\n",
      "9639/9639 [==============================] - 2s 190us/step\n",
      "TN:108,FP:63,FN:2,TP:162,Macc:0.809691859267,F1:0.832899472494\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2913 - acc: 0.8381 - val_loss: 0.5612 - val_acc: 0.7829\n",
      "9639/9639 [==============================] - 2s 193us/step\n",
      "TN:91,FP:80,FN:0,TP:164,Macc:0.766081820692,F1:0.803916236771\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2900 - acc: 0.8373 - val_loss: 0.6205 - val_acc: 0.7066\n",
      "9639/9639 [==============================] - 2s 207us/step\n",
      "TN:62,FP:109,FN:0,TP:164,Macc:0.681286504509,F1:0.750566886808\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2879 - acc: 0.8379 - val_loss: 0.5958 - val_acc: 0.7381\n",
      "9639/9639 [==============================] - 2s 200us/step\n",
      "TN:74,FP:97,FN:0,TP:164,Macc:0.71637422155,F1:0.77175945247\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2871 - acc: 0.8363 - val_loss: 0.5311 - val_acc: 0.8194\n",
      "9639/9639 [==============================] - 2s 191us/step\n",
      "TN:106,FP:65,FN:3,TP:161,Macc:0.800795126144,F1:0.825635618102\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2873 - acc: 0.8387 - val_loss: 0.5768 - val_acc: 0.7609\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:87,FP:84,FN:0,TP:164,Macc:0.754385915012,F1:0.796111191005\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2873 - acc: 0.8384 - val_loss: 0.5821 - val_acc: 0.7587\n",
      "9639/9639 [==============================] - 2s 193us/step\n",
      "TN:87,FP:84,FN:0,TP:164,Macc:0.754385915012,F1:0.796111191005\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2869 - acc: 0.8390 - val_loss: 0.5913 - val_acc: 0.7368\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:77,FP:94,FN:0,TP:164,Macc:0.725146150811,F1:0.777245917243\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2849 - acc: 0.8416 - val_loss: 0.5651 - val_acc: 0.7744\n",
      "9639/9639 [==============================] - 2s 202us/step\n",
      "TN:93,FP:78,FN:0,TP:164,Macc:0.771929773532,F1:0.807876432659\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2826 - acc: 0.8427 - val_loss: 0.5326 - val_acc: 0.8051\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:100,FP:71,FN:0,TP:164,Macc:0.792397608473,F1:0.822049766707\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2835 - acc: 0.8423 - val_loss: 0.5960 - val_acc: 0.7232\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:67,FP:104,FN:0,TP:164,Macc:0.69590638661,F1:0.759254039387\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2828 - acc: 0.8418 - val_loss: 0.5984 - val_acc: 0.7230\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:70,FP:101,FN:0,TP:164,Macc:0.70467831587,F1:0.764563530249\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2806 - acc: 0.8429 - val_loss: 0.5540 - val_acc: 0.7763\n",
      "9639/9639 [==============================] - 2s 206us/step\n",
      "TN:91,FP:80,FN:0,TP:164,Macc:0.766081820692,F1:0.803916236771\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2824 - acc: 0.8415 - val_loss: 0.5508 - val_acc: 0.7888\n",
      "9639/9639 [==============================] - 2s 202us/step\n",
      "TN:100,FP:71,FN:0,TP:164,Macc:0.792397608473,F1:0.822049766707\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2790 - acc: 0.8455 - val_loss: 0.5389 - val_acc: 0.7938\n",
      "9639/9639 [==============================] - 2s 203us/step\n",
      "TN:96,FP:75,FN:0,TP:164,Macc:0.780701702793,F1:0.813890427733\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2799 - acc: 0.8434 - val_loss: 0.5744 - val_acc: 0.7522\n",
      "9639/9639 [==============================] - 2s 203us/step\n",
      "TN:81,FP:90,FN:0,TP:164,Macc:0.736842056491,F1:0.784683708924\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2797 - acc: 0.8445 - val_loss: 0.5443 - val_acc: 0.7888\n",
      "9639/9639 [==============================] - 2s 201us/step\n",
      "TN:95,FP:76,FN:0,TP:164,Macc:0.777777726372,F1:0.81187583858\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2802 - acc: 0.8426 - val_loss: 0.5385 - val_acc: 0.7970\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:100,FP:71,FN:0,TP:164,Macc:0.792397608473,F1:0.822049766707\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2799 - acc: 0.8456 - val_loss: 0.5785 - val_acc: 0.7479\n",
      "9639/9639 [==============================] - 2s 201us/step\n",
      "TN:82,FP:89,FN:0,TP:164,Macc:0.739766032911,F1:0.786565452479\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2797 - acc: 0.8459 - val_loss: 0.5741 - val_acc: 0.7489\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:80,FP:91,FN:0,TP:164,Macc:0.733918080071,F1:0.782810947478\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2791 - acc: 0.8453 - val_loss: 0.5278 - val_acc: 0.8056\n",
      "9639/9639 [==============================] - 2s 202us/step\n",
      "TN:103,FP:68,FN:1,TP:163,Macc:0.79812075745,F1:0.825311068004\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2774 - acc: 0.8452 - val_loss: 0.5450 - val_acc: 0.7840\n",
      "9639/9639 [==============================] - 2s 199us/step\n",
      "TN:95,FP:76,FN:0,TP:164,Macc:0.777777726372,F1:0.81187583858\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2774 - acc: 0.8483 - val_loss: 0.5625 - val_acc: 0.7626\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:89,FP:82,FN:0,TP:164,Macc:0.760233867852,F1:0.799994677108\n",
      "Loss: 0.556657\n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 1662.9382\n",
      "Function value obtained: 0.5567\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.7692 - acc: 0.8324 - val_loss: 0.6155 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 7s 737us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.3198 - acc: 0.8325 - val_loss: 0.6049 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.3070 - acc: 0.8327 - val_loss: 0.5700 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 205us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.3024 - acc: 0.8325 - val_loss: 0.6257 - val_acc: 0.7038\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:67,FP:104,FN:0,TP:164,Macc:0.69590638661,F1:0.759254039387\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2996 - acc: 0.8370 - val_loss: 0.5827 - val_acc: 0.7676\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:90,FP:81,FN:0,TP:164,Macc:0.763157844272,F1:0.801950662836\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2955 - acc: 0.8401 - val_loss: 0.6344 - val_acc: 0.6757\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:56,FP:115,FN:0,TP:164,Macc:0.663742645989,F1:0.740401154446\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2951 - acc: 0.8387 - val_loss: 0.6242 - val_acc: 0.6970\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:67,FP:104,FN:0,TP:164,Macc:0.69590638661,F1:0.759254039387\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2919 - acc: 0.8429 - val_loss: 0.6705 - val_acc: 0.6019\n",
      "9639/9639 [==============================] - 2s 206us/step\n",
      "TN:23,FP:148,FN:0,TP:164,Macc:0.567251424126,F1:0.689070630147\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2881 - acc: 0.8441 - val_loss: 0.6526 - val_acc: 0.6333\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:38,FP:133,FN:0,TP:164,Macc:0.611111070427,F1:0.711491670073\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2878 - acc: 0.8433 - val_loss: 0.6649 - val_acc: 0.6117\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:29,FP:142,FN:0,TP:164,Macc:0.584795282646,F1:0.697867309841\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2859 - acc: 0.8444 - val_loss: 0.6416 - val_acc: 0.6500\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:43,FP:128,FN:0,TP:164,Macc:0.625730952528,F1:0.719293144311\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2865 - acc: 0.8444 - val_loss: 0.6050 - val_acc: 0.7101\n",
      "9639/9639 [==============================] - 2s 190us/step\n",
      "TN:65,FP:106,FN:0,TP:164,Macc:0.69005843377,F1:0.75575515848\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2854 - acc: 0.8459 - val_loss: 0.6274 - val_acc: 0.6734\n",
      "9639/9639 [==============================] - 2s 193us/step\n",
      "TN:56,FP:115,FN:0,TP:164,Macc:0.663742645989,F1:0.740401154446\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2842 - acc: 0.8467 - val_loss: 0.5906 - val_acc: 0.7283\n",
      "9639/9639 [==============================] - 2s 193us/step\n",
      "TN:76,FP:95,FN:0,TP:164,Macc:0.722222174391,F1:0.775408448705\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2840 - acc: 0.8471 - val_loss: 0.5955 - val_acc: 0.7196\n",
      "9639/9639 [==============================] - ETA:  - 2s 193us/step\n",
      "TN:73,FP:98,FN:0,TP:164,Macc:0.71345024513,F1:0.769947802984\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2820 - acc: 0.8461 - val_loss: 0.6292 - val_acc: 0.6629\n",
      "9639/9639 [==============================] - 2s 192us/step\n",
      "TN:55,FP:116,FN:0,TP:164,Macc:0.660818669569,F1:0.738733577586\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2830 - acc: 0.8459 - val_loss: 0.6036 - val_acc: 0.7037\n",
      "9639/9639 [==============================] - 2s 209us/step\n",
      "TN:70,FP:101,FN:0,TP:164,Macc:0.70467831587,F1:0.764563530249\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2834 - acc: 0.8462 - val_loss: 0.5933 - val_acc: 0.7210\n",
      "9639/9639 [==============================] - 2s 188us/step\n",
      "TN:70,FP:101,FN:0,TP:164,Macc:0.70467831587,F1:0.764563530249\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2810 - acc: 0.8488 - val_loss: 0.5917 - val_acc: 0.7208\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:74,FP:97,FN:0,TP:164,Macc:0.71637422155,F1:0.77175945247\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2796 - acc: 0.8456 - val_loss: 0.5971 - val_acc: 0.7152\n",
      "9639/9639 [==============================] - 2s 190us/step\n",
      "TN:72,FP:99,FN:0,TP:164,Macc:0.71052626871,F1:0.768144639012\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2799 - acc: 0.8465 - val_loss: 0.5840 - val_acc: 0.7308\n",
      "9639/9639 [==============================] - 2s 191us/step\n",
      "TN:77,FP:94,FN:0,TP:164,Macc:0.725146150811,F1:0.777245917243\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2794 - acc: 0.8480 - val_loss: 0.6283 - val_acc: 0.6589\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:53,FP:118,FN:0,TP:164,Macc:0.654970716728,F1:0.735420857724\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2783 - acc: 0.8469 - val_loss: 0.6105 - val_acc: 0.6848\n",
      "9639/9639 [==============================] - ETA:  - 2s 193us/step\n",
      "TN:59,FP:112,FN:0,TP:164,Macc:0.672514575249,F1:0.745449364581\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2783 - acc: 0.8480 - val_loss: 0.6186 - val_acc: 0.6791\n",
      "9639/9639 [==============================] - 2s 192us/step\n",
      "TN:58,FP:113,FN:0,TP:164,Macc:0.669590598829,F1:0.74375899638\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2781 - acc: 0.8452 - val_loss: 0.6112 - val_acc: 0.6862\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:58,FP:113,FN:0,TP:164,Macc:0.669590598829,F1:0.74375899638\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2769 - acc: 0.8478 - val_loss: 0.5926 - val_acc: 0.7217\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:76,FP:95,FN:2,TP:162,Macc:0.716124613824,F1:0.769590927733\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2778 - acc: 0.8492 - val_loss: 0.5778 - val_acc: 0.7428\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:84,FP:87,FN:0,TP:164,Macc:0.745613985751,F1:0.790356145635\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2762 - acc: 0.8492 - val_loss: 0.6325 - val_acc: 0.6481\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:50,FP:121,FN:0,TP:164,Macc:0.646198787468,F1:0.730507113115\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2757 - acc: 0.8499 - val_loss: 0.5971 - val_acc: 0.7062\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:67,FP:104,FN:0,TP:164,Macc:0.69590638661,F1:0.759254039387\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2761 - acc: 0.8480 - val_loss: 0.5855 - val_acc: 0.7261\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:76,FP:95,FN:0,TP:164,Macc:0.722222174391,F1:0.775408448705\n",
      "Loss: 0.579486\n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 1657.8937\n",
      "Function value obtained: 0.5795\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.7588 - acc: 0.8324 - val_loss: 0.6005 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 7s 759us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.3152 - acc: 0.8326 - val_loss: 0.5867 - val_acc: 0.7741\n",
      "9639/9639 [==============================] - 2s 208us/step\n",
      "TN:85,FP:86,FN:0,TP:164,Macc:0.748537962171,F1:0.792265226668\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.3034 - acc: 0.8380 - val_loss: 0.5686 - val_acc: 0.7809\n",
      "9639/9639 [==============================] - 2s 191us/step\n",
      "TN:98,FP:73,FN:5,TP:159,Macc:0.771305754216,F1:0.80302492063\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2957 - acc: 0.8445 - val_loss: 0.5601 - val_acc: 0.7832\n",
      "9639/9639 [==============================] - 2s 192us/step\n",
      "TN:94,FP:77,FN:0,TP:164,Macc:0.774853749952,F1:0.809871198059\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2916 - acc: 0.8487 - val_loss: 0.5963 - val_acc: 0.7351\n",
      "9639/9639 [==============================] - 2s 193us/step\n",
      "TN:76,FP:95,FN:0,TP:164,Macc:0.722222174391,F1:0.775408448705\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2878 - acc: 0.8492 - val_loss: 0.5425 - val_acc: 0.7952\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:99,FP:72,FN:1,TP:163,Macc:0.78642485177,F1:0.817037235655\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2866 - acc: 0.8485 - val_loss: 0.5954 - val_acc: 0.7286\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:76,FP:95,FN:0,TP:164,Macc:0.722222174391,F1:0.775408448705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2839 - acc: 0.8516 - val_loss: 0.5372 - val_acc: 0.7854\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:96,FP:75,FN:0,TP:164,Macc:0.780701702793,F1:0.813890427733\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2827 - acc: 0.8528 - val_loss: 0.5936 - val_acc: 0.7285\n",
      "9639/9639 [==============================] - 2s 199us/step\n",
      "TN:74,FP:97,FN:0,TP:164,Macc:0.71637422155,F1:0.77175945247\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2809 - acc: 0.8531 - val_loss: 0.5661 - val_acc: 0.7538\n",
      "9639/9639 [==============================] - 2s 206us/step\n",
      "TN:86,FP:85,FN:2,TP:162,Macc:0.745364378025,F1:0.788315850024\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2814 - acc: 0.8518 - val_loss: 0.5190 - val_acc: 0.8073\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:109,FP:62,FN:8,TP:156,Macc:0.794323153987,F1:0.816748489605\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2812 - acc: 0.8539 - val_loss: 0.5420 - val_acc: 0.7870\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:100,FP:71,FN:2,TP:162,Macc:0.786300047906,F1:0.816115527752\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2791 - acc: 0.8530 - val_loss: 0.5724 - val_acc: 0.7487\n",
      "9639/9639 [==============================] - 2s 192us/step\n",
      "TN:81,FP:90,FN:1,TP:163,Macc:0.733793276208,F1:0.781769289663\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2789 - acc: 0.8542 - val_loss: 0.5417 - val_acc: 0.7832\n",
      "9639/9639 [==============================] - 2s 206us/step\n",
      "TN:92,FP:79,FN:0,TP:164,Macc:0.769005797112,F1:0.805891469588\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2779 - acc: 0.8561 - val_loss: 0.5335 - val_acc: 0.7897\n",
      "9639/9639 [==============================] - 2s 193us/step\n",
      "TN:100,FP:71,FN:0,TP:164,Macc:0.792397608473,F1:0.822049766707\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2763 - acc: 0.8581 - val_loss: 0.5258 - val_acc: 0.8020\n",
      "9639/9639 [==============================] - 2s 193us/step\n",
      "TN:101,FP:70,FN:0,TP:164,Macc:0.795321584893,F1:0.82411522763\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2762 - acc: 0.8561 - val_loss: 0.5715 - val_acc: 0.7555\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:84,FP:87,FN:0,TP:164,Macc:0.745613985751,F1:0.790356145635\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2763 - acc: 0.8545 - val_loss: 0.5456 - val_acc: 0.7765\n",
      "9639/9639 [==============================] - 2s 199us/step\n",
      "TN:94,FP:77,FN:2,TP:162,Macc:0.768756189386,F1:0.803964869962\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2753 - acc: 0.8565 - val_loss: 0.5627 - val_acc: 0.7582\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:86,FP:85,FN:0,TP:164,Macc:0.751461938592,F1:0.794183552685\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2722 - acc: 0.8685 - val_loss: 0.5261 - val_acc: 0.7759\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:96,FP:75,FN:2,TP:162,Macc:0.774604142226,F1:0.807974687854\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2693 - acc: 0.8741 - val_loss: 0.5374 - val_acc: 0.7574\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:87,FP:84,FN:2,TP:162,Macc:0.748288354445,F1:0.790238580071\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2673 - acc: 0.8741 - val_loss: 0.4994 - val_acc: 0.7835\n",
      "9639/9639 [==============================] - 2s 202us/step\n",
      "TN:98,FP:73,FN:4,TP:160,Macc:0.774354534499,F1:0.80603996156\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2665 - acc: 0.8758 - val_loss: 0.5041 - val_acc: 0.7819\n",
      "9639/9639 [==============================] - 2s 207us/step\n",
      "TN:97,FP:74,FN:2,TP:162,Macc:0.777528118646,F1:0.809994633685\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2666 - acc: 0.8760 - val_loss: 0.4762 - val_acc: 0.7983\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:105,FP:66,FN:5,TP:159,Macc:0.791773589157,F1:0.817475308842\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2636 - acc: 0.8776 - val_loss: 0.4771 - val_acc: 0.8028\n",
      "9639/9639 [==============================] - 2s 192us/step\n",
      "TN:104,FP:67,FN:5,TP:159,Macc:0.788849612737,F1:0.815379208424\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2653 - acc: 0.8770 - val_loss: 0.5093 - val_acc: 0.7735\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:89,FP:82,FN:1,TP:163,Macc:0.757185087569,F1:0.797060687549\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2638 - acc: 0.8783 - val_loss: 0.4894 - val_acc: 0.7873\n",
      "9639/9639 [==============================] - 2s 199us/step\n",
      "TN:98,FP:73,FN:3,TP:161,Macc:0.777403314783,F1:0.809039851579\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2622 - acc: 0.8789 - val_loss: 0.5153 - val_acc: 0.7658\n",
      "9639/9639 [==============================] - 2s 196us/step\n",
      "TN:93,FP:78,FN:1,TP:163,Macc:0.768880993249,F1:0.804932926723\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2619 - acc: 0.8804 - val_loss: 0.5027 - val_acc: 0.7765\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:95,FP:76,FN:4,TP:160,Macc:0.765582605239,F1:0.799994634235\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2598 - acc: 0.8799 - val_loss: 0.5246 - val_acc: 0.7555\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:84,FP:87,FN:0,TP:164,Macc:0.745613985751,F1:0.790356145635\n",
      "Loss: 0.518729\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 1677.5436\n",
      "Function value obtained: 0.5187\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.7660 - acc: 0.8322 - val_loss: 0.6329 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 7s 776us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.3222 - acc: 0.8326 - val_loss: 0.6384 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.3099 - acc: 0.8325 - val_loss: 0.5947 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.3037 - acc: 0.8327 - val_loss: 0.5622 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 202us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2997 - acc: 0.8313 - val_loss: 0.5619 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 205us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2952 - acc: 0.8335 - val_loss: 0.5984 - val_acc: 0.7636\n",
      "9639/9639 [==============================] - 2s 203us/step\n",
      "TN:89,FP:82,FN:0,TP:164,Macc:0.760233867852,F1:0.799994677108\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2925 - acc: 0.8340 - val_loss: 0.5585 - val_acc: 0.7993\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:102,FP:69,FN:1,TP:163,Macc:0.79519678103,F1:0.823226939709\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2920 - acc: 0.8365 - val_loss: 0.5774 - val_acc: 0.7761\n",
      "9639/9639 [==============================] - 2s 195us/step\n",
      "TN:92,FP:79,FN:0,TP:164,Macc:0.769005797112,F1:0.805891469588\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2906 - acc: 0.8398 - val_loss: 0.5661 - val_acc: 0.7775\n",
      "9639/9639 [==============================] - 2s 207us/step\n",
      "TN:90,FP:81,FN:0,TP:164,Macc:0.763157844272,F1:0.801950662836\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2887 - acc: 0.8403 - val_loss: 0.5362 - val_acc: 0.8092\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:102,FP:69,FN:0,TP:164,Macc:0.798245561313,F1:0.826191093944\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2868 - acc: 0.8390 - val_loss: 0.5781 - val_acc: 0.7636\n",
      "9639/9639 [==============================] - 2s 201us/step\n",
      "TN:91,FP:80,FN:0,TP:164,Macc:0.766081820692,F1:0.803916236771\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2878 - acc: 0.8416 - val_loss: 0.5713 - val_acc: 0.7686\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:92,FP:79,FN:0,TP:164,Macc:0.769005797112,F1:0.805891469588\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2845 - acc: 0.8419 - val_loss: 0.5731 - val_acc: 0.7600\n",
      "9639/9639 [==============================] - 2s 200us/step\n",
      "TN:80,FP:91,FN:0,TP:164,Macc:0.733918080071,F1:0.782810947478\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2861 - acc: 0.8405 - val_loss: 0.5696 - val_acc: 0.7710\n",
      "9639/9639 [==============================] - 2s 201us/step\n",
      "TN:93,FP:78,FN:0,TP:164,Macc:0.771929773532,F1:0.807876432659\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2846 - acc: 0.8421 - val_loss: 0.5792 - val_acc: 0.7540\n",
      "9639/9639 [==============================] - 2s 201us/step\n",
      "TN:86,FP:85,FN:0,TP:164,Macc:0.751461938592,F1:0.794183552685\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2834 - acc: 0.8433 - val_loss: 0.5986 - val_acc: 0.7254\n",
      "9639/9639 [==============================] - 2s 203us/step\n",
      "TN:70,FP:101,FN:0,TP:164,Macc:0.70467831587,F1:0.764563530249\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2827 - acc: 0.8439 - val_loss: 0.5620 - val_acc: 0.7728\n",
      "9639/9639 [==============================] - 2s 204us/step\n",
      "TN:92,FP:79,FN:0,TP:164,Macc:0.769005797112,F1:0.805891469588\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2830 - acc: 0.8412 - val_loss: 0.5407 - val_acc: 0.7994\n",
      "9639/9639 [==============================] - 2s 202us/step\n",
      "TN:100,FP:71,FN:0,TP:164,Macc:0.792397608473,F1:0.822049766707\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2821 - acc: 0.8419 - val_loss: 0.5438 - val_acc: 0.7970\n",
      "9639/9639 [==============================] - 2s 201us/step\n",
      "TN:99,FP:72,FN:0,TP:164,Macc:0.789473632053,F1:0.819994633135\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2797 - acc: 0.8421 - val_loss: 0.5504 - val_acc: 0.7875\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:93,FP:78,FN:0,TP:164,Macc:0.771929773532,F1:0.807876432659\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2815 - acc: 0.8425 - val_loss: 0.5303 - val_acc: 0.8098\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:103,FP:68,FN:0,TP:164,Macc:0.801169537733,F1:0.828277444479\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2798 - acc: 0.8413 - val_loss: 0.5306 - val_acc: 0.8161\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:104,FP:67,FN:0,TP:164,Macc:0.804093514153,F1:0.830374358861\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2786 - acc: 0.8449 - val_loss: 0.5458 - val_acc: 0.7971\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:103,FP:68,FN:1,TP:163,Macc:0.79812075745,F1:0.825311068004\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2782 - acc: 0.8452 - val_loss: 0.5234 - val_acc: 0.8144\n",
      "9639/9639 [==============================] - 2s 199us/step\n",
      "TN:104,FP:67,FN:0,TP:164,Macc:0.804093514153,F1:0.830374358861\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2799 - acc: 0.8421 - val_loss: 0.5248 - val_acc: 0.8189\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:110,FP:61,FN:2,TP:162,Macc:0.815539812107,F1:0.8372038826\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2792 - acc: 0.8439 - val_loss: 0.5165 - val_acc: 0.8229\n",
      "9639/9639 [==============================] - 2s 198us/step\n",
      "TN:110,FP:61,FN:1,TP:163,Macc:0.818588592391,F1:0.840200769484\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 43s 36ms/step - loss: 0.2785 - acc: 0.8451 - val_loss: 0.5230 - val_acc: 0.8154\n",
      "9639/9639 [==============================] - 2s 194us/step\n",
      "TN:106,FP:65,FN:0,TP:164,Macc:0.809941466993,F1:0.834600201732\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2769 - acc: 0.8457 - val_loss: 0.5811 - val_acc: 0.7505\n",
      "9639/9639 [==============================] - 2s 197us/step\n",
      "TN:76,FP:95,FN:0,TP:164,Macc:0.722222174391,F1:0.775408448705\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2774 - acc: 0.8450 - val_loss: 0.5403 - val_acc: 0.7977\n",
      "9639/9639 [==============================] - 2s 204us/step\n",
      "TN:101,FP:70,FN:0,TP:164,Macc:0.795321584893,F1:0.82411522763\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2778 - acc: 0.8449 - val_loss: 0.5147 - val_acc: 0.8201\n",
      "9639/9639 [==============================] - 2s 204us/step\n",
      "TN:105,FP:66,FN:0,TP:164,Macc:0.807017490573,F1:0.832481917528\n",
      "Loss: 0.508707\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 1698.2422\n",
      "Function value obtained: 0.5087\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "args [1, 2, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.7601 - acc: 0.8393 - val_loss: 0.6089 - val_acc: 0.7700\n",
      "9639/9639 [==============================] - 8s 833us/step\n",
      "TN:100,FP:71,FN:10,TP:154,Macc:0.76190980564,F1:0.791768369423\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.3152 - acc: 0.8470 - val_loss: 0.8050 - val_acc: 0.6030\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:146,FP:25,FN:96,TP:68,Macc:0.634217616599,F1:0.529177753898\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.3001 - acc: 0.8509 - val_loss: 0.5829 - val_acc: 0.7671\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:138,FP:33,FN:47,TP:117,Macc:0.760216039121,F1:0.745217388697\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2942 - acc: 0.8537 - val_loss: 0.5150 - val_acc: 0.7997\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:132,FP:39,FN:28,TP:136,Macc:0.800599005983,F1:0.802354335764\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2912 - acc: 0.8534 - val_loss: 0.5101 - val_acc: 0.7761\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:133,FP:38,FN:38,TP:126,Macc:0.77303517957,F1:0.768287131434\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2846 - acc: 0.8555 - val_loss: 0.6002 - val_acc: 0.7165\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:144,FP:27,FN:62,TP:102,Macc:0.732028193392,F1:0.696240260034\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2820 - acc: 0.8577 - val_loss: 0.5691 - val_acc: 0.7534\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:135,FP:36,FN:43,TP:121,Macc:0.763639230994,F1:0.753888531984\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2778 - acc: 0.8691 - val_loss: 0.7025 - val_acc: 0.6777\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:147,FP:24,FN:74,TP:90,Macc:0.704214759252,F1:0.647476641108\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2710 - acc: 0.8768 - val_loss: 0.6530 - val_acc: 0.7241\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:145,FP:26,FN:60,TP:104,Macc:0.741049730378,F1:0.707477513855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2668 - acc: 0.8821 - val_loss: 0.8492 - val_acc: 0.6531\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:149,FP:22,FN:79,TP:85,Macc:0.694818810676,F1:0.627300965499\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2628 - acc: 0.8829 - val_loss: 1.6707 - val_acc: 0.5391\n",
      "9639/9639 [==============================] - 2s 222us/step\n",
      "TN:163,FP:8,FN:133,TP:31,Macc:0.571120345258,F1:0.305415271556\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2611 - acc: 0.8849 - val_loss: 1.5789 - val_acc: 0.5351\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:160,FP:11,FN:135,TP:29,Macc:0.556250855431,F1:0.284310226974\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2575 - acc: 0.8864 - val_loss: 1.2413 - val_acc: 0.5871\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:157,FP:14,FN:105,TP:59,Macc:0.63894233467,F1:0.49788556005\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2559 - acc: 0.8865 - val_loss: 1.1580 - val_acc: 0.6083\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:157,FP:14,FN:99,TP:65,Macc:0.65723501637,F1:0.534974548435\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2552 - acc: 0.8880 - val_loss: 1.4860 - val_acc: 0.5570\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:159,FP:12,FN:114,TP:50,Macc:0.617351264961,F1:0.442473453407\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2516 - acc: 0.8899 - val_loss: 1.4557 - val_acc: 0.5545\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:161,FP:10,FN:125,TP:39,Macc:0.589662634685,F1:0.366193248559\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2510 - acc: 0.8901 - val_loss: 2.0459 - val_acc: 0.5307\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:163,FP:8,FN:132,TP:32,Macc:0.574169125542,F1:0.313721988504\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2492 - acc: 0.8892 - val_loss: 1.5548 - val_acc: 0.5595\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:158,FP:13,FN:115,TP:49,Macc:0.611378508257,F1:0.433623896747\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2480 - acc: 0.8913 - val_loss: 1.2417 - val_acc: 0.5503\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:161,FP:10,FN:127,TP:37,Macc:0.583565074118,F1:0.350707055049\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2452 - acc: 0.8922 - val_loss: 2.0855 - val_acc: 0.5136\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:164,FP:7,FN:139,TP:25,Macc:0.555751639978,F1:0.255099006803\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2462 - acc: 0.8915 - val_loss: 0.9852 - val_acc: 0.6143\n",
      "9639/9639 [==============================] - 2s 221us/step\n",
      "TN:155,FP:16,FN:95,TP:69,Macc:0.663582184663,F1:0.554211872175\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2417 - acc: 0.8939 - val_loss: 1.4610 - val_acc: 0.5495\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:163,FP:8,FN:125,TP:39,Macc:0.595510587525,F1:0.369664399042\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2429 - acc: 0.8921 - val_loss: 1.2415 - val_acc: 0.5911\n",
      "9639/9639 [==============================] - 2s 221us/step\n",
      "TN:153,FP:18,FN:104,TP:60,Macc:0.630295209273,F1:0.495862918153\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2396 - acc: 0.8949 - val_loss: 1.2951 - val_acc: 0.5631\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:159,FP:12,FN:120,TP:44,Macc:0.599058583261,F1:0.399995785498\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2400 - acc: 0.8939 - val_loss: 1.3208 - val_acc: 0.5691\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:158,FP:13,FN:114,TP:50,Macc:0.614427288541,F1:0.440524180525\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2385 - acc: 0.8951 - val_loss: 1.3648 - val_acc: 0.5611\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:158,FP:13,FN:116,TP:48,Macc:0.608329727974,F1:0.426662277575\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2382 - acc: 0.8956 - val_loss: 1.2401 - val_acc: 0.5670\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:160,FP:11,FN:117,TP:47,Macc:0.611128900531,F1:0.423419135421\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2360 - acc: 0.8966 - val_loss: 1.5700 - val_acc: 0.5328\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:163,FP:8,FN:134,TP:30,Macc:0.568071564975,F1:0.297026310595\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2354 - acc: 0.8968 - val_loss: 1.0937 - val_acc: 0.5909\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:157,FP:14,FN:105,TP:59,Macc:0.63894233467,F1:0.49788556005\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 44s 36ms/step - loss: 0.2361 - acc: 0.8965 - val_loss: 1.6912 - val_acc: 0.5432\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:165,FP:6,FN:129,TP:35,Macc:0.589163419232,F1:0.341459858025\n",
      "Loss: 1.680472\n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 1735.4189\n",
      "Function value obtained: 1.6805\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.7590 - acc: 0.8402 - val_loss: 0.6087 - val_acc: 0.7802\n",
      "9639/9639 [==============================] - 8s 863us/step\n",
      "TN:111,FP:60,FN:18,TP:146,Macc:0.769683303994,F1:0.789183713171\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.3023 - acc: 0.8563 - val_loss: 0.5322 - val_acc: 0.7829\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:111,FP:60,FN:12,TP:152,Macc:0.787975985694,F1:0.808505180661\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2901 - acc: 0.8639 - val_loss: 0.5304 - val_acc: 0.7773\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:108,FP:63,FN:10,TP:154,Macc:0.785301617001,F1:0.808393509918\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2812 - acc: 0.8730 - val_loss: 0.5121 - val_acc: 0.7709\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:100,FP:71,FN:4,TP:160,Macc:0.78020248734,F1:0.810121195433\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2738 - acc: 0.8774 - val_loss: 0.5555 - val_acc: 0.7211\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:81,FP:90,FN:5,TP:159,Macc:0.721598155075,F1:0.769970478916\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2691 - acc: 0.8804 - val_loss: 0.6306 - val_acc: 0.6509\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:48,FP:123,FN:0,TP:164,Macc:0.640350834628,F1:0.727267600923\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2678 - acc: 0.8817 - val_loss: 0.5575 - val_acc: 0.7208\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:74,FP:97,FN:1,TP:163,Macc:0.713325441267,F1:0.76886266662\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2649 - acc: 0.8822 - val_loss: 0.4996 - val_acc: 0.7947\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:108,FP:63,FN:11,TP:153,Macc:0.782252836717,F1:0.805257714302\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2617 - acc: 0.8824 - val_loss: 0.4919 - val_acc: 0.7775\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:99,FP:72,FN:0,TP:164,Macc:0.789473632053,F1:0.819994633135\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2594 - acc: 0.8839 - val_loss: 0.4665 - val_acc: 0.8003\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:105,FP:66,FN:3,TP:161,Macc:0.797871149723,F1:0.82352400825\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2595 - acc: 0.8831 - val_loss: 0.4368 - val_acc: 0.8159\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:115,FP:56,FN:5,TP:159,Macc:0.821013353358,F1:0.839044682848\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2569 - acc: 0.8845 - val_loss: 0.4703 - val_acc: 0.7983\n",
      "9639/9639 [==============================] - 2s 228us/step\n",
      "TN:108,FP:63,FN:9,TP:155,Macc:0.788350397284,F1:0.811512887813\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2561 - acc: 0.8858 - val_loss: 0.4409 - val_acc: 0.8150\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:113,FP:58,FN:4,TP:160,Macc:0.818214180801,F1:0.837690896775\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2555 - acc: 0.8859 - val_loss: 0.4424 - val_acc: 0.8076\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:112,FP:59,FN:6,TP:158,Macc:0.809192643814,F1:0.829390884033\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2565 - acc: 0.8847 - val_loss: 0.4754 - val_acc: 0.7986\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:118,FP:53,FN:17,TP:147,Macc:0.793199919218,F1:0.80768681271\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2521 - acc: 0.8865 - val_loss: 0.4523 - val_acc: 0.8007\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:113,FP:58,FN:6,TP:158,Macc:0.812116620234,F1:0.831573502251\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2524 - acc: 0.8864 - val_loss: 0.4769 - val_acc: 0.7840\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:100,FP:71,FN:2,TP:162,Macc:0.786300047906,F1:0.816115527752\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2519 - acc: 0.8867 - val_loss: 0.4274 - val_acc: 0.8222\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:114,FP:57,FN:5,TP:159,Macc:0.818089376938,F1:0.836836659841\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2524 - acc: 0.8874 - val_loss: 0.4413 - val_acc: 0.8117\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:107,FP:64,FN:0,TP:164,Macc:0.812865443414,F1:0.836729293558\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2493 - acc: 0.8883 - val_loss: 0.4199 - val_acc: 0.8245\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:117,FP:54,FN:6,TP:158,Macc:0.823812525915,F1:0.840420072409\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2490 - acc: 0.8876 - val_loss: 0.4039 - val_acc: 0.8335\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:116,FP:55,FN:3,TP:161,Macc:0.830034890345,F1:0.847362975021\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2484 - acc: 0.8883 - val_loss: 0.4126 - val_acc: 0.8282\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:118,FP:53,FN:5,TP:159,Macc:0.829785282618,F1:0.845739221034\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2479 - acc: 0.8895 - val_loss: 0.3991 - val_acc: 0.8365\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:124,FP:47,FN:9,TP:155,Macc:0.835134020005,F1:0.84698904393\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2459 - acc: 0.8893 - val_loss: 0.4132 - val_acc: 0.8282\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:116,FP:55,FN:5,TP:159,Macc:0.823937329778,F1:0.841264388574\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2491 - acc: 0.8871 - val_loss: 0.3955 - val_acc: 0.8369\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:116,FP:55,FN:1,TP:163,Macc:0.836132450911,F1:0.853397702153\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2461 - acc: 0.8902 - val_loss: 0.4020 - val_acc: 0.8340\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:118,FP:53,FN:8,TP:156,Macc:0.820638941768,F1:0.836455656757\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2461 - acc: 0.8902 - val_loss: 0.4170 - val_acc: 0.8278\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:120,FP:51,FN:15,TP:149,Macc:0.805145432625,F1:0.818675823035\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2459 - acc: 0.8904 - val_loss: 0.4153 - val_acc: 0.8311\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:130,FP:41,FN:20,TP:144,Macc:0.81914129541,F1:0.825209367644\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2441 - acc: 0.8914 - val_loss: 0.3827 - val_acc: 0.8465\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:126,FP:45,FN:7,TP:157,Macc:0.847079533412,F1:0.857918005021\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2458 - acc: 0.8894 - val_loss: 0.4139 - val_acc: 0.8280\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:113,FP:58,FN:3,TP:161,Macc:0.821262961084,F1:0.840725635658\n",
      "Loss: 0.407015\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 1804.6322\n",
      "Function value obtained: 0.4070\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.7551 - acc: 0.8353 - val_loss: 0.6127 - val_acc: 0.7557\n",
      "9639/9639 [==============================] - 9s 898us/step\n",
      "TN:93,FP:78,FN:7,TP:157,Macc:0.750588311549,F1:0.786962049345\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3040 - acc: 0.8496 - val_loss: 0.5500 - val_acc: 0.7839\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:101,FP:70,FN:5,TP:159,Macc:0.780077683476,F1:0.809154910537\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2905 - acc: 0.8573 - val_loss: 0.5662 - val_acc: 0.7370\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:83,FP:88,FN:3,TP:161,Macc:0.733543668481,F1:0.779655708423\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2799 - acc: 0.8705 - val_loss: 0.5163 - val_acc: 0.7802\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:103,FP:68,FN:5,TP:159,Macc:0.785925636317,F1:0.813293829798\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2735 - acc: 0.8744 - val_loss: 0.5787 - val_acc: 0.7009\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:76,FP:95,FN:11,TP:153,Macc:0.688685591274,F1:0.742713135606\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2693 - acc: 0.8784 - val_loss: 0.5230 - val_acc: 0.7616\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:101,FP:70,FN:8,TP:156,Macc:0.770931342627,F1:0.799994593908\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2645 - acc: 0.8804 - val_loss: 0.4880 - val_acc: 0.8045\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:123,FP:48,FN:21,TP:143,Macc:0.795624680186,F1:0.805628284743\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2615 - acc: 0.8826 - val_loss: 0.4618 - val_acc: 0.8054\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:115,FP:56,FN:13,TP:151,Macc:0.796623111091,F1:0.814010698157\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2608 - acc: 0.8825 - val_loss: 0.4732 - val_acc: 0.7993\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:108,FP:63,FN:5,TP:159,Macc:0.800545518417,F1:0.82382877415\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2573 - acc: 0.8846 - val_loss: 0.4543 - val_acc: 0.8140\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:119,FP:52,FN:18,TP:146,Macc:0.793075115355,F1:0.806624333787\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2594 - acc: 0.8832 - val_loss: 0.4545 - val_acc: 0.8136\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:117,FP:54,FN:8,TP:156,Macc:0.817714965348,F1:0.834219133096\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2564 - acc: 0.8845 - val_loss: 0.4395 - val_acc: 0.8199\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:136,FP:35,FN:32,TP:132,Macc:0.80009979053,F1:0.797577529049\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2556 - acc: 0.8847 - val_loss: 0.4373 - val_acc: 0.8199\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:123,FP:48,FN:18,TP:146,Macc:0.804771021035,F1:0.815636946636\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2549 - acc: 0.8847 - val_loss: 0.4401 - val_acc: 0.8132\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:113,FP:58,FN:7,TP:157,Macc:0.809067839951,F1:0.828490593751\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2539 - acc: 0.8848 - val_loss: 0.4256 - val_acc: 0.8246\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:117,FP:54,FN:11,TP:153,Macc:0.808568624498,F1:0.824792368676\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2526 - acc: 0.8859 - val_loss: 0.4480 - val_acc: 0.8174\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:133,FP:38,FN:26,TP:138,Macc:0.80962054297,F1:0.811759160245\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2511 - acc: 0.8870 - val_loss: 0.4270 - val_acc: 0.8310\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:129,FP:42,FN:15,TP:149,Macc:0.831461220406,F1:0.839431099548\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2510 - acc: 0.8869 - val_loss: 0.4091 - val_acc: 0.8453\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:139,FP:32,FN:24,TP:140,Macc:0.833261962057,F1:0.833327781924\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2509 - acc: 0.8870 - val_loss: 0.4114 - val_acc: 0.8390\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:131,FP:40,FN:21,TP:143,Macc:0.819016491546,F1:0.824201957066\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2487 - acc: 0.8880 - val_loss: 0.4519 - val_acc: 0.8261\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:132,FP:39,FN:29,TP:135,Macc:0.7975502257,F1:0.798811020905\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2491 - acc: 0.8888 - val_loss: 0.4390 - val_acc: 0.8238\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:144,FP:27,FN:34,TP:130,Macc:0.817394041324,F1:0.809963294493\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2459 - acc: 0.8887 - val_loss: 0.4117 - val_acc: 0.8415\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:140,FP:31,FN:20,TP:144,Macc:0.84838105961,F1:0.849551972817\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2465 - acc: 0.8881 - val_loss: 0.4218 - val_acc: 0.8358\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:152,FP:19,FN:34,TP:130,Macc:0.840785852685,F1:0.8306653808\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2478 - acc: 0.8889 - val_loss: 0.4218 - val_acc: 0.8323\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:134,FP:37,FN:24,TP:140,Macc:0.818642079957,F1:0.821108824557\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2455 - acc: 0.8886 - val_loss: 0.4875 - val_acc: 0.8030\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:149,FP:22,FN:46,TP:118,Macc:0.795428560025,F1:0.776310267611\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2450 - acc: 0.8907 - val_loss: 0.4162 - val_acc: 0.8279\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:132,FP:39,FN:21,TP:143,Macc:0.821940467966,F1:0.82658405774\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2444 - acc: 0.8899 - val_loss: 0.4798 - val_acc: 0.7986\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:141,FP:30,FN:37,TP:127,Macc:0.799475771214,F1:0.79127170699\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2447 - acc: 0.8901 - val_loss: 0.4368 - val_acc: 0.8174\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:131,FP:40,FN:26,TP:138,Macc:0.80377259013,F1:0.8070120012\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2437 - acc: 0.8907 - val_loss: 0.4087 - val_acc: 0.8440\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:144,FP:27,FN:27,TP:137,Macc:0.838735503308,F1:0.835360297664\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2452 - acc: 0.8898 - val_loss: 0.4458 - val_acc: 0.8118\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:134,FP:37,FN:28,TP:136,Macc:0.806446958823,F1:0.807116112991\n",
      "Loss: 0.439539\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 1826.9055\n",
      "Function value obtained: 0.4395\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.7534 - acc: 0.8375 - val_loss: 0.6052 - val_acc: 0.7525\n",
      "9639/9639 [==============================] - 9s 933us/step\n",
      "TN:85,FP:86,FN:4,TP:160,Macc:0.736342841038,F1:0.780482483034\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3017 - acc: 0.8562 - val_loss: 0.6010 - val_acc: 0.7093\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:68,FP:103,FN:0,TP:164,Macc:0.69883036303,F1:0.761015656971\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2884 - acc: 0.8678 - val_loss: 0.5929 - val_acc: 0.7115\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:74,FP:97,FN:5,TP:159,Macc:0.701130320134,F1:0.757137581419\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2793 - acc: 0.8735 - val_loss: 0.5778 - val_acc: 0.7095\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:71,FP:100,FN:4,TP:160,Macc:0.695407171157,F1:0.754711723959\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2768 - acc: 0.8758 - val_loss: 0.5755 - val_acc: 0.7138\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:77,FP:94,FN:6,TP:158,Macc:0.706853469111,F1:0.759610090597\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2714 - acc: 0.8797 - val_loss: 0.5364 - val_acc: 0.7497\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:87,FP:84,FN:1,TP:163,Macc:0.751337134728,F1:0.793182029812\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2703 - acc: 0.8791 - val_loss: 0.5642 - val_acc: 0.7178\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:78,FP:93,FN:2,TP:162,Macc:0.721972566664,F1:0.7732644086\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2691 - acc: 0.8805 - val_loss: 0.5376 - val_acc: 0.7422\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:90,FP:81,FN:7,TP:157,Macc:0.741816382289,F1:0.781089171022\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2666 - acc: 0.8802 - val_loss: 0.5394 - val_acc: 0.7460\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:91,FP:80,FN:8,TP:156,Macc:0.741691578426,F1:0.779994635336\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2641 - acc: 0.8816 - val_loss: 0.5339 - val_acc: 0.7453\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:88,FP:83,FN:6,TP:158,Macc:0.739017209732,F1:0.780241570041\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2627 - acc: 0.8821 - val_loss: 0.4981 - val_acc: 0.7857\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:112,FP:59,FN:19,TP:145,Macc:0.769558500131,F1:0.788037996168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2614 - acc: 0.8835 - val_loss: 0.5522 - val_acc: 0.7313\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:86,FP:85,FN:6,TP:158,Macc:0.733169256892,F1:0.776407441699\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2612 - acc: 0.8831 - val_loss: 0.5063 - val_acc: 0.7727\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:100,FP:71,FN:10,TP:154,Macc:0.76190980564,F1:0.791768369423\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2594 - acc: 0.8843 - val_loss: 0.4736 - val_acc: 0.7967\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:109,FP:62,FN:11,TP:153,Macc:0.785176813137,F1:0.807382415558\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2618 - acc: 0.8837 - val_loss: 0.4650 - val_acc: 0.8103\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:129,FP:42,FN:27,TP:137,Macc:0.794875857006,F1:0.798828278561\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2584 - acc: 0.8840 - val_loss: 0.5089 - val_acc: 0.7686\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:98,FP:73,FN:1,TP:163,Macc:0.783500875349,F1:0.81499463341\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2584 - acc: 0.8835 - val_loss: 0.4780 - val_acc: 0.7926\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:118,FP:53,FN:19,TP:145,Macc:0.787102358652,F1:0.801099472245\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2572 - acc: 0.8851 - val_loss: 0.4823 - val_acc: 0.7893\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:101,FP:70,FN:3,TP:161,Macc:0.786175244043,F1:0.81518448629\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2564 - acc: 0.8858 - val_loss: 0.4934 - val_acc: 0.7859\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:109,FP:62,FN:10,TP:154,Macc:0.788225593421,F1:0.810520871892\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2561 - acc: 0.8857 - val_loss: 0.4611 - val_acc: 0.8029\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:114,FP:57,FN:13,TP:151,Macc:0.793699134671,F1:0.811822485959\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2546 - acc: 0.8862 - val_loss: 0.4723 - val_acc: 0.7988\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:111,FP:60,FN:9,TP:155,Macc:0.797122326544,F1:0.817936504655\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2550 - acc: 0.8860 - val_loss: 0.4709 - val_acc: 0.7985\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:111,FP:60,FN:11,TP:153,Macc:0.791024765978,F1:0.811665633116\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2544 - acc: 0.8853 - val_loss: 0.4715 - val_acc: 0.8003\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:112,FP:59,FN:11,TP:153,Macc:0.793948742398,F1:0.813824329286\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2536 - acc: 0.8866 - val_loss: 0.4851 - val_acc: 0.7890\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:105,FP:66,FN:6,TP:158,Macc:0.788724808874,F1:0.81442757507\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2552 - acc: 0.8847 - val_loss: 0.4544 - val_acc: 0.8096\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:115,FP:56,FN:14,TP:150,Macc:0.793574330808,F1:0.810805333506\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2549 - acc: 0.8863 - val_loss: 0.4571 - val_acc: 0.8076\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:115,FP:56,FN:16,TP:148,Macc:0.787476770241,F1:0.804342343019\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2524 - acc: 0.8863 - val_loss: 0.4455 - val_acc: 0.8142\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:124,FP:47,FN:17,TP:147,Macc:0.810743777739,F1:0.821223538471\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2505 - acc: 0.8872 - val_loss: 0.4546 - val_acc: 0.8165\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:127,FP:44,FN:23,TP:141,Macc:0.801223025299,F1:0.808017391651\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2517 - acc: 0.8865 - val_loss: 0.4497 - val_acc: 0.8156\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:124,FP:47,FN:19,TP:145,Macc:0.804646217172,F1:0.814601225292\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2523 - acc: 0.8869 - val_loss: 0.4498 - val_acc: 0.8254\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:132,FP:39,FN:27,TP:137,Macc:0.803647786267,F1:0.805876807685\n",
      "Loss: 0.443381\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 1844.1117\n",
      "Function value obtained: 0.4434\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.7647 - acc: 0.8322 - val_loss: 0.6025 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 9s 911us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3125 - acc: 0.8438 - val_loss: 0.5811 - val_acc: 0.7666\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:98,FP:73,FN:12,TP:152,Macc:0.749964292233,F1:0.781485593656\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.3001 - acc: 0.8508 - val_loss: 0.5681 - val_acc: 0.7641\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:88,FP:83,FN:4,TP:160,Macc:0.745114770299,F1:0.786235450995\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2926 - acc: 0.8539 - val_loss: 0.5809 - val_acc: 0.7404\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:86,FP:85,FN:8,TP:156,Macc:0.727071696325,F1:0.770365027368\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2911 - acc: 0.8574 - val_loss: 0.5641 - val_acc: 0.7527\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:96,FP:75,FN:13,TP:151,Macc:0.741067559109,F1:0.774353569715\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2861 - acc: 0.8619 - val_loss: 0.5489 - val_acc: 0.7554\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:92,FP:79,FN:6,TP:158,Macc:0.750713115412,F1:0.788024564261\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2814 - acc: 0.8675 - val_loss: 0.5269 - val_acc: 0.7628\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:113,FP:58,FN:27,TP:137,Macc:0.748092234285,F1:0.76322569205\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2798 - acc: 0.8702 - val_loss: 0.5383 - val_acc: 0.7423\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:84,FP:87,FN:5,TP:159,Macc:0.730370084335,F1:0.775604434515\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2765 - acc: 0.8725 - val_loss: 0.5799 - val_acc: 0.6979\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:71,FP:100,FN:7,TP:157,Macc:0.686260830307,F1:0.745837959854\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2712 - acc: 0.8748 - val_loss: 0.5370 - val_acc: 0.7345\n",
      "9639/9639 [==============================] - 2s 214us/step\n",
      "TN:82,FP:89,FN:4,TP:160,Macc:0.727570911778,F1:0.774813093669\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2711 - acc: 0.8746 - val_loss: 0.6221 - val_acc: 0.6540\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:51,FP:120,FN:4,TP:160,Macc:0.636927642755,F1:0.720715560462\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2670 - acc: 0.8779 - val_loss: 0.6767 - val_acc: 0.6016\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:25,FP:146,FN:0,TP:164,Macc:0.573099376966,F1:0.69197811209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2690 - acc: 0.8766 - val_loss: 0.5968 - val_acc: 0.6763\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:57,FP:114,FN:2,TP:162,Macc:0.660569061842,F1:0.736358455945\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2663 - acc: 0.8785 - val_loss: 0.5386 - val_acc: 0.7343\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:74,FP:97,FN:6,TP:158,Macc:0.698081539851,F1:0.754171330844\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2661 - acc: 0.8777 - val_loss: 0.5561 - val_acc: 0.7163\n",
      "9639/9639 [==============================] - 2s 213us/step\n",
      "TN:76,FP:95,FN:8,TP:156,Macc:0.697831932124,F1:0.751801930813\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2636 - acc: 0.8796 - val_loss: 0.5121 - val_acc: 0.7525\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:86,FP:85,FN:1,TP:163,Macc:0.748413158308,F1:0.791256822332\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2646 - acc: 0.8780 - val_loss: 0.5130 - val_acc: 0.7524\n",
      "9639/9639 [==============================] - 2s 222us/step\n",
      "TN:93,FP:78,FN:11,TP:153,Macc:0.738393190416,F1:0.774678159434\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2624 - acc: 0.8807 - val_loss: 0.5070 - val_acc: 0.7590\n",
      "9639/9639 [==============================] - 2s 214us/step\n",
      "TN:97,FP:74,FN:11,TP:153,Macc:0.750089096096,F1:0.782603294442\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2616 - acc: 0.8802 - val_loss: 0.5875 - val_acc: 0.6900\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:61,FP:110,FN:1,TP:163,Macc:0.675313747806,F1:0.745990227999\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2607 - acc: 0.8794 - val_loss: 0.4597 - val_acc: 0.7937\n",
      "9639/9639 [==============================] - 2s 209us/step\n",
      "TN:98,FP:73,FN:3,TP:161,Macc:0.777403314783,F1:0.809039851579\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2595 - acc: 0.8791 - val_loss: 0.4503 - val_acc: 0.7975\n",
      "9639/9639 [==============================] - 2s 211us/step\n",
      "TN:103,FP:68,FN:7,TP:157,Macc:0.77982807575,F1:0.807192533075\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2574 - acc: 0.8813 - val_loss: 0.4795 - val_acc: 0.7759\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:95,FP:76,FN:8,TP:156,Macc:0.753387484106,F1:0.787873406321\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2578 - acc: 0.8821 - val_loss: 0.5285 - val_acc: 0.7372\n",
      "9639/9639 [==============================] - 2s 214us/step\n",
      "TN:73,FP:98,FN:2,TP:162,Macc:0.707352684564,F1:0.764145685733\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2578 - acc: 0.8814 - val_loss: 0.4614 - val_acc: 0.7910\n",
      "9639/9639 [==============================] - 2s 210us/step\n",
      "TN:100,FP:71,FN:6,TP:158,Macc:0.774104926773,F1:0.804065852297\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2570 - acc: 0.8827 - val_loss: 0.4898 - val_acc: 0.7682\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:91,FP:80,FN:2,TP:162,Macc:0.759984260125,F1:0.798024216444\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2569 - acc: 0.8814 - val_loss: 0.4705 - val_acc: 0.7832\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:94,FP:77,FN:3,TP:161,Macc:0.765707409102,F1:0.800989667444\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2559 - acc: 0.8827 - val_loss: 0.5041 - val_acc: 0.7540\n",
      "9639/9639 [==============================] - 2s 215us/step\n",
      "TN:85,FP:86,FN:6,TP:158,Macc:0.730245280472,F1:0.774504473652\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2539 - acc: 0.8843 - val_loss: 0.5151 - val_acc: 0.7510\n",
      "9639/9639 [==============================] - 2s 214us/step\n",
      "TN:80,FP:91,FN:1,TP:163,Macc:0.730869299788,F1:0.779899020181\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 44s 37ms/step - loss: 0.2535 - acc: 0.8838 - val_loss: 0.4617 - val_acc: 0.7891\n",
      "9639/9639 [==============================] - 2s 212us/step\n",
      "TN:100,FP:71,FN:5,TP:159,Macc:0.777153707056,F1:0.807101208286\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2536 - acc: 0.8841 - val_loss: 0.5565 - val_acc: 0.7182\n",
      "9639/9639 [==============================] - 2s 212us/step\n",
      "TN:68,FP:103,FN:3,TP:161,Macc:0.68968402218,F1:0.752331210207\n",
      "Loss: 0.549797\n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 1819.1576\n",
      "Function value obtained: 0.5498\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.7586 - acc: 0.8323 - val_loss: 0.6395 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 10s 987us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3088 - acc: 0.8338 - val_loss: 0.5822 - val_acc: 0.7817\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:106,FP:65,FN:7,TP:157,Macc:0.78860000501,F1:0.81346608044\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2977 - acc: 0.8424 - val_loss: 0.5659 - val_acc: 0.7879\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:102,FP:69,FN:0,TP:164,Macc:0.798245561313,F1:0.826191093944\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2914 - acc: 0.8487 - val_loss: 0.5496 - val_acc: 0.7939\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:104,FP:67,FN:4,TP:160,Macc:0.79189839302,F1:0.818408919024\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2889 - acc: 0.8516 - val_loss: 0.5506 - val_acc: 0.8100\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:113,FP:58,FN:9,TP:155,Macc:0.802970279384,F1:0.822275712072\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2860 - acc: 0.8544 - val_loss: 0.5703 - val_acc: 0.7609\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:91,FP:80,FN:2,TP:162,Macc:0.759984260125,F1:0.798024216444\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2801 - acc: 0.8669 - val_loss: 0.5456 - val_acc: 0.7779\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:101,FP:70,FN:7,TP:157,Macc:0.77398012291,F1:0.803063651346\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2781 - acc: 0.8703 - val_loss: 0.5381 - val_acc: 0.7716\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:108,FP:63,FN:18,TP:146,Macc:0.760911374734,F1:0.782836356972\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2732 - acc: 0.8760 - val_loss: 0.5409 - val_acc: 0.7689\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:98,FP:73,FN:6,TP:158,Macc:0.768256973933,F1:0.799994613719\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2718 - acc: 0.8759 - val_loss: 0.5226 - val_acc: 0.7777\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:103,FP:68,FN:9,TP:155,Macc:0.773730515183,F1:0.801028174064\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2688 - acc: 0.8800 - val_loss: 0.5439 - val_acc: 0.7434\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:84,FP:87,FN:4,TP:160,Macc:0.733418864618,F1:0.778583490448\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2661 - acc: 0.8798 - val_loss: 0.5630 - val_acc: 0.7307\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:85,FP:86,FN:6,TP:158,Macc:0.730245280472,F1:0.774504473652\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2646 - acc: 0.8820 - val_loss: 0.5211 - val_acc: 0.7663\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:109,FP:62,FN:17,TP:147,Macc:0.766884131437,F1:0.788198286951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2633 - acc: 0.8816 - val_loss: 0.5267 - val_acc: 0.7599\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:95,FP:76,FN:6,TP:158,Macc:0.759485044673,F1:0.793964475529\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2641 - acc: 0.8808 - val_loss: 0.4858 - val_acc: 0.7984\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:116,FP:55,FN:17,TP:147,Macc:0.787351966378,F1:0.803273199565\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2613 - acc: 0.8817 - val_loss: 0.4837 - val_acc: 0.8027\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:115,FP:56,FN:13,TP:151,Macc:0.796623111091,F1:0.814010698157\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2604 - acc: 0.8820 - val_loss: 0.4911 - val_acc: 0.7905\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:106,FP:65,FN:3,TP:161,Macc:0.800795126144,F1:0.825635618102\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2577 - acc: 0.8853 - val_loss: 0.4828 - val_acc: 0.7947\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:109,FP:62,FN:9,TP:155,Macc:0.791274373704,F1:0.813642853447\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2582 - acc: 0.8835 - val_loss: 0.4738 - val_acc: 0.8064\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:130,FP:41,FN:26,TP:138,Macc:0.80084861371,F1:0.804659181977\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2566 - acc: 0.8856 - val_loss: 0.4836 - val_acc: 0.7983\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:138,FP:33,FN:38,TP:126,Macc:0.787655061671,F1:0.780180206731\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2561 - acc: 0.8849 - val_loss: 0.4795 - val_acc: 0.7901\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:152,FP:19,FN:49,TP:115,Macc:0.795054148436,F1:0.771806579337\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2538 - acc: 0.8872 - val_loss: 0.4619 - val_acc: 0.8103\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:147,FP:24,FN:40,TP:124,Macc:0.807873288885,F1:0.794866253325\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2549 - acc: 0.8863 - val_loss: 0.4565 - val_acc: 0.8128\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:135,FP:36,FN:30,TP:134,Macc:0.803273374677,F1:0.802389658541\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2525 - acc: 0.8879 - val_loss: 0.4368 - val_acc: 0.8288\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:129,FP:42,FN:16,TP:148,Macc:0.828412440123,F1:0.836152669831\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2533 - acc: 0.8867 - val_loss: 0.4691 - val_acc: 0.7884\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:146,FP:25,FN:39,TP:125,Macc:0.807998092748,F1:0.796172799137\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2541 - acc: 0.8862 - val_loss: 0.4607 - val_acc: 0.7983\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:145,FP:26,FN:37,TP:127,Macc:0.811171676895,F1:0.801256280705\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2529 - acc: 0.8868 - val_loss: 0.5349 - val_acc: 0.7421\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:163,FP:8,FN:68,TP:96,Macc:0.769291063674,F1:0.716412627349\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2520 - acc: 0.8884 - val_loss: 0.5903 - val_acc: 0.6886\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:167,FP:4,FN:92,TP:72,Macc:0.707816242554,F1:0.599995184482\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2498 - acc: 0.8881 - val_loss: 0.4804 - val_acc: 0.7747\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:154,FP:17,FN:52,TP:112,Macc:0.791755760426,F1:0.76449964057\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2505 - acc: 0.8869 - val_loss: 0.6364 - val_acc: 0.6581\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:170,FP:1,FN:108,TP:56,Macc:0.667807687282,F1:0.506783069177\n",
      "Loss: 0.629609\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 1893.3108\n",
      "Function value obtained: 0.6296\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.7673 - acc: 0.8321 - val_loss: 0.6089 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 9s 983us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.3144 - acc: 0.8357 - val_loss: 0.5913 - val_acc: 0.7751\n",
      "9639/9639 [==============================] - 2s 228us/step\n",
      "TN:89,FP:82,FN:0,TP:164,Macc:0.760233867852,F1:0.799994677108\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.3004 - acc: 0.8473 - val_loss: 0.5440 - val_acc: 0.7922\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:99,FP:72,FN:2,TP:162,Macc:0.783376071486,F1:0.814064976929\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2882 - acc: 0.8633 - val_loss: 0.6729 - val_acc: 0.6068\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:65,FP:106,FN:32,TP:132,Macc:0.592497464704,F1:0.656711068383\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2838 - acc: 0.8697 - val_loss: 0.6614 - val_acc: 0.6203\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:50,FP:121,FN:12,TP:152,Macc:0.609613424068,F1:0.695646981108\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2748 - acc: 0.8757 - val_loss: 0.6250 - val_acc: 0.6618\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:54,FP:117,FN:1,TP:163,Macc:0.654845912865,F1:0.734229073305\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2746 - acc: 0.8766 - val_loss: 0.6486 - val_acc: 0.6411\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:46,FP:125,FN:9,TP:155,Macc:0.607063859238,F1:0.698193039056\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2712 - acc: 0.8768 - val_loss: 0.5391 - val_acc: 0.7453\n",
      "9639/9639 [==============================] - 2s 222us/step\n",
      "TN:92,FP:79,FN:14,TP:150,Macc:0.726322873146,F1:0.763353386385\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2679 - acc: 0.8775 - val_loss: 0.5717 - val_acc: 0.7085\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:73,FP:98,FN:3,TP:161,Macc:0.70430390428,F1:0.76122405228\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2665 - acc: 0.8781 - val_loss: 0.6232 - val_acc: 0.6576\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:48,FP:123,FN:0,TP:164,Macc:0.640350834628,F1:0.727267600923\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2646 - acc: 0.8795 - val_loss: 0.6125 - val_acc: 0.6699\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:61,FP:110,FN:6,TP:158,Macc:0.660069846389,F1:0.731476263025\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2651 - acc: 0.8808 - val_loss: 0.5255 - val_acc: 0.7490\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:95,FP:76,FN:8,TP:156,Macc:0.753387484106,F1:0.787873406321\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2631 - acc: 0.8804 - val_loss: 0.6062 - val_acc: 0.6733\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:58,FP:113,FN:0,TP:164,Macc:0.669590598829,F1:0.74375899638\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2620 - acc: 0.8811 - val_loss: 0.6098 - val_acc: 0.6706\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:70,FP:101,FN:17,TP:147,Macc:0.652849051054,F1:0.713586923571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2609 - acc: 0.8814 - val_loss: 0.5173 - val_acc: 0.7532\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:91,FP:80,FN:10,TP:154,Macc:0.735594017859,F1:0.773863974128\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2604 - acc: 0.8825 - val_loss: 0.6010 - val_acc: 0.6783\n",
      "9639/9639 [==============================] - 2s 223us/step\n",
      "TN:58,FP:113,FN:0,TP:164,Macc:0.669590598829,F1:0.74375899638\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2606 - acc: 0.8817 - val_loss: 0.5276 - val_acc: 0.7459\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:90,FP:81,FN:8,TP:156,Macc:0.738767602005,F1:0.778049502465\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2572 - acc: 0.8824 - val_loss: 0.4704 - val_acc: 0.7931\n",
      "9639/9639 [==============================] - 2s 216us/step\n",
      "TN:102,FP:69,FN:4,TP:160,Macc:0.78605044018,F1:0.814243968776\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2576 - acc: 0.8825 - val_loss: 0.4691 - val_acc: 0.7980\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:113,FP:58,FN:11,TP:153,Macc:0.796872718818,F1:0.81599453856\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2570 - acc: 0.8826 - val_loss: 0.4674 - val_acc: 0.7959\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:103,FP:68,FN:5,TP:159,Macc:0.785925636317,F1:0.813293829798\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2552 - acc: 0.8846 - val_loss: 0.6033 - val_acc: 0.6793\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:70,FP:101,FN:9,TP:155,Macc:0.67723929332,F1:0.73808996337\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2550 - acc: 0.8850 - val_loss: 0.5091 - val_acc: 0.7637\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:93,FP:78,FN:6,TP:158,Macc:0.753637091832,F1:0.789994634786\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2539 - acc: 0.8852 - val_loss: 0.5042 - val_acc: 0.7677\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:100,FP:71,FN:10,TP:154,Macc:0.76190980564,F1:0.791768369423\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2532 - acc: 0.8839 - val_loss: 0.4474 - val_acc: 0.8081\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:106,FP:65,FN:3,TP:161,Macc:0.800795126144,F1:0.825635618102\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2516 - acc: 0.8849 - val_loss: 0.4739 - val_acc: 0.7918\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:105,FP:66,FN:10,TP:154,Macc:0.77652968774,F1:0.802077904387\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2545 - acc: 0.8842 - val_loss: 0.5058 - val_acc: 0.7643\n",
      "9639/9639 [==============================] - 2s 219us/step\n",
      "TN:97,FP:74,FN:7,TP:157,Macc:0.762284217229,F1:0.794931322862\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2524 - acc: 0.8851 - val_loss: 0.4447 - val_acc: 0.8122\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:117,FP:54,FN:11,TP:153,Macc:0.808568624498,F1:0.824792368676\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2507 - acc: 0.8862 - val_loss: 0.4902 - val_acc: 0.7797\n",
      "9639/9639 [==============================] - 2s 218us/step\n",
      "TN:107,FP:64,FN:10,TP:154,Macc:0.78237764058,F1:0.806277286021\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2498 - acc: 0.8871 - val_loss: 0.5164 - val_acc: 0.7593\n",
      "9639/9639 [==============================] - 2s 217us/step\n",
      "TN:115,FP:56,FN:20,TP:144,Macc:0.775281649108,F1:0.791203297224\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2515 - acc: 0.8848 - val_loss: 0.4974 - val_acc: 0.7714\n",
      "9639/9639 [==============================] - 2s 220us/step\n",
      "TN:97,FP:74,FN:8,TP:156,Macc:0.759235436946,F1:0.791872782741\n",
      "Loss: 0.491312\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 1860.0191\n",
      "Function value obtained: 0.4913\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.7621 - acc: 0.8418 - val_loss: 0.6534 - val_acc: 0.6608\n",
      "9639/9639 [==============================] - 10s 1ms/step\n",
      "TN:50,FP:121,FN:3,TP:161,Macc:0.637052446618,F1:0.72196794359\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.3033 - acc: 0.8635 - val_loss: 0.6442 - val_acc: 0.6316\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:52,FP:119,FN:9,TP:155,Macc:0.624607717759,F1:0.707757368462\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2937 - acc: 0.8680 - val_loss: 0.6075 - val_acc: 0.7258\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:141,FP:30,FN:55,TP:109,Macc:0.744597726115,F1:0.719466432439\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2813 - acc: 0.8733 - val_loss: 0.5661 - val_acc: 0.7454\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:139,FP:32,FN:47,TP:117,Macc:0.763140015541,F1:0.74759829399\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2751 - acc: 0.8773 - val_loss: 0.5421 - val_acc: 0.7688\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:149,FP:22,FN:51,TP:113,Macc:0.780184658609,F1:0.755847338973\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2678 - acc: 0.8824 - val_loss: 0.4985 - val_acc: 0.8031\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:131,FP:40,FN:25,TP:139,Macc:0.806821370413,F1:0.810490085393\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2659 - acc: 0.8817 - val_loss: 0.4823 - val_acc: 0.8044\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:144,FP:27,FN:30,TP:134,Macc:0.829589162458,F1:0.824609829301\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2616 - acc: 0.8838 - val_loss: 0.4482 - val_acc: 0.8339\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:128,FP:43,FN:23,TP:141,Macc:0.804147001719,F1:0.810339294561\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2606 - acc: 0.8845 - val_loss: 0.5218 - val_acc: 0.7612\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:142,FP:29,FN:48,TP:116,Macc:0.768863164518,F1:0.750803528868\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2600 - acc: 0.8840 - val_loss: 0.5131 - val_acc: 0.7924\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:143,FP:28,FN:42,TP:122,Macc:0.790079822638,F1:0.777064520222\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2562 - acc: 0.8857 - val_loss: 0.4547 - val_acc: 0.8376\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:139,FP:32,FN:30,TP:134,Macc:0.814969280357,F1:0.812115658219\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2555 - acc: 0.8852 - val_loss: 0.5096 - val_acc: 0.7955\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:145,FP:26,FN:39,TP:125,Macc:0.805074116328,F1:0.793645247627\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2545 - acc: 0.8857 - val_loss: 0.4385 - val_acc: 0.8363\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:131,FP:40,FN:21,TP:143,Macc:0.819016491546,F1:0.824201957066\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2544 - acc: 0.8864 - val_loss: 0.4361 - val_acc: 0.8343\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:130,FP:41,FN:17,TP:147,Macc:0.828287636259,F1:0.83522174613\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2532 - acc: 0.8867 - val_loss: 0.4519 - val_acc: 0.8262\n",
      "9639/9639 [==============================] - 2s 251us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:114,FP:57,FN:11,TP:153,Macc:0.799796695238,F1:0.818176353292\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2515 - acc: 0.8876 - val_loss: 0.4490 - val_acc: 0.8265\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:134,FP:37,FN:22,TP:142,Macc:0.824739640523,F1:0.82798279564\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2514 - acc: 0.8872 - val_loss: 0.4709 - val_acc: 0.8196\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:140,FP:31,FN:27,TP:137,Macc:0.827039597627,F1:0.825295650966\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2512 - acc: 0.8884 - val_loss: 0.4546 - val_acc: 0.8232\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:141,FP:30,FN:31,TP:133,Macc:0.817768452914,F1:0.813450102853\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2500 - acc: 0.8886 - val_loss: 0.4597 - val_acc: 0.8188\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:143,FP:28,FN:35,TP:129,Macc:0.811421284621,F1:0.803732765325\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2498 - acc: 0.8876 - val_loss: 0.4507 - val_acc: 0.8276\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:140,FP:31,FN:24,TP:140,Macc:0.836185938477,F1:0.83581534307\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2481 - acc: 0.8896 - val_loss: 0.4788 - val_acc: 0.8116\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:147,FP:24,FN:40,TP:124,Macc:0.807873288885,F1:0.794866253325\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2463 - acc: 0.8899 - val_loss: 0.4527 - val_acc: 0.8230\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:127,FP:44,FN:22,TP:142,Macc:0.804271805583,F1:0.811423042192\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2447 - acc: 0.8909 - val_loss: 0.4638 - val_acc: 0.8177\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:143,FP:28,FN:31,TP:133,Macc:0.823616405754,F1:0.818455983564\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2446 - acc: 0.8908 - val_loss: 0.4714 - val_acc: 0.8124\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:144,FP:27,FN:37,TP:127,Macc:0.808247700475,F1:0.798736588583\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2428 - acc: 0.8926 - val_loss: 0.4917 - val_acc: 0.7989\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:141,FP:30,FN:38,TP:126,Macc:0.796426990931,F1:0.787494449335\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2438 - acc: 0.8918 - val_loss: 0.4829 - val_acc: 0.8022\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:141,FP:30,FN:37,TP:127,Macc:0.799475771214,F1:0.79127170699\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2419 - acc: 0.8934 - val_loss: 0.4753 - val_acc: 0.8108\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:144,FP:27,FN:36,TP:128,Macc:0.811296480758,F1:0.802502286061\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2421 - acc: 0.8925 - val_loss: 0.4281 - val_acc: 0.8371\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:135,FP:36,FN:22,TP:142,Macc:0.827663616943,F1:0.83040381256\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2408 - acc: 0.8920 - val_loss: 0.4240 - val_acc: 0.8363\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:144,FP:27,FN:28,TP:136,Macc:0.835686723024,F1:0.831798725471\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2414 - acc: 0.8935 - val_loss: 0.4510 - val_acc: 0.8211\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:138,FP:33,FN:23,TP:141,Macc:0.83338676592,F1:0.834313977173\n",
      "Loss: 0.444412\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 1918.1038\n",
      "Function value obtained: 0.4444\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.7706 - acc: 0.8324 - val_loss: 0.6046 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 10s 1ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.3211 - acc: 0.8324 - val_loss: 0.6713 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3093 - acc: 0.8327 - val_loss: 0.5839 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3036 - acc: 0.8325 - val_loss: 0.5752 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2965 - acc: 0.8441 - val_loss: 0.5962 - val_acc: 0.7049\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:87,FP:84,FN:26,TP:138,Macc:0.675117627646,F1:0.715020490201\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2855 - acc: 0.8622 - val_loss: 0.6083 - val_acc: 0.6701\n",
      "9639/9639 [==============================] - 2s 228us/step\n",
      "TN:75,FP:96,FN:21,TP:143,Macc:0.655273812021,F1:0.709672071144\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2820 - acc: 0.8657 - val_loss: 0.5436 - val_acc: 0.7399\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:97,FP:74,FN:18,TP:146,Macc:0.728747634113,F1:0.76041124011\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2784 - acc: 0.8690 - val_loss: 0.5622 - val_acc: 0.7142\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:91,FP:80,FN:26,TP:138,Macc:0.686813533326,F1:0.722507657341\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2772 - acc: 0.8701 - val_loss: 0.5261 - val_acc: 0.7614\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:94,FP:77,FN:1,TP:163,Macc:0.771804969669,F1:0.8069253438\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2742 - acc: 0.8717 - val_loss: 0.5245 - val_acc: 0.7643\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:106,FP:65,FN:10,TP:154,Macc:0.77945366416,F1:0.804172112955\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2689 - acc: 0.8757 - val_loss: 0.5804 - val_acc: 0.7086\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:97,FP:74,FN:37,TP:127,Macc:0.67082080873,F1:0.695884925575\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2687 - acc: 0.8758 - val_loss: 0.4816 - val_acc: 0.7938\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:111,FP:60,FN:9,TP:155,Macc:0.797122326544,F1:0.817936504655\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2647 - acc: 0.8785 - val_loss: 0.5392 - val_acc: 0.7378\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:107,FP:64,FN:31,TP:133,Macc:0.718353254631,F1:0.736836606359\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2636 - acc: 0.8800 - val_loss: 0.4955 - val_acc: 0.7851\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:130,FP:41,FN:37,TP:127,Macc:0.767312030593,F1:0.765054691105\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2636 - acc: 0.8803 - val_loss: 0.5672 - val_acc: 0.7175\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:94,FP:77,FN:25,TP:139,Macc:0.69863424287,F1:0.731573508045\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2616 - acc: 0.8791 - val_loss: 0.5936 - val_acc: 0.6952\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:86,FP:85,FN:23,TP:141,Macc:0.681339992075,F1:0.723071521328\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2583 - acc: 0.8818 - val_loss: 0.5555 - val_acc: 0.7251\n",
      "9639/9639 [==============================] - 2s 224us/step\n",
      "TN:100,FP:71,FN:27,TP:137,Macc:0.710080540823,F1:0.73655367321\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2604 - acc: 0.8808 - val_loss: 0.5348 - val_acc: 0.7435\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:112,FP:59,FN:33,TP:131,Macc:0.726875576165,F1:0.740107478064\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2561 - acc: 0.8828 - val_loss: 0.4466 - val_acc: 0.8197\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:129,FP:42,FN:24,TP:140,Macc:0.804022197856,F1:0.809243018381\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2573 - acc: 0.8823 - val_loss: 0.5045 - val_acc: 0.7679\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:114,FP:57,FN:23,TP:141,Macc:0.763211331838,F1:0.779000026075\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2528 - acc: 0.8837 - val_loss: 0.4949 - val_acc: 0.7831\n",
      "9639/9639 [==============================] - 2s 226us/step\n",
      "TN:140,FP:31,FN:46,TP:118,Macc:0.769112772244,F1:0.753988069899\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2553 - acc: 0.8823 - val_loss: 0.5200 - val_acc: 0.7618\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:128,FP:43,FN:35,TP:129,Macc:0.76756163832,F1:0.767851595738\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2551 - acc: 0.8831 - val_loss: 0.5049 - val_acc: 0.7708\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:127,FP:44,FN:30,TP:134,Macc:0.779881563316,F1:0.78362018984\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2531 - acc: 0.8838 - val_loss: 0.5097 - val_acc: 0.7701\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:123,FP:48,FN:24,TP:140,Macc:0.786478339336,F1:0.795449021344\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2533 - acc: 0.8852 - val_loss: 0.4716 - val_acc: 0.8012\n",
      "9639/9639 [==============================] - 2s 228us/step\n",
      "TN:124,FP:47,FN:22,TP:142,Macc:0.795499876322,F1:0.804527055386\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2530 - acc: 0.8845 - val_loss: 0.4722 - val_acc: 0.7973\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:120,FP:51,FN:19,TP:145,Macc:0.792950311492,F1:0.805550049821\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2527 - acc: 0.8835 - val_loss: 0.4718 - val_acc: 0.7896\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:109,FP:62,FN:11,TP:153,Macc:0.785176813137,F1:0.807382415558\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 45s 37ms/step - loss: 0.2504 - acc: 0.8850 - val_loss: 0.5217 - val_acc: 0.7518\n",
      "9639/9639 [==============================] - 2s 225us/step\n",
      "TN:123,FP:48,FN:33,TP:131,Macc:0.759039316786,F1:0.763842858066\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2506 - acc: 0.8857 - val_loss: 0.4900 - val_acc: 0.7751\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:115,FP:56,FN:19,TP:145,Macc:0.778330429391,F1:0.794515056611\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2500 - acc: 0.8857 - val_loss: 0.4592 - val_acc: 0.7947\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:130,FP:41,FN:33,TP:131,Macc:0.779507151726,F1:0.779756356863\n",
      "Loss: 0.453408\n",
      "Iteration No: 31 ended. Search finished for the next optimal point.\n",
      "Time taken: 1879.4791\n",
      "Function value obtained: 0.4534\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 32 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.7633 - acc: 0.8328 - val_loss: 0.6456 - val_acc: 0.7062\n",
      "9639/9639 [==============================] - 11s 1ms/step\n",
      "TN:70,FP:101,FN:0,TP:164,Macc:0.70467831587,F1:0.764563530249\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3094 - acc: 0.8402 - val_loss: 0.6228 - val_acc: 0.6956\n",
      "9639/9639 [==============================] - 3s 281us/step\n",
      "TN:65,FP:106,FN:0,TP:164,Macc:0.69005843377,F1:0.75575515848\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2951 - acc: 0.8442 - val_loss: 0.5825 - val_acc: 0.7483\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:83,FP:88,FN:0,TP:164,Macc:0.742690009331,F1:0.788456242916\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2904 - acc: 0.8460 - val_loss: 0.5613 - val_acc: 0.7743\n",
      "9639/9639 [==============================] - 2s 257us/step\n",
      "TN:102,FP:69,FN:5,TP:159,Macc:0.783001659897,F1:0.811219090909\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2885 - acc: 0.8471 - val_loss: 0.5564 - val_acc: 0.7812\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:98,FP:73,FN:2,TP:162,Macc:0.780452095066,F1:0.812024704603\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2848 - acc: 0.8531 - val_loss: 0.5190 - val_acc: 0.8110\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:107,FP:64,FN:2,TP:162,Macc:0.806767882847,F1:0.830763822941\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2823 - acc: 0.8570 - val_loss: 0.5237 - val_acc: 0.8028\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:106,FP:65,FN:4,TP:160,Macc:0.79774634586,F1:0.822616696726\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2777 - acc: 0.8700 - val_loss: 0.5047 - val_acc: 0.7945\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:102,FP:69,FN:6,TP:158,Macc:0.779952879613,F1:0.808178740572\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2737 - acc: 0.8742 - val_loss: 0.5259 - val_acc: 0.7620\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:89,FP:82,FN:0,TP:164,Macc:0.760233867852,F1:0.799994677108\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2704 - acc: 0.8758 - val_loss: 0.5234 - val_acc: 0.7610\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:93,FP:78,FN:2,TP:162,Macc:0.765832212966,F1:0.80197484902\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2681 - acc: 0.8782 - val_loss: 0.5026 - val_acc: 0.7843\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:102,FP:69,FN:9,TP:155,Macc:0.770806538763,F1:0.798963658422\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2670 - acc: 0.8781 - val_loss: 0.5055 - val_acc: 0.7794\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:95,FP:76,FN:1,TP:163,Macc:0.774728946089,F1:0.808927648847\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2657 - acc: 0.8794 - val_loss: 0.5095 - val_acc: 0.7734\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:103,FP:68,FN:11,TP:153,Macc:0.767632954617,F1:0.794799769981\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2617 - acc: 0.8823 - val_loss: 0.4814 - val_acc: 0.7979\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:115,FP:56,FN:14,TP:150,Macc:0.793574330808,F1:0.810805333506\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2639 - acc: 0.8824 - val_loss: 0.4732 - val_acc: 0.8063\n",
      "9639/9639 [==============================] - 2s 257us/step\n",
      "TN:118,FP:53,FN:13,TP:151,Macc:0.805395040352,F1:0.820646689869\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2601 - acc: 0.8835 - val_loss: 0.4756 - val_acc: 0.8088\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:124,FP:47,FN:22,TP:142,Macc:0.795499876322,F1:0.804527055386\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2604 - acc: 0.8834 - val_loss: 0.4947 - val_acc: 0.7871\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:118,FP:53,FN:18,TP:146,Macc:0.790151138935,F1:0.804402215915\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2609 - acc: 0.8826 - val_loss: 0.4764 - val_acc: 0.8009\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:121,FP:50,FN:17,TP:147,Macc:0.801971848479,F1:0.814398928498\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2579 - acc: 0.8845 - val_loss: 0.4708 - val_acc: 0.8101\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:131,FP:40,FN:27,TP:137,Macc:0.800723809846,F1:0.803513517775\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2581 - acc: 0.8835 - val_loss: 0.4733 - val_acc: 0.8050\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:127,FP:44,FN:22,TP:142,Macc:0.804271805583,F1:0.811423042192\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2584 - acc: 0.8839 - val_loss: 0.4826 - val_acc: 0.7994\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:128,FP:43,FN:26,TP:138,Macc:0.795000660869,F1:0.799994462377\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2570 - acc: 0.8852 - val_loss: 0.4607 - val_acc: 0.8150\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:127,FP:44,FN:25,TP:139,Macc:0.795125464733,F1:0.801147203486\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2536 - acc: 0.8870 - val_loss: 0.4534 - val_acc: 0.8142\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:116,FP:55,FN:9,TP:155,Macc:0.811742208645,F1:0.828871539828\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2551 - acc: 0.8868 - val_loss: 0.4485 - val_acc: 0.8156\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:124,FP:47,FN:14,TP:150,Macc:0.819890118589,F1:0.831019426099\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2545 - acc: 0.8857 - val_loss: 0.4529 - val_acc: 0.8152\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:119,FP:52,FN:14,TP:150,Macc:0.805270236488,F1:0.819666641202\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2555 - acc: 0.8858 - val_loss: 0.4933 - val_acc: 0.8052\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:148,FP:23,FN:49,TP:115,Macc:0.783358242755,F1:0.761583889298\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2536 - acc: 0.8863 - val_loss: 0.4635 - val_acc: 0.8144\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:140,FP:31,FN:34,TP:130,Macc:0.805698135644,F1:0.799994446353\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2527 - acc: 0.8871 - val_loss: 0.4645 - val_acc: 0.8084\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:141,FP:30,FN:37,TP:127,Macc:0.799475771214,F1:0.79127170699\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2542 - acc: 0.8871 - val_loss: 0.4387 - val_acc: 0.8167\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:119,FP:52,FN:13,TP:151,Macc:0.808319016772,F1:0.822882796196\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2545 - acc: 0.8864 - val_loss: 0.4420 - val_acc: 0.8206\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:135,FP:36,FN:26,TP:138,Macc:0.81546849581,F1:0.816562499039\n",
      "Loss: 0.435454\n",
      "Iteration No: 32 ended. Search finished for the next optimal point.\n",
      "Time taken: 1966.1700\n",
      "Function value obtained: 0.4355\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 33 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.7425 - acc: 0.8562 - val_loss: 0.5898 - val_acc: 0.4696\n",
      "9639/9639 [==============================] - 11s 1ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2978 - acc: 0.8659 - val_loss: 0.5724 - val_acc: 0.4739\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2882 - acc: 0.8671 - val_loss: 0.5784 - val_acc: 0.5630\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:170,FP:1,FN:134,TP:30,Macc:0.588539399916,F1:0.307689331573\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2803 - acc: 0.8679 - val_loss: 0.5396 - val_acc: 0.6199\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:169,FP:2,FN:121,TP:43,Macc:0.625249567179,F1:0.41147949336\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2781 - acc: 0.8691 - val_loss: 0.5438 - val_acc: 0.5901\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:169,FP:2,FN:127,TP:37,Macc:0.606956885479,F1:0.364528565636\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2739 - acc: 0.8698 - val_loss: 0.5999 - val_acc: 0.6507\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:168,FP:3,FN:112,TP:52,Macc:0.649764613308,F1:0.474881659551\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2708 - acc: 0.8709 - val_loss: 0.4822 - val_acc: 0.7250\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:149,FP:22,FN:80,TP:84,Macc:0.691770030393,F1:0.622216925367\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2690 - acc: 0.8703 - val_loss: 0.4698 - val_acc: 0.7349\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:143,FP:28,FN:72,TP:92,Macc:0.698616414138,F1:0.647881905817\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2656 - acc: 0.8726 - val_loss: 0.4736 - val_acc: 0.7314\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:140,FP:31,FN:72,TP:92,Macc:0.689844484878,F1:0.641109545724\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2652 - acc: 0.8712 - val_loss: 0.4578 - val_acc: 0.7379\n",
      "9639/9639 [==============================] - 2s 239us/step\n",
      "TN:144,FP:27,FN:71,TP:93,Macc:0.704589170842,F1:0.654924158792\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2631 - acc: 0.8728 - val_loss: 0.4778 - val_acc: 0.7263\n",
      "9639/9639 [==============================] - 2s 227us/step\n",
      "TN:143,FP:28,FN:71,TP:93,Macc:0.701665194422,F1:0.652626153815\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2625 - acc: 0.8738 - val_loss: 0.4527 - val_acc: 0.7502\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:141,FP:30,FN:66,TP:98,Macc:0.711061142998,F1:0.671227409783\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2599 - acc: 0.8741 - val_loss: 0.4528 - val_acc: 0.7324\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:144,FP:27,FN:71,TP:93,Macc:0.704589170842,F1:0.654924158792\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2587 - acc: 0.8755 - val_loss: 0.5300 - val_acc: 0.6909\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:114,FP:57,FN:59,TP:105,Macc:0.653455241639,F1:0.644166235923\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2590 - acc: 0.8748 - val_loss: 0.4478 - val_acc: 0.7547\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:149,FP:22,FN:75,TP:89,Macc:0.707013931809,F1:0.647267379825\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2564 - acc: 0.8755 - val_loss: 0.4574 - val_acc: 0.7527\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:151,FP:20,FN:72,TP:92,Macc:0.722008225499,F1:0.666661308801\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2557 - acc: 0.8768 - val_loss: 0.4495 - val_acc: 0.7729\n",
      "9639/9639 [==============================] - 2s 228us/step\n",
      "TN:127,FP:44,FN:36,TP:128,Macc:0.761588881616,F1:0.761899215176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2533 - acc: 0.8821 - val_loss: 0.4254 - val_acc: 0.8059\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:129,FP:42,FN:27,TP:137,Macc:0.794875857006,F1:0.798828278561\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2508 - acc: 0.8816 - val_loss: 0.4128 - val_acc: 0.8115\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:131,FP:40,FN:25,TP:139,Macc:0.806821370413,F1:0.810490085393\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2499 - acc: 0.8839 - val_loss: 0.4758 - val_acc: 0.7581\n",
      "9639/9639 [==============================] - 2s 241us/step\n",
      "TN:141,FP:30,FN:57,TP:107,Macc:0.738500165548,F1:0.710957947482\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2505 - acc: 0.8818 - val_loss: 0.4370 - val_acc: 0.7963\n",
      "9639/9639 [==============================] - 2s 229us/step\n",
      "TN:149,FP:22,FN:47,TP:117,Macc:0.792379779742,F1:0.77227170913\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2460 - acc: 0.8857 - val_loss: 0.4678 - val_acc: 0.7765\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:157,FP:14,FN:65,TP:99,Macc:0.760893546003,F1:0.714796073753\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2489 - acc: 0.8855 - val_loss: 0.4241 - val_acc: 0.7980\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:137,FP:34,FN:35,TP:129,Macc:0.7938774261,F1:0.788985272696\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2478 - acc: 0.8861 - val_loss: 0.4219 - val_acc: 0.8061\n",
      "9639/9639 [==============================] - 2s 231us/step\n",
      "TN:134,FP:37,FN:33,TP:131,Macc:0.791203057407,F1:0.789151075049\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2468 - acc: 0.8861 - val_loss: 0.5388 - val_acc: 0.7383\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:159,FP:12,FN:79,TP:85,Macc:0.724058574877,F1:0.651335803743\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2452 - acc: 0.8866 - val_loss: 0.4304 - val_acc: 0.8062\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:150,FP:21,FN:43,TP:121,Macc:0.807498877295,F1:0.790844144811\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2446 - acc: 0.8869 - val_loss: 0.4485 - val_acc: 0.7954\n",
      "9639/9639 [==============================] - 2s 230us/step\n",
      "TN:154,FP:17,FN:50,TP:114,Macc:0.797853320992,F1:0.772875867157\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2460 - acc: 0.8874 - val_loss: 0.4317 - val_acc: 0.8053\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:152,FP:19,FN:49,TP:115,Macc:0.795054148436,F1:0.771806579337\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2439 - acc: 0.8875 - val_loss: 0.4710 - val_acc: 0.7718\n",
      "9639/9639 [==============================] - 2s 228us/step\n",
      "TN:156,FP:15,FN:68,TP:96,Macc:0.748823228733,F1:0.698176466658\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2423 - acc: 0.8891 - val_loss: 0.3923 - val_acc: 0.8273\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:133,FP:38,FN:22,TP:142,Macc:0.821815664103,F1:0.825575854485\n",
      "Loss: 0.385946\n",
      "Iteration No: 33 ended. Search finished for the next optimal point.\n",
      "Time taken: 1931.4949\n",
      "Function value obtained: 0.3859\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 34 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.7792 - acc: 0.8323 - val_loss: 0.6940 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 11s 1ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3296 - acc: 0.8327 - val_loss: 0.8853 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3189 - acc: 0.8324 - val_loss: 0.6451 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3091 - acc: 0.8327 - val_loss: 0.6983 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3068 - acc: 0.8325 - val_loss: 1.0144 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2995 - acc: 0.8323 - val_loss: 0.5939 - val_acc: 0.6982\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:135,FP:36,FN:54,TP:110,Macc:0.730102647877,F1:0.709671887576\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2998 - acc: 0.8324 - val_loss: 0.6248 - val_acc: 0.6424\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:135,FP:36,FN:72,TP:92,Macc:0.675224602778,F1:0.630131522471\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2956 - acc: 0.8339 - val_loss: 0.5547 - val_acc: 0.7863\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:88,FP:83,FN:6,TP:158,Macc:0.739017209732,F1:0.780241570041\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2952 - acc: 0.8352 - val_loss: 0.6515 - val_acc: 0.6481\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:139,FP:32,FN:73,TP:91,Macc:0.683871728175,F1:0.634140905144\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2934 - acc: 0.8369 - val_loss: 0.5729 - val_acc: 0.7325\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:134,FP:37,FN:48,TP:116,Macc:0.745471353157,F1:0.73185565461\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2905 - acc: 0.8384 - val_loss: 0.5526 - val_acc: 0.7661\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:121,FP:50,FN:27,TP:137,Macc:0.771484045645,F1:0.780621255353\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2891 - acc: 0.8410 - val_loss: 0.5631 - val_acc: 0.7317\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:134,FP:37,FN:43,TP:121,Macc:0.760715254574,F1:0.751547245633\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2886 - acc: 0.8423 - val_loss: 0.5297 - val_acc: 0.7633\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:131,FP:40,FN:37,TP:127,Macc:0.770236007013,F1:0.767366050696\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2878 - acc: 0.8419 - val_loss: 0.5444 - val_acc: 0.7656\n",
      "9639/9639 [==============================] - 2s 236us/step\n",
      "TN:131,FP:40,FN:32,TP:132,Macc:0.78547990843,F1:0.785708737425\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2871 - acc: 0.8418 - val_loss: 0.5638 - val_acc: 0.7331\n",
      "9639/9639 [==============================] - 2s 233us/step\n",
      "TN:134,FP:37,FN:47,TP:117,Macc:0.74852013344,F1:0.735843511176\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2863 - acc: 0.8461 - val_loss: 0.6668 - val_acc: 0.6435\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:136,FP:35,FN:76,TP:88,Macc:0.665953458065,F1:0.613234983403\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2845 - acc: 0.8445 - val_loss: 0.5339 - val_acc: 0.7552\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:134,FP:37,FN:39,TP:125,Macc:0.772910375707,F1:0.766865614139\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2837 - acc: 0.8465 - val_loss: 0.6176 - val_acc: 0.6957\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:143,FP:28,FN:67,TP:97,Macc:0.713860315555,F1:0.67127482592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2836 - acc: 0.8435 - val_loss: 0.6898 - val_acc: 0.6194\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:140,FP:31,FN:84,TP:80,Macc:0.653259121478,F1:0.581812839612\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2823 - acc: 0.8445 - val_loss: 0.5385 - val_acc: 0.7642\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:134,FP:37,FN:35,TP:129,Macc:0.78510549684,F1:0.781812629938\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2834 - acc: 0.8426 - val_loss: 0.6006 - val_acc: 0.6975\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:134,FP:37,FN:53,TP:111,Macc:0.730227451741,F1:0.711532925872\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2824 - acc: 0.8446 - val_loss: 0.6112 - val_acc: 0.7100\n",
      "9639/9639 [==============================] - 2s 235us/step\n",
      "TN:142,FP:29,FN:59,TP:105,Macc:0.735326581401,F1:0.704692490335\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2814 - acc: 0.8445 - val_loss: 0.6102 - val_acc: 0.7097\n",
      "9639/9639 [==============================] - 2s 242us/step\n",
      "TN:139,FP:32,FN:55,TP:109,Macc:0.738749773274,F1:0.714748578123\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2808 - acc: 0.8446 - val_loss: 0.6836 - val_acc: 0.6525\n",
      "9639/9639 [==============================] - 2s 238us/step\n",
      "TN:140,FP:31,FN:74,TP:90,Macc:0.683746924312,F1:0.631573523862\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2812 - acc: 0.8436 - val_loss: 0.5875 - val_acc: 0.7229\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:134,FP:37,FN:51,TP:113,Macc:0.736325012307,F1:0.719739683477\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2800 - acc: 0.8460 - val_loss: 0.5542 - val_acc: 0.7536\n",
      "9639/9639 [==============================] - 2s 240us/step\n",
      "TN:136,FP:35,FN:43,TP:121,Macc:0.766563207414,F1:0.756244451485\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2802 - acc: 0.8455 - val_loss: 0.6808 - val_acc: 0.6845\n",
      "9639/9639 [==============================] - 2s 237us/step\n",
      "TN:143,FP:28,FN:68,TP:96,Macc:0.710811535272,F1:0.66666122188\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2773 - acc: 0.8455 - val_loss: 0.6715 - val_acc: 0.6749\n",
      "9639/9639 [==============================] - 2s 232us/step\n",
      "TN:146,FP:25,FN:79,TP:85,Macc:0.686046881416,F1:0.620432620057\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2767 - acc: 0.8466 - val_loss: 0.6661 - val_acc: 0.6771\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:142,FP:29,FN:69,TP:95,Macc:0.704838778568,F1:0.659716777967\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2793 - acc: 0.8454 - val_loss: 0.6329 - val_acc: 0.7040\n",
      "9639/9639 [==============================] - 2s 234us/step\n",
      "TN:144,FP:27,FN:64,TP:100,Macc:0.725930632825,F1:0.687279760367\n",
      "Loss: 0.626191\n",
      "Iteration No: 34 ended. Search finished for the next optimal point.\n",
      "Time taken: 1943.5141\n",
      "Function value obtained: 0.6262\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 35 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.7536 - acc: 0.8432 - val_loss: 0.5573 - val_acc: 0.8137\n",
      "9639/9639 [==============================] - 11s 1ms/step\n",
      "TN:104,FP:67,FN:7,TP:157,Macc:0.78275205217,F1:0.809272936187\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2980 - acc: 0.8634 - val_loss: 0.5051 - val_acc: 0.8129\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:127,FP:44,FN:26,TP:138,Macc:0.792076684449,F1:0.797682325475\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2814 - acc: 0.8729 - val_loss: 0.4418 - val_acc: 0.8314\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:118,FP:53,FN:9,TP:155,Macc:0.817590161485,F1:0.833327861031\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2725 - acc: 0.8789 - val_loss: 0.4294 - val_acc: 0.8354\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:121,FP:50,FN:16,TP:148,Macc:0.805020628762,F1:0.817674056872\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2676 - acc: 0.8820 - val_loss: 0.4350 - val_acc: 0.8281\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:116,FP:55,FN:11,TP:153,Macc:0.805644648078,F1:0.822575173495\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2607 - acc: 0.8845 - val_loss: 0.4577 - val_acc: 0.8036\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:107,FP:64,FN:7,TP:157,Macc:0.79152398143,F1:0.815578989572\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2608 - acc: 0.8853 - val_loss: 0.4287 - val_acc: 0.8315\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:128,FP:43,FN:19,TP:145,Macc:0.816342122853,F1:0.823858110477\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2569 - acc: 0.8868 - val_loss: 0.4135 - val_acc: 0.8360\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:128,FP:43,FN:15,TP:149,Macc:0.828537243986,F1:0.837073134015\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2554 - acc: 0.8860 - val_loss: 0.4212 - val_acc: 0.8309\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:123,FP:48,FN:17,TP:147,Macc:0.807819801319,F1:0.81893599504\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2541 - acc: 0.8869 - val_loss: 0.4003 - val_acc: 0.8491\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:129,FP:42,FN:16,TP:148,Macc:0.828412440123,F1:0.836152669831\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2511 - acc: 0.8892 - val_loss: 0.4157 - val_acc: 0.8399\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:134,FP:37,FN:21,TP:143,Macc:0.827788420807,F1:0.831389807601\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2510 - acc: 0.8893 - val_loss: 0.4473 - val_acc: 0.8347\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:136,FP:35,FN:25,TP:139,Macc:0.821441252514,F1:0.822479658417\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2505 - acc: 0.8887 - val_loss: 0.3956 - val_acc: 0.8429\n",
      "9639/9639 [==============================] - 3s 277us/step\n",
      "TN:126,FP:45,FN:14,TP:150,Macc:0.825738071429,F1:0.835649085937\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2483 - acc: 0.8892 - val_loss: 0.3959 - val_acc: 0.8481\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:134,FP:37,FN:18,TP:146,Macc:0.836934761657,F1:0.841493022252\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2494 - acc: 0.8886 - val_loss: 0.3871 - val_acc: 0.8489\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:129,FP:42,FN:12,TP:152,Macc:0.840607561256,F1:0.849156497648\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2457 - acc: 0.8899 - val_loss: 0.3861 - val_acc: 0.8465\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:134,FP:37,FN:19,TP:145,Macc:0.833885981373,F1:0.838144750646\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2438 - acc: 0.8918 - val_loss: 0.3832 - val_acc: 0.8460\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:136,FP:35,FN:18,TP:146,Macc:0.842782714497,F1:0.846371271012\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2441 - acc: 0.8914 - val_loss: 0.3921 - val_acc: 0.8468\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:135,FP:36,FN:20,TP:144,Macc:0.83376117751,F1:0.837203760717\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2427 - acc: 0.8919 - val_loss: 0.3927 - val_acc: 0.8488\n",
      "9639/9639 [==============================] - 3s 271us/step\n",
      "TN:134,FP:37,FN:19,TP:145,Macc:0.833885981373,F1:0.838144750646\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2430 - acc: 0.8920 - val_loss: 0.3823 - val_acc: 0.8542\n",
      "9639/9639 [==============================] - 2s 257us/step\n",
      "TN:134,FP:37,FN:18,TP:146,Macc:0.836934761657,F1:0.841493022252\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2425 - acc: 0.8922 - val_loss: 0.3842 - val_acc: 0.8456\n",
      "9639/9639 [==============================] - 3s 274us/step\n",
      "TN:125,FP:46,FN:9,TP:155,Macc:0.838057996425,F1:0.849309573854\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2396 - acc: 0.8926 - val_loss: 0.3919 - val_acc: 0.8459\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:133,FP:38,FN:21,TP:143,Macc:0.824864444386,F1:0.828979967774\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2404 - acc: 0.8924 - val_loss: 0.3754 - val_acc: 0.8536\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:129,FP:42,FN:10,TP:154,Macc:0.846705121823,F1:0.855550046763\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2393 - acc: 0.8933 - val_loss: 0.3846 - val_acc: 0.8465\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:137,FP:34,FN:22,TP:142,Macc:0.833511569784,F1:0.835288570486\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2390 - acc: 0.8922 - val_loss: 0.4061 - val_acc: 0.8424\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:137,FP:34,FN:21,TP:143,Macc:0.836560350067,F1:0.838704131339\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2411 - acc: 0.8916 - val_loss: 0.4171 - val_acc: 0.8325\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:139,FP:32,FN:26,TP:138,Macc:0.82716440149,F1:0.826341752771\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2380 - acc: 0.8932 - val_loss: 0.4250 - val_acc: 0.8318\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:138,FP:33,FN:26,TP:138,Macc:0.82424042507,F1:0.823875045347\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2383 - acc: 0.8939 - val_loss: 0.4396 - val_acc: 0.8199\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:140,FP:31,FN:30,TP:134,Macc:0.817893256777,F1:0.81458411127\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2377 - acc: 0.8939 - val_loss: 0.3873 - val_acc: 0.8535\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:134,FP:37,FN:19,TP:145,Macc:0.833885981373,F1:0.838144750646\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2369 - acc: 0.8942 - val_loss: 0.4270 - val_acc: 0.8345\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:142,FP:29,FN:26,TP:138,Macc:0.835936330751,F1:0.833831303073\n",
      "Loss: 0.420902\n",
      "Iteration No: 35 ended. Search finished for the next optimal point.\n",
      "Time taken: 1993.4001\n",
      "Function value obtained: 0.4209\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 36 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.7606 - acc: 0.8456 - val_loss: 0.6385 - val_acc: 0.6795\n",
      "9639/9639 [==============================] - 12s 1ms/step\n",
      "TN:67,FP:104,FN:10,TP:154,Macc:0.665418583777,F1:0.729852554787\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3015 - acc: 0.8662 - val_loss: 0.5681 - val_acc: 0.7181\n",
      "9639/9639 [==============================] - 3s 282us/step\n",
      "TN:71,FP:100,FN:2,TP:162,Macc:0.701504731724,F1:0.760558132108\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2892 - acc: 0.8718 - val_loss: 0.5916 - val_acc: 0.6901\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:68,FP:103,FN:8,TP:156,Macc:0.674440120763,F1:0.737583391572\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2779 - acc: 0.8753 - val_loss: 0.6283 - val_acc: 0.6487\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:48,FP:123,FN:4,TP:160,Macc:0.628155713495,F1:0.715878523506\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2721 - acc: 0.8789 - val_loss: 0.6447 - val_acc: 0.6395\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:38,FP:133,FN:3,TP:161,Macc:0.601964729577,F1:0.703051677941\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2676 - acc: 0.8803 - val_loss: 0.6402 - val_acc: 0.6388\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:43,FP:128,FN:1,TP:163,Macc:0.622682172244,F1:0.716478410373\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2657 - acc: 0.8845 - val_loss: 0.6324 - val_acc: 0.6533\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:48,FP:123,FN:1,TP:163,Macc:0.637302054345,F1:0.72443931332\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2641 - acc: 0.8824 - val_loss: 0.6858 - val_acc: 0.6074\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:26,FP:145,FN:2,TP:162,Macc:0.569925792819,F1:0.68789306406\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2615 - acc: 0.8824 - val_loss: 0.5757 - val_acc: 0.6991\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:65,FP:106,FN:3,TP:161,Macc:0.68091209292,F1:0.747094543993\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2577 - acc: 0.8847 - val_loss: 0.5831 - val_acc: 0.6907\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:54,FP:117,FN:4,TP:160,Macc:0.645699572015,F1:0.725618407716\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2593 - acc: 0.8846 - val_loss: 0.5527 - val_acc: 0.7096\n",
      "9639/9639 [==============================] - 3s 269us/step\n",
      "TN:68,FP:103,FN:4,TP:160,Macc:0.686635241897,F1:0.749409276981\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2583 - acc: 0.8848 - val_loss: 0.6078 - val_acc: 0.6649\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:45,FP:126,FN:2,TP:162,Macc:0.625481344801,F1:0.716809038371\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2562 - acc: 0.8862 - val_loss: 0.5170 - val_acc: 0.7480\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:83,FP:88,FN:6,TP:158,Macc:0.724397327631,F1:0.770726385997\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2556 - acc: 0.8866 - val_loss: 0.4868 - val_acc: 0.7660\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:87,FP:84,FN:5,TP:159,Macc:0.739142013595,F1:0.781321446347\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2557 - acc: 0.8852 - val_loss: 0.4583 - val_acc: 0.7802\n",
      "9639/9639 [==============================] - 3s 274us/step\n",
      "TN:96,FP:75,FN:6,TP:158,Macc:0.762409021093,F1:0.795964395368\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2526 - acc: 0.8877 - val_loss: 0.5238 - val_acc: 0.7408\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:78,FP:93,FN:4,TP:160,Macc:0.715875006098,F1:0.767380801214\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2528 - acc: 0.8861 - val_loss: 0.4618 - val_acc: 0.7813\n",
      "9639/9639 [==============================] - 3s 283us/step\n",
      "TN:93,FP:78,FN:6,TP:158,Macc:0.753637091832,F1:0.789994634786\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2515 - acc: 0.8877 - val_loss: 0.4566 - val_acc: 0.7801\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:95,FP:76,FN:6,TP:158,Macc:0.759485044673,F1:0.793964475529\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2507 - acc: 0.8876 - val_loss: 0.5097 - val_acc: 0.7359\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:76,FP:95,FN:6,TP:158,Macc:0.703929492691,F1:0.757788475581\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2498 - acc: 0.8866 - val_loss: 0.4580 - val_acc: 0.7769\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:91,FP:80,FN:6,TP:158,Macc:0.747789138992,F1:0.786064295128\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2480 - acc: 0.8880 - val_loss: 0.4703 - val_acc: 0.7667\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:88,FP:83,FN:6,TP:158,Macc:0.739017209732,F1:0.780241570041\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2486 - acc: 0.8885 - val_loss: 0.4515 - val_acc: 0.7778\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:94,FP:77,FN:9,TP:155,Macc:0.747414727403,F1:0.782822901552\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2475 - acc: 0.8893 - val_loss: 0.4185 - val_acc: 0.8112\n",
      "9639/9639 [==============================] - 3s 271us/step\n",
      "TN:106,FP:65,FN:9,TP:155,Macc:0.782502444444,F1:0.807286237422\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2484 - acc: 0.8899 - val_loss: 0.4083 - val_acc: 0.8179\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:109,FP:62,FN:6,TP:158,Macc:0.800420714554,F1:0.822911236526\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2478 - acc: 0.8896 - val_loss: 0.4461 - val_acc: 0.7833\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:94,FP:77,FN:6,TP:158,Macc:0.756561068252,F1:0.791974580396\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2459 - acc: 0.8893 - val_loss: 0.4699 - val_acc: 0.7713\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:94,FP:77,FN:6,TP:158,Macc:0.756561068252,F1:0.791974580396\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2463 - acc: 0.8885 - val_loss: 0.4661 - val_acc: 0.7638\n",
      "9639/9639 [==============================] - 3s 283us/step\n",
      "TN:87,FP:84,FN:7,TP:157,Macc:0.733044453028,F1:0.775303298704\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2474 - acc: 0.8890 - val_loss: 0.4743 - val_acc: 0.7631\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:90,FP:81,FN:6,TP:158,Macc:0.744865162572,F1:0.784113754421\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2465 - acc: 0.8895 - val_loss: 0.5042 - val_acc: 0.7401\n",
      "9639/9639 [==============================] - 3s 277us/step\n",
      "TN:77,FP:94,FN:5,TP:159,Macc:0.709902249394,F1:0.762584638397\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2437 - acc: 0.8901 - val_loss: 0.4605 - val_acc: 0.7737\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:87,FP:84,FN:5,TP:159,Macc:0.739142013595,F1:0.781321446347\n",
      "Loss: 0.453795\n",
      "Iteration No: 36 ended. Search finished for the next optimal point.\n",
      "Time taken: 2020.2926\n",
      "Function value obtained: 0.4538\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 37 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.7591 - acc: 0.8423 - val_loss: 0.5918 - val_acc: 0.7721\n",
      "9639/9639 [==============================] - 12s 1ms/step\n",
      "TN:96,FP:75,FN:12,TP:152,Macc:0.744116339393,F1:0.777488205216\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.3070 - acc: 0.8580 - val_loss: 0.5966 - val_acc: 0.7074\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:68,FP:103,FN:3,TP:161,Macc:0.68968402218,F1:0.752331210207\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2918 - acc: 0.8682 - val_loss: 0.4938 - val_acc: 0.8009\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:106,FP:65,FN:8,TP:156,Macc:0.785551224727,F1:0.810384184674\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2851 - acc: 0.8695 - val_loss: 0.5137 - val_acc: 0.7757\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:92,FP:79,FN:4,TP:160,Macc:0.756810675979,F1:0.794039312192\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2774 - acc: 0.8751 - val_loss: 0.5683 - val_acc: 0.7145\n",
      "9639/9639 [==============================] - 3s 273us/step\n",
      "TN:64,FP:107,FN:0,TP:164,Macc:0.68713445735,F1:0.754017783182\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2725 - acc: 0.8774 - val_loss: 0.4769 - val_acc: 0.7944\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:100,FP:71,FN:5,TP:159,Macc:0.777153707056,F1:0.807101208286\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2693 - acc: 0.8795 - val_loss: 0.5163 - val_acc: 0.7563\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:82,FP:89,FN:2,TP:162,Macc:0.733668472345,F1:0.78071759193\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2667 - acc: 0.8802 - val_loss: 0.5183 - val_acc: 0.7536\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:84,FP:87,FN:1,TP:163,Macc:0.742565205468,F1:0.787434309051\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2660 - acc: 0.8815 - val_loss: 0.5283 - val_acc: 0.7444\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:82,FP:89,FN:4,TP:160,Macc:0.727570911778,F1:0.774813093669\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2634 - acc: 0.8819 - val_loss: 0.4663 - val_acc: 0.7948\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:101,FP:70,FN:4,TP:160,Macc:0.78312646376,F1:0.812177350134\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2612 - acc: 0.8829 - val_loss: 0.5606 - val_acc: 0.7170\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:68,FP:103,FN:1,TP:163,Macc:0.695781582747,F1:0.758134305606\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2574 - acc: 0.8860 - val_loss: 0.5319 - val_acc: 0.7377\n",
      "9639/9639 [==============================] - 3s 269us/step\n",
      "TN:76,FP:95,FN:1,TP:163,Macc:0.719173394107,F1:0.772506580997\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2558 - acc: 0.8857 - val_loss: 0.5413 - val_acc: 0.7314\n",
      "9639/9639 [==============================] - 3s 269us/step\n",
      "TN:77,FP:94,FN:2,TP:162,Macc:0.719048590244,F1:0.771423294956\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2555 - acc: 0.8856 - val_loss: 0.5753 - val_acc: 0.7024\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:65,FP:106,FN:1,TP:163,Macc:0.687009653486,F1:0.752881621229\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2547 - acc: 0.8861 - val_loss: 0.5734 - val_acc: 0.7032\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:67,FP:104,FN:2,TP:162,Macc:0.689808826043,F1:0.753483143054\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2534 - acc: 0.8867 - val_loss: 0.5569 - val_acc: 0.7168\n",
      "9639/9639 [==============================] - 3s 273us/step\n",
      "TN:68,FP:103,FN:0,TP:164,Macc:0.69883036303,F1:0.761015656971\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2519 - acc: 0.8879 - val_loss: 0.5196 - val_acc: 0.7505\n",
      "9639/9639 [==============================] - 3s 269us/step\n",
      "TN:84,FP:87,FN:1,TP:163,Macc:0.742565205468,F1:0.787434309051\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2522 - acc: 0.8875 - val_loss: 0.5435 - val_acc: 0.7288\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:73,FP:98,FN:1,TP:163,Macc:0.710401464847,F1:0.767053570361\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2502 - acc: 0.8881 - val_loss: 0.4837 - val_acc: 0.7801\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:96,FP:75,FN:1,TP:163,Macc:0.777652922509,F1:0.810939915656\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2486 - acc: 0.8888 - val_loss: 0.5158 - val_acc: 0.7517\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:83,FP:88,FN:1,TP:163,Macc:0.739641229048,F1:0.785536868782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2475 - acc: 0.8885 - val_loss: 0.4780 - val_acc: 0.7841\n",
      "9639/9639 [==============================] - 3s 269us/step\n",
      "TN:97,FP:74,FN:1,TP:163,Macc:0.780576898929,F1:0.812962218752\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2477 - acc: 0.8901 - val_loss: 0.4912 - val_acc: 0.7792\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:98,FP:73,FN:4,TP:160,Macc:0.774354534499,F1:0.80603996156\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2476 - acc: 0.8889 - val_loss: 0.4929 - val_acc: 0.7719\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:91,FP:80,FN:3,TP:161,Macc:0.756935479842,F1:0.79505638405\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2458 - acc: 0.8906 - val_loss: 0.4736 - val_acc: 0.7879\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:98,FP:73,FN:1,TP:163,Macc:0.783500875349,F1:0.81499463341\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2457 - acc: 0.8899 - val_loss: 0.4900 - val_acc: 0.7772\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:97,FP:74,FN:0,TP:164,Macc:0.783625679213,F1:0.815915039761\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2456 - acc: 0.8906 - val_loss: 0.5012 - val_acc: 0.7698\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:93,FP:78,FN:2,TP:162,Macc:0.765832212966,F1:0.80197484902\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2453 - acc: 0.8898 - val_loss: 0.4910 - val_acc: 0.7790\n",
      "9639/9639 [==============================] - 3s 270us/step\n",
      "TN:97,FP:74,FN:1,TP:163,Macc:0.780576898929,F1:0.812962218752\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2427 - acc: 0.8921 - val_loss: 0.4768 - val_acc: 0.7905\n",
      "9639/9639 [==============================] - 3s 271us/step\n",
      "TN:100,FP:71,FN:1,TP:163,Macc:0.78934882819,F1:0.81909010228\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2446 - acc: 0.8901 - val_loss: 0.4996 - val_acc: 0.7755\n",
      "9639/9639 [==============================] - 3s 271us/step\n",
      "TN:100,FP:71,FN:4,TP:160,Macc:0.78020248734,F1:0.810121195433\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2450 - acc: 0.8908 - val_loss: 0.5087 - val_acc: 0.7676\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:90,FP:81,FN:2,TP:162,Macc:0.757060283705,F1:0.796063460291\n",
      "Loss: 0.501623\n",
      "Iteration No: 37 ended. Search finished for the next optimal point.\n",
      "Time taken: 2056.6023\n",
      "Function value obtained: 0.5016\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 38 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.7680 - acc: 0.8373 - val_loss: 0.6243 - val_acc: 0.7401\n",
      "9639/9639 [==============================] - 12s 1ms/step\n",
      "TN:75,FP:96,FN:0,TP:164,Macc:0.719298197971,F1:0.773579647508\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3126 - acc: 0.8542 - val_loss: 0.5489 - val_acc: 0.7799\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:97,FP:74,FN:2,TP:162,Macc:0.777528118646,F1:0.809994633685\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2963 - acc: 0.8578 - val_loss: 0.5956 - val_acc: 0.7186\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:75,FP:96,FN:11,TP:153,Macc:0.685761614854,F1:0.740914790392\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2886 - acc: 0.8664 - val_loss: 0.5904 - val_acc: 0.7132\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:73,FP:98,FN:5,TP:159,Macc:0.698206343714,F1:0.755339147006\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2823 - acc: 0.8733 - val_loss: 0.5858 - val_acc: 0.7076\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:71,FP:100,FN:3,TP:161,Macc:0.69845595144,F1:0.757641806142\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2755 - acc: 0.8759 - val_loss: 0.5543 - val_acc: 0.7313\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:80,FP:91,FN:6,TP:158,Macc:0.715625398371,F1:0.765127864162\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2718 - acc: 0.8777 - val_loss: 0.6099 - val_acc: 0.6715\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:58,FP:113,FN:5,TP:159,Macc:0.654346697412,F1:0.729352598871\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2702 - acc: 0.8800 - val_loss: 0.4898 - val_acc: 0.7847\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:100,FP:71,FN:5,TP:159,Macc:0.777153707056,F1:0.807101208286\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2675 - acc: 0.8817 - val_loss: 0.5257 - val_acc: 0.7509\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:81,FP:90,FN:1,TP:163,Macc:0.733793276208,F1:0.781769289663\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2652 - acc: 0.8814 - val_loss: 0.5492 - val_acc: 0.7251\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:74,FP:97,FN:3,TP:161,Macc:0.707227880701,F1:0.763027908506\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2639 - acc: 0.8819 - val_loss: 0.5593 - val_acc: 0.7164\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:73,FP:98,FN:4,TP:160,Macc:0.701255123997,F1:0.75828857226\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2619 - acc: 0.8820 - val_loss: 0.6338 - val_acc: 0.6446\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:37,FP:134,FN:2,TP:162,Macc:0.60208953344,F1:0.704342745329\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2605 - acc: 0.8830 - val_loss: 0.5566 - val_acc: 0.7174\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:69,FP:102,FN:3,TP:161,Macc:0.6926079986,F1:0.754093117488\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2590 - acc: 0.8830 - val_loss: 0.4902 - val_acc: 0.7763\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:91,FP:80,FN:5,TP:159,Macc:0.750837919276,F1:0.789076533307\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2621 - acc: 0.8825 - val_loss: 0.5496 - val_acc: 0.7288\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:74,FP:97,FN:4,TP:160,Macc:0.704179100417,F1:0.760089740581\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2587 - acc: 0.8841 - val_loss: 0.5356 - val_acc: 0.7362\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:75,FP:96,FN:3,TP:161,Macc:0.710151857121,F1:0.764840334157\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2562 - acc: 0.8850 - val_loss: 0.4946 - val_acc: 0.7694\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:88,FP:83,FN:3,TP:161,Macc:0.748163550582,F1:0.789210355211\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2544 - acc: 0.8856 - val_loss: 0.5453 - val_acc: 0.7256\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:71,FP:100,FN:2,TP:162,Macc:0.701504731724,F1:0.760558132108\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2536 - acc: 0.8859 - val_loss: 0.5330 - val_acc: 0.7387\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:78,FP:93,FN:4,TP:160,Macc:0.715875006098,F1:0.767380801214\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2536 - acc: 0.8868 - val_loss: 0.5084 - val_acc: 0.7580\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:84,FP:87,FN:5,TP:159,Macc:0.730370084335,F1:0.775604434515\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2531 - acc: 0.8864 - val_loss: 0.5346 - val_acc: 0.7359\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:74,FP:97,FN:3,TP:161,Macc:0.707227880701,F1:0.763027908506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2514 - acc: 0.8872 - val_loss: 0.4985 - val_acc: 0.7638\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:86,FP:85,FN:3,TP:161,Macc:0.742315597742,F1:0.785360531552\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2508 - acc: 0.8875 - val_loss: 0.5122 - val_acc: 0.7540\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:83,FP:88,FN:3,TP:161,Macc:0.733543668481,F1:0.779655708423\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 45s 38ms/step - loss: 0.2523 - acc: 0.8863 - val_loss: 0.5560 - val_acc: 0.7166\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:69,FP:102,FN:3,TP:161,Macc:0.6926079986,F1:0.754093117488\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2503 - acc: 0.8867 - val_loss: 0.4769 - val_acc: 0.7808\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:94,FP:77,FN:4,TP:160,Macc:0.762658628819,F1:0.797999626058\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2505 - acc: 0.8877 - val_loss: 0.4965 - val_acc: 0.7630\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:82,FP:89,FN:2,TP:162,Macc:0.733668472345,F1:0.78071759193\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2489 - acc: 0.8884 - val_loss: 0.4613 - val_acc: 0.7897\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:103,FP:68,FN:5,TP:159,Macc:0.785925636317,F1:0.813293829798\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2487 - acc: 0.8876 - val_loss: 0.5120 - val_acc: 0.7525\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:83,FP:88,FN:4,TP:160,Macc:0.730494888198,F1:0.776693716314\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2497 - acc: 0.8877 - val_loss: 0.4614 - val_acc: 0.7895\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:103,FP:68,FN:4,TP:160,Macc:0.7889744166,F1:0.816321131439\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2485 - acc: 0.8891 - val_loss: 0.4785 - val_acc: 0.7761\n",
      "9639/9639 [==============================] - 2s 243us/step\n",
      "TN:91,FP:80,FN:3,TP:161,Macc:0.756935479842,F1:0.79505638405\n",
      "Loss: 0.472000\n",
      "Iteration No: 38 ended. Search finished for the next optimal point.\n",
      "Time taken: 2006.1562\n",
      "Function value obtained: 0.4720\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 39 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.7674 - acc: 0.8380 - val_loss: 0.6252 - val_acc: 0.6731\n",
      "9639/9639 [==============================] - 12s 1ms/step\n",
      "TN:149,FP:22,FN:91,TP:73,Macc:0.658233447276,F1:0.563701406227\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3105 - acc: 0.8579 - val_loss: 0.5437 - val_acc: 0.7722\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:114,FP:57,FN:25,TP:139,Macc:0.757113771271,F1:0.772216718526\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2956 - acc: 0.8644 - val_loss: 0.5036 - val_acc: 0.7812\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:132,FP:39,FN:43,TP:121,Macc:0.754867301733,F1:0.746908030409\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2876 - acc: 0.8709 - val_loss: 0.4753 - val_acc: 0.8137\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:122,FP:49,FN:18,TP:146,Macc:0.801847044615,F1:0.813364964741\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2818 - acc: 0.8746 - val_loss: 0.5080 - val_acc: 0.7837\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:106,FP:65,FN:9,TP:155,Macc:0.782502444444,F1:0.807286237422\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2744 - acc: 0.8764 - val_loss: 0.4549 - val_acc: 0.8202\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:113,FP:58,FN:6,TP:158,Macc:0.812116620234,F1:0.831573502251\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2726 - acc: 0.8788 - val_loss: 0.5291 - val_acc: 0.7514\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:90,FP:81,FN:10,TP:154,Macc:0.732670041439,F1:0.77192445619\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2678 - acc: 0.8814 - val_loss: 0.4987 - val_acc: 0.7767\n",
      "9639/9639 [==============================] - 2s 245us/step\n",
      "TN:95,FP:76,FN:8,TP:156,Macc:0.753387484106,F1:0.787873406321\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2632 - acc: 0.8825 - val_loss: 0.4851 - val_acc: 0.7879\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:105,FP:66,FN:13,TP:151,Macc:0.76738334689,F1:0.792645479332\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2613 - acc: 0.8834 - val_loss: 0.4880 - val_acc: 0.7804\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:97,FP:74,FN:6,TP:158,Macc:0.765332997513,F1:0.797974415861\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2593 - acc: 0.8839 - val_loss: 0.4475 - val_acc: 0.8205\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:118,FP:53,FN:15,TP:149,Macc:0.799297479785,F1:0.814202160656\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2569 - acc: 0.8858 - val_loss: 0.4641 - val_acc: 0.8166\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:118,FP:53,FN:14,TP:150,Macc:0.802346260068,F1:0.817433205242\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2573 - acc: 0.8850 - val_loss: 0.5163 - val_acc: 0.7598\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:94,FP:77,FN:10,TP:154,Macc:0.744365947119,F1:0.779741450291\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2527 - acc: 0.8873 - val_loss: 0.4566 - val_acc: 0.8138\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:114,FP:57,FN:9,TP:155,Macc:0.805894255804,F1:0.824462626535\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2542 - acc: 0.8857 - val_loss: 0.5566 - val_acc: 0.7113\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:67,FP:104,FN:2,TP:162,Macc:0.689808826043,F1:0.753483143054\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2524 - acc: 0.8852 - val_loss: 0.4546 - val_acc: 0.8088\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:108,FP:63,FN:9,TP:155,Macc:0.788350397284,F1:0.811512887813\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2521 - acc: 0.8868 - val_loss: 0.4456 - val_acc: 0.8265\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:122,FP:49,FN:16,TP:148,Macc:0.807944605182,F1:0.819939094365\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2503 - acc: 0.8875 - val_loss: 0.4669 - val_acc: 0.8043\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:115,FP:56,FN:12,TP:152,Macc:0.799671891375,F1:0.817198829727\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2496 - acc: 0.8884 - val_loss: 0.4545 - val_acc: 0.8082\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:110,FP:61,FN:9,TP:155,Macc:0.794198350124,F1:0.815784029482\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2491 - acc: 0.8868 - val_loss: 0.4346 - val_acc: 0.8329\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:123,FP:48,FN:16,TP:148,Macc:0.810868581602,F1:0.822216715468\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2466 - acc: 0.8896 - val_loss: 0.4445 - val_acc: 0.8243\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:123,FP:48,FN:15,TP:149,Macc:0.813917361885,F1:0.825479260232\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2483 - acc: 0.8881 - val_loss: 0.4412 - val_acc: 0.8252\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:121,FP:50,FN:12,TP:152,Macc:0.817215749895,F1:0.830595602293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2468 - acc: 0.8880 - val_loss: 0.4362 - val_acc: 0.8315\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:126,FP:45,FN:15,TP:149,Macc:0.822689291146,F1:0.832396722142\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2441 - acc: 0.8911 - val_loss: 0.4309 - val_acc: 0.8370\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:126,FP:45,FN:14,TP:150,Macc:0.825738071429,F1:0.835649085937\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2465 - acc: 0.8897 - val_loss: 0.4353 - val_acc: 0.8309\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:122,FP:49,FN:14,TP:150,Macc:0.814042165749,F1:0.826440782071\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2459 - acc: 0.8899 - val_loss: 0.4356 - val_acc: 0.8230\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:120,FP:51,FN:15,TP:149,Macc:0.805145432625,F1:0.818675823035\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2439 - acc: 0.8898 - val_loss: 0.4115 - val_acc: 0.8371\n",
      "9639/9639 [==============================] - 2s 244us/step\n",
      "TN:123,FP:48,FN:13,TP:151,Macc:0.820014922452,F1:0.83195042361\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2438 - acc: 0.8904 - val_loss: 0.4230 - val_acc: 0.8331\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:122,FP:49,FN:12,TP:152,Macc:0.820139726315,F1:0.832871218681\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2442 - acc: 0.8907 - val_loss: 0.4364 - val_acc: 0.8167\n",
      "9639/9639 [==============================] - 2s 246us/step\n",
      "TN:115,FP:56,FN:9,TP:155,Macc:0.808818232225,F1:0.826661204601\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2432 - acc: 0.8904 - val_loss: 0.4155 - val_acc: 0.8369\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:126,FP:45,FN:17,TP:147,Macc:0.816591730579,F1:0.825837179654\n",
      "Loss: 0.409546\n",
      "Iteration No: 39 ended. Search finished for the next optimal point.\n",
      "Time taken: 2038.7565\n",
      "Function value obtained: 0.4095\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 40 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.7585 - acc: 0.8454 - val_loss: 0.6324 - val_acc: 0.7093\n",
      "9639/9639 [==============================] - 13s 1ms/step\n",
      "TN:78,FP:93,FN:11,TP:153,Macc:0.694533544114,F1:0.746336143404\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3031 - acc: 0.8597 - val_loss: 0.4928 - val_acc: 0.8188\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:107,FP:64,FN:4,TP:160,Macc:0.80067032228,F1:0.824736852835\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2887 - acc: 0.8687 - val_loss: 0.5081 - val_acc: 0.8030\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:119,FP:52,FN:19,TP:145,Macc:0.790026335072,F1:0.803318596764\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2816 - acc: 0.8733 - val_loss: 0.4716 - val_acc: 0.8074\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:107,FP:64,FN:3,TP:161,Macc:0.803719102564,F1:0.82775808461\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2727 - acc: 0.8788 - val_loss: 0.4472 - val_acc: 0.8252\n",
      "9639/9639 [==============================] - 3s 278us/step\n",
      "TN:120,FP:51,FN:14,TP:150,Macc:0.808194212908,F1:0.821912315232\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2645 - acc: 0.8821 - val_loss: 0.4283 - val_acc: 0.8295\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:122,FP:49,FN:12,TP:152,Macc:0.820139726315,F1:0.832871218681\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2638 - acc: 0.8830 - val_loss: 0.4321 - val_acc: 0.8312\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:121,FP:50,FN:13,TP:151,Macc:0.814166969612,F1:0.827391766956\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2607 - acc: 0.8856 - val_loss: 0.4381 - val_acc: 0.8214\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:110,FP:61,FN:5,TP:159,Macc:0.806393471257,F1:0.828119569561\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2581 - acc: 0.8842 - val_loss: 0.4738 - val_acc: 0.8022\n",
      "9639/9639 [==============================] - 3s 282us/step\n",
      "TN:130,FP:41,FN:31,TP:133,Macc:0.785604712293,F1:0.78697670215\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2553 - acc: 0.8874 - val_loss: 0.4121 - val_acc: 0.8398\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:121,FP:50,FN:9,TP:155,Macc:0.826362090745,F1:0.840102918933\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2521 - acc: 0.8876 - val_loss: 0.4620 - val_acc: 0.8107\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:133,FP:38,FN:27,TP:137,Macc:0.806571762687,F1:0.808254040396\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2523 - acc: 0.8876 - val_loss: 0.4476 - val_acc: 0.8096\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:130,FP:41,FN:27,TP:137,Macc:0.797799833426,F1:0.80116404836\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2522 - acc: 0.8880 - val_loss: 0.4600 - val_acc: 0.7984\n",
      "9639/9639 [==============================] - 3s 277us/step\n",
      "TN:132,FP:39,FN:26,TP:138,Macc:0.80669656655,F1:0.809378620035\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2509 - acc: 0.8898 - val_loss: 0.5093 - val_acc: 0.7741\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:137,FP:34,FN:36,TP:128,Macc:0.790828645817,F1:0.785270520871\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2515 - acc: 0.8864 - val_loss: 0.5868 - val_acc: 0.7307\n",
      "9639/9639 [==============================] - 3s 286us/step\n",
      "TN:141,FP:30,FN:56,TP:108,Macc:0.741548945831,F1:0.715226276784\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2481 - acc: 0.8900 - val_loss: 0.5642 - val_acc: 0.7558\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:142,FP:29,FN:44,TP:120,Macc:0.781058285651,F1:0.766767621716\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2471 - acc: 0.8893 - val_loss: 0.4989 - val_acc: 0.7762\n",
      "9639/9639 [==============================] - 3s 283us/step\n",
      "TN:137,FP:34,FN:37,TP:127,Macc:0.787779865534,F1:0.781532909142\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2461 - acc: 0.8910 - val_loss: 0.4690 - val_acc: 0.7883\n",
      "9639/9639 [==============================] - ETA:  - 3s 279us/step\n",
      "TN:136,FP:35,FN:30,TP:134,Macc:0.806197351097,F1:0.804799252912\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2454 - acc: 0.8921 - val_loss: 0.4939 - val_acc: 0.7816\n",
      "9639/9639 [==============================] - 3s 286us/step\n",
      "TN:140,FP:31,FN:38,TP:126,Macc:0.793503014511,F1:0.785041177822\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2441 - acc: 0.8904 - val_loss: 0.6497 - val_acc: 0.7085\n",
      "9639/9639 [==============================] - 3s 273us/step\n",
      "TN:146,FP:25,FN:68,TP:96,Macc:0.719583464532,F1:0.673678783767\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2439 - acc: 0.8912 - val_loss: 0.6989 - val_acc: 0.6771\n",
      "9639/9639 [==============================] - 3s 277us/step\n",
      "TN:146,FP:25,FN:80,TP:84,Macc:0.682998101132,F1:0.615379289074\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2435 - acc: 0.8905 - val_loss: 0.5521 - val_acc: 0.7457\n",
      "9639/9639 [==============================] - 3s 275us/step\n",
      "TN:143,FP:28,FN:50,TP:114,Macc:0.765689580371,F1:0.745092514116\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2435 - acc: 0.8918 - val_loss: 0.5596 - val_acc: 0.7353\n",
      "9639/9639 [==============================] - 3s 277us/step\n",
      "TN:143,FP:28,FN:60,TP:104,Macc:0.735201777538,F1:0.702697214797\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2431 - acc: 0.8914 - val_loss: 0.4315 - val_acc: 0.8146\n",
      "9639/9639 [==============================] - 3s 275us/step\n",
      "TN:130,FP:41,FN:21,TP:143,Macc:0.816092515126,F1:0.821833546707\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2420 - acc: 0.8925 - val_loss: 0.5109 - val_acc: 0.7722\n",
      "9639/9639 [==============================] - 3s 277us/step\n",
      "TN:139,FP:32,FN:36,TP:128,Macc:0.796676598657,F1:0.790117904016\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2409 - acc: 0.8928 - val_loss: 0.6297 - val_acc: 0.7054\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:145,FP:26,FN:67,TP:97,Macc:0.719708268395,F1:0.675952748626\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2416 - acc: 0.8922 - val_loss: 0.5624 - val_acc: 0.7386\n",
      "9639/9639 [==============================] - 3s 278us/step\n",
      "TN:143,FP:28,FN:55,TP:109,Macc:0.750445678955,F1:0.724246983055\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2394 - acc: 0.8938 - val_loss: 0.4697 - val_acc: 0.7866\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:135,FP:36,FN:36,TP:128,Macc:0.784980692977,F1:0.780482252567\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2396 - acc: 0.8932 - val_loss: 0.5713 - val_acc: 0.7408\n",
      "9639/9639 [==============================] - 3s 276us/step\n",
      "TN:144,FP:27,FN:54,TP:110,Macc:0.756418435658,F1:0.730891500841\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2393 - acc: 0.8932 - val_loss: 0.5393 - val_acc: 0.7514\n",
      "9639/9639 [==============================] - 3s 275us/step\n",
      "TN:142,FP:29,FN:53,TP:111,Macc:0.753619263101,F1:0.730257639367\n",
      "Loss: 0.532647\n",
      "Iteration No: 40 ended. Search finished for the next optimal point.\n",
      "Time taken: 2090.7107\n",
      "Function value obtained: 0.5326\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 41 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.7697 - acc: 0.8423 - val_loss: 0.5468 - val_acc: 0.7809\n",
      "9639/9639 [==============================] - 13s 1ms/step\n",
      "TN:127,FP:44,FN:32,TP:132,Macc:0.773784002749,F1:0.776465044883\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.3137 - acc: 0.8567 - val_loss: 0.4939 - val_acc: 0.7720\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:136,FP:35,FN:43,TP:121,Macc:0.766563207414,F1:0.756244451485\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.3047 - acc: 0.8600 - val_loss: 0.6747 - val_acc: 0.5808\n",
      "9639/9639 [==============================] - 2s 247us/step\n",
      "TN:146,FP:25,FN:106,TP:58,Macc:0.603729813766,F1:0.46963067723\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2962 - acc: 0.8663 - val_loss: 0.5990 - val_acc: 0.6562\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:138,FP:33,FN:72,TP:92,Macc:0.683996532038,F1:0.636672752431\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2913 - acc: 0.8683 - val_loss: 0.5219 - val_acc: 0.7290\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:137,FP:34,FN:48,TP:116,Macc:0.754243282417,F1:0.738847962392\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2820 - acc: 0.8734 - val_loss: 0.5070 - val_acc: 0.7594\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:133,FP:38,FN:40,TP:124,Macc:0.766937619004,F1:0.760730645228\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2790 - acc: 0.8741 - val_loss: 0.4926 - val_acc: 0.7758\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:132,FP:39,FN:35,TP:129,Macc:0.779257544,F1:0.777102883077\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2771 - acc: 0.8764 - val_loss: 0.4843 - val_acc: 0.7545\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:136,FP:35,FN:45,TP:119,Macc:0.760465646847,F1:0.748422126657\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2731 - acc: 0.8779 - val_loss: 0.6424 - val_acc: 0.6837\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:154,FP:17,FN:79,TP:85,Macc:0.709438692776,F1:0.639092490347\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2716 - acc: 0.8782 - val_loss: 0.4736 - val_acc: 0.7813\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:136,FP:35,FN:39,TP:125,Macc:0.778758328547,F1:0.771599386756\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2699 - acc: 0.8787 - val_loss: 0.6030 - val_acc: 0.7021\n",
      "9639/9639 [==============================] - 2s 248us/step\n",
      "TN:159,FP:12,FN:81,TP:83,Macc:0.71796101431,F1:0.640921476882\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2648 - acc: 0.8816 - val_loss: 0.4751 - val_acc: 0.7765\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:135,FP:36,FN:42,TP:122,Macc:0.766688011277,F1:0.757758425332\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2651 - acc: 0.8816 - val_loss: 0.4998 - val_acc: 0.7673\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:143,FP:28,FN:47,TP:117,Macc:0.774835921221,F1:0.757276020316\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2620 - acc: 0.8833 - val_loss: 0.4923 - val_acc: 0.7675\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:101,FP:70,FN:21,TP:143,Macc:0.731297198943,F1:0.758615238336\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2628 - acc: 0.8831 - val_loss: 0.5170 - val_acc: 0.7714\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:144,FP:27,FN:47,TP:117,Macc:0.777759897641,F1:0.759734728704\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2594 - acc: 0.8840 - val_loss: 0.4836 - val_acc: 0.7817\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:143,FP:28,FN:43,TP:121,Macc:0.787031042355,F1:0.773157397624\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2571 - acc: 0.8844 - val_loss: 0.4547 - val_acc: 0.7939\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:125,FP:46,FN:28,TP:136,Macc:0.780131171043,F1:0.786121632569\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2561 - acc: 0.8848 - val_loss: 0.4499 - val_acc: 0.8114\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:134,FP:37,FN:30,TP:134,Macc:0.800349398257,F1:0.799994449902\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2556 - acc: 0.8848 - val_loss: 0.4876 - val_acc: 0.7761\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:145,FP:26,FN:46,TP:118,Macc:0.783732654345,F1:0.766228234733\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2539 - acc: 0.8856 - val_loss: 0.5105 - val_acc: 0.7543\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:136,FP:35,FN:45,TP:119,Macc:0.760465646847,F1:0.748422126657\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2532 - acc: 0.8858 - val_loss: 0.4866 - val_acc: 0.7761\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:142,FP:29,FN:43,TP:121,Macc:0.784107065934,F1:0.770695093917\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2532 - acc: 0.8852 - val_loss: 0.4429 - val_acc: 0.8072\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:139,FP:32,FN:34,TP:130,Macc:0.802774159224,F1:0.797540458693\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2499 - acc: 0.8881 - val_loss: 0.4764 - val_acc: 0.7795\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:136,FP:35,FN:40,TP:124,Macc:0.775709548264,F1:0.767796306646\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2516 - acc: 0.8873 - val_loss: 0.4478 - val_acc: 0.7971\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:133,FP:38,FN:30,TP:134,Macc:0.797425421837,F1:0.79761349855\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2492 - acc: 0.8883 - val_loss: 0.4822 - val_acc: 0.7794\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:144,FP:27,FN:43,TP:121,Macc:0.789955018775,F1:0.775635485451\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2495 - acc: 0.8873 - val_loss: 0.4925 - val_acc: 0.7787\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:146,FP:25,FN:44,TP:120,Macc:0.792754191332,F1:0.776693494661\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2488 - acc: 0.8883 - val_loss: 0.5117 - val_acc: 0.7591\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:146,FP:25,FN:49,TP:115,Macc:0.777510289915,F1:0.756573426935\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2483 - acc: 0.8875 - val_loss: 0.4456 - val_acc: 0.7947\n",
      "9639/9639 [==============================] - 2s 250us/step\n",
      "TN:136,FP:35,FN:33,TP:131,Macc:0.797051010247,F1:0.79393384125\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2493 - acc: 0.8877 - val_loss: 0.4437 - val_acc: 0.7924\n",
      "9639/9639 [==============================] - 2s 249us/step\n",
      "TN:133,FP:38,FN:32,TP:132,Macc:0.79132786127,F1:0.790413611426\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2464 - acc: 0.8880 - val_loss: 0.4098 - val_acc: 0.8258\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:132,FP:39,FN:25,TP:139,Macc:0.809745346833,F1:0.81285995404\n",
      "Loss: 0.402749\n",
      "Iteration No: 41 ended. Search finished for the next optimal point.\n",
      "Time taken: 2069.8916\n",
      "Function value obtained: 0.4027\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 42 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.7698 - acc: 0.8411 - val_loss: 0.5800 - val_acc: 0.7288\n",
      "9639/9639 [==============================] - 13s 1ms/step\n",
      "TN:139,FP:32,FN:66,TP:98,Macc:0.705213190158,F1:0.666661190382\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3122 - acc: 0.8506 - val_loss: 0.5215 - val_acc: 0.8205\n",
      "9639/9639 [==============================] - 3s 267us/step\n",
      "TN:115,FP:56,FN:7,TP:157,Macc:0.814915792791,F1:0.832885791028\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2997 - acc: 0.8586 - val_loss: 0.5418 - val_acc: 0.7905\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:98,FP:73,FN:3,TP:161,Macc:0.777403314783,F1:0.809039851579\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2918 - acc: 0.8662 - val_loss: 0.4915 - val_acc: 0.7985\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:113,FP:58,FN:10,TP:154,Macc:0.799921499101,F1:0.81914347791\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2845 - acc: 0.8720 - val_loss: 0.5273 - val_acc: 0.7551\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:82,FP:89,FN:4,TP:160,Macc:0.727570911778,F1:0.774813093669\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2804 - acc: 0.8744 - val_loss: 0.4861 - val_acc: 0.7869\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:95,FP:76,FN:5,TP:159,Macc:0.762533824956,F1:0.796987111448\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2764 - acc: 0.8756 - val_loss: 0.4805 - val_acc: 0.7855\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:92,FP:79,FN:1,TP:163,Macc:0.765957016829,F1:0.802950324551\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2739 - acc: 0.8773 - val_loss: 0.5439 - val_acc: 0.7329\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:75,FP:96,FN:5,TP:159,Macc:0.704054296554,F1:0.758944600283\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2687 - acc: 0.8794 - val_loss: 0.5236 - val_acc: 0.7556\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:80,FP:91,FN:2,TP:162,Macc:0.727820519504,F1:0.776973126846\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2656 - acc: 0.8813 - val_loss: 0.5521 - val_acc: 0.7264\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:70,FP:101,FN:1,TP:163,Macc:0.701629535587,F1:0.761677004119\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2649 - acc: 0.8816 - val_loss: 0.5193 - val_acc: 0.7544\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:81,FP:90,FN:3,TP:161,Macc:0.727695715641,F1:0.775898315077\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2619 - acc: 0.8830 - val_loss: 0.6347 - val_acc: 0.6535\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:43,FP:128,FN:0,TP:164,Macc:0.625730952528,F1:0.719293144311\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2609 - acc: 0.8828 - val_loss: 0.5126 - val_acc: 0.7607\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:83,FP:88,FN:0,TP:164,Macc:0.742690009331,F1:0.788456242916\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2588 - acc: 0.8846 - val_loss: 0.6198 - val_acc: 0.6665\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:44,FP:127,FN:1,TP:163,Macc:0.625606148664,F1:0.718056562884\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2607 - acc: 0.8834 - val_loss: 0.6103 - val_acc: 0.6772\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:51,FP:120,FN:0,TP:164,Macc:0.649122763888,F1:0.732137715836\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2581 - acc: 0.8852 - val_loss: 0.5634 - val_acc: 0.7154\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:62,FP:109,FN:0,TP:164,Macc:0.681286504509,F1:0.750566886808\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2563 - acc: 0.8850 - val_loss: 0.4786 - val_acc: 0.7802\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:89,FP:82,FN:0,TP:164,Macc:0.760233867852,F1:0.799994677108\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2555 - acc: 0.8859 - val_loss: 0.5112 - val_acc: 0.7629\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:86,FP:85,FN:0,TP:164,Macc:0.751461938592,F1:0.794183552685\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2531 - acc: 0.8862 - val_loss: 0.5179 - val_acc: 0.7524\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:80,FP:91,FN:1,TP:163,Macc:0.730869299788,F1:0.779899020181\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2534 - acc: 0.8878 - val_loss: 0.5163 - val_acc: 0.7510\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:80,FP:91,FN:1,TP:163,Macc:0.730869299788,F1:0.779899020181\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2520 - acc: 0.8866 - val_loss: 0.5304 - val_acc: 0.7457\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:79,FP:92,FN:1,TP:163,Macc:0.727945323368,F1:0.778037678039\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2500 - acc: 0.8886 - val_loss: 0.5221 - val_acc: 0.7455\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:81,FP:90,FN:1,TP:163,Macc:0.733793276208,F1:0.781769289663\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2518 - acc: 0.8876 - val_loss: 0.5019 - val_acc: 0.7626\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:89,FP:82,FN:5,TP:159,Macc:0.744989966435,F1:0.785179841377\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2499 - acc: 0.8880 - val_loss: 0.5104 - val_acc: 0.7538\n",
      "9639/9639 [==============================] - 2s 253us/step\n",
      "TN:82,FP:89,FN:1,TP:163,Macc:0.736717252628,F1:0.783648550863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2500 - acc: 0.8877 - val_loss: 0.4827 - val_acc: 0.7733\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:88,FP:83,FN:1,TP:163,Macc:0.754261111148,F1:0.795116628589\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2495 - acc: 0.8882 - val_loss: 0.4915 - val_acc: 0.7691\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:88,FP:83,FN:1,TP:163,Macc:0.754261111148,F1:0.795116628589\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2491 - acc: 0.8887 - val_loss: 0.4789 - val_acc: 0.7791\n",
      "9639/9639 [==============================] - 2s 251us/step\n",
      "TN:91,FP:80,FN:2,TP:162,Macc:0.759984260125,F1:0.798024216444\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2464 - acc: 0.8897 - val_loss: 0.4478 - val_acc: 0.8001\n",
      "9639/9639 [==============================] - 2s 252us/step\n",
      "TN:102,FP:69,FN:4,TP:160,Macc:0.78605044018,F1:0.814243968776\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2464 - acc: 0.8904 - val_loss: 0.4968 - val_acc: 0.7658\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:86,FP:85,FN:1,TP:163,Macc:0.748413158308,F1:0.791256822332\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2449 - acc: 0.8901 - val_loss: 0.5184 - val_acc: 0.7487\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:81,FP:90,FN:1,TP:163,Macc:0.733793276208,F1:0.781769289663\n",
      "Loss: 0.511692\n",
      "Iteration No: 42 ended. Search finished for the next optimal point.\n",
      "Time taken: 2080.2798\n",
      "Function value obtained: 0.5117\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 43 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.7590 - acc: 0.8427 - val_loss: 0.5750 - val_acc: 0.7655\n",
      "9639/9639 [==============================] - 14s 1ms/step\n",
      "TN:111,FP:60,FN:18,TP:146,Macc:0.769683303994,F1:0.789183713171\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.3015 - acc: 0.8620 - val_loss: 0.5249 - val_acc: 0.7860\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:109,FP:62,FN:13,TP:151,Macc:0.779079252571,F1:0.80105555416\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2872 - acc: 0.8694 - val_loss: 0.4832 - val_acc: 0.8082\n",
      "9639/9639 [==============================] - 3s 280us/step\n",
      "TN:111,FP:60,FN:13,TP:151,Macc:0.784927205411,F1:0.80532787252\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2774 - acc: 0.8753 - val_loss: 0.4748 - val_acc: 0.8081\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:104,FP:67,FN:0,TP:164,Macc:0.804093514153,F1:0.830374358861\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2739 - acc: 0.8766 - val_loss: 0.4629 - val_acc: 0.8024\n",
      "9639/9639 [==============================] - 3s 282us/step\n",
      "TN:102,FP:69,FN:4,TP:160,Macc:0.78605044018,F1:0.814243968776\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2688 - acc: 0.8800 - val_loss: 0.4871 - val_acc: 0.7867\n",
      "9639/9639 [==============================] - 3s 278us/step\n",
      "TN:92,FP:79,FN:1,TP:163,Macc:0.765957016829,F1:0.802950324551\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2662 - acc: 0.8817 - val_loss: 0.4477 - val_acc: 0.8131\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:102,FP:69,FN:6,TP:158,Macc:0.779952879613,F1:0.808178740572\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2640 - acc: 0.8829 - val_loss: 0.4963 - val_acc: 0.7743\n",
      "9639/9639 [==============================] - 3s 282us/step\n",
      "TN:89,FP:82,FN:1,TP:163,Macc:0.757185087569,F1:0.797060687549\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2610 - acc: 0.8845 - val_loss: 0.4297 - val_acc: 0.8292\n",
      "9639/9639 [==============================] - 3s 290us/step\n",
      "TN:112,FP:59,FN:10,TP:154,Macc:0.796997522681,F1:0.816970672594\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2607 - acc: 0.8846 - val_loss: 0.4333 - val_acc: 0.8265\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:113,FP:58,FN:8,TP:156,Macc:0.806019059668,F1:0.825391373625\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2573 - acc: 0.8851 - val_loss: 0.4449 - val_acc: 0.8132\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:112,FP:59,FN:13,TP:151,Macc:0.787851181831,F1:0.807481166755\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2563 - acc: 0.8856 - val_loss: 0.4577 - val_acc: 0.8015\n",
      "9639/9639 [==============================] - 3s 282us/step\n",
      "TN:102,FP:69,FN:6,TP:158,Macc:0.779952879613,F1:0.808178740572\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2560 - acc: 0.8862 - val_loss: 0.4582 - val_acc: 0.8015\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:100,FP:71,FN:6,TP:158,Macc:0.774104926773,F1:0.804065852297\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2540 - acc: 0.8875 - val_loss: 0.4335 - val_acc: 0.8239\n",
      "9639/9639 [==============================] - 3s 283us/step\n",
      "TN:117,FP:54,FN:12,TP:152,Macc:0.805519844215,F1:0.821616143674\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 47s 40ms/step - loss: 0.2543 - acc: 0.8876 - val_loss: 0.4394 - val_acc: 0.8145\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:109,FP:62,FN:9,TP:155,Macc:0.791274373704,F1:0.813642853447\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2524 - acc: 0.8882 - val_loss: 0.4307 - val_acc: 0.8299\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:125,FP:46,FN:19,TP:145,Macc:0.807570193592,F1:0.816895889678\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2524 - acc: 0.8878 - val_loss: 0.4366 - val_acc: 0.8203\n",
      "9639/9639 [==============================] - 3s 280us/step\n",
      "TN:120,FP:51,FN:16,TP:148,Macc:0.802096652342,F1:0.815421498993\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2518 - acc: 0.8880 - val_loss: 0.4445 - val_acc: 0.8136\n",
      "9639/9639 [==============================] - 3s 278us/step\n",
      "TN:122,FP:49,FN:19,TP:145,Macc:0.798798264332,F1:0.810050354801\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2494 - acc: 0.8894 - val_loss: 0.4411 - val_acc: 0.8125\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:110,FP:61,FN:10,TP:154,Macc:0.791149569841,F1:0.812659460106\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2490 - acc: 0.8886 - val_loss: 0.4227 - val_acc: 0.8267\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:122,FP:49,FN:11,TP:153,Macc:0.823188506599,F1:0.836060082839\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 47s 40ms/step - loss: 0.2485 - acc: 0.8894 - val_loss: 0.4292 - val_acc: 0.8242\n",
      "9639/9639 [==============================] - 3s 281us/step\n",
      "TN:121,FP:50,FN:14,TP:150,Macc:0.811118189329,F1:0.824170328197\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2459 - acc: 0.8887 - val_loss: 0.4325 - val_acc: 0.8184\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:114,FP:57,FN:9,TP:155,Macc:0.805894255804,F1:0.824462626535\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2481 - acc: 0.8892 - val_loss: 0.4462 - val_acc: 0.8198\n",
      "9639/9639 [==============================] - 3s 283us/step\n",
      "TN:119,FP:52,FN:17,TP:147,Macc:0.796123895638,F1:0.809911857454\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2454 - acc: 0.8902 - val_loss: 0.4260 - val_acc: 0.8202\n",
      "9639/9639 [==============================] - 3s 278us/step\n",
      "TN:116,FP:55,FN:12,TP:152,Macc:0.802595867795,F1:0.819401533416\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2451 - acc: 0.8901 - val_loss: 0.4232 - val_acc: 0.8265\n",
      "9639/9639 [==============================] - 3s 283us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:119,FP:52,FN:12,TP:152,Macc:0.811367797055,F1:0.826081472153\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2463 - acc: 0.8899 - val_loss: 0.4322 - val_acc: 0.8185\n",
      "9639/9639 [==============================] - 3s 278us/step\n",
      "TN:114,FP:57,FN:11,TP:153,Macc:0.799796695238,F1:0.818176353292\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2437 - acc: 0.8912 - val_loss: 0.4317 - val_acc: 0.8150\n",
      "9639/9639 [==============================] - 3s 279us/step\n",
      "TN:114,FP:57,FN:12,TP:152,Macc:0.796747914954,F1:0.815007936843\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2442 - acc: 0.8912 - val_loss: 0.4525 - val_acc: 0.7998\n",
      "9639/9639 [==============================] - 3s 280us/step\n",
      "TN:108,FP:63,FN:9,TP:155,Macc:0.788350397284,F1:0.811512887813\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2450 - acc: 0.8907 - val_loss: 0.4337 - val_acc: 0.8207\n",
      "9639/9639 [==============================] - 3s 280us/step\n",
      "TN:118,FP:53,FN:14,TP:150,Macc:0.802346260068,F1:0.817433205242\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2441 - acc: 0.8920 - val_loss: 0.4316 - val_acc: 0.8198\n",
      "9639/9639 [==============================] - 3s 278us/step\n",
      "TN:118,FP:53,FN:15,TP:149,Macc:0.799297479785,F1:0.814202160656\n",
      "Loss: 0.424674\n",
      "Iteration No: 43 ended. Search finished for the next optimal point.\n",
      "Time taken: 2146.8736\n",
      "Function value obtained: 0.4247\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 44 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.7497 - acc: 0.8453 - val_loss: 0.5667 - val_acc: 0.7312\n",
      "9639/9639 [==============================] - 14s 1ms/step\n",
      "TN:144,FP:27,FN:64,TP:100,Macc:0.725930632825,F1:0.687279760367\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.3081 - acc: 0.8592 - val_loss: 0.5108 - val_acc: 0.7693\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:140,FP:31,FN:52,TP:112,Macc:0.750820090544,F1:0.7296361673\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2949 - acc: 0.8635 - val_loss: 0.4982 - val_acc: 0.8160\n",
      "9639/9639 [==============================] - 3s 274us/step\n",
      "TN:123,FP:48,FN:15,TP:149,Macc:0.813917361885,F1:0.825479260232\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2846 - acc: 0.8695 - val_loss: 0.4650 - val_acc: 0.8034\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:139,FP:32,FN:43,TP:121,Macc:0.775335136674,F1:0.763401393744\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2791 - acc: 0.8742 - val_loss: 0.4236 - val_acc: 0.8340\n",
      "9639/9639 [==============================] - 3s 273us/step\n",
      "TN:127,FP:44,FN:16,TP:148,Macc:0.822564487282,F1:0.831455156834\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2713 - acc: 0.8776 - val_loss: 0.4296 - val_acc: 0.8293\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:129,FP:42,FN:19,TP:145,Macc:0.819266099273,F1:0.826205298078\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2673 - acc: 0.8801 - val_loss: 0.4461 - val_acc: 0.8190\n",
      "9639/9639 [==============================] - 3s 274us/step\n",
      "TN:105,FP:66,FN:3,TP:161,Macc:0.797871149723,F1:0.82352400825\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2630 - acc: 0.8842 - val_loss: 0.4781 - val_acc: 0.7747\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:94,FP:77,FN:4,TP:160,Macc:0.762658628819,F1:0.797999626058\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2605 - acc: 0.8838 - val_loss: 0.4513 - val_acc: 0.8122\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:110,FP:61,FN:8,TP:156,Macc:0.797247130407,F1:0.818892196975\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2573 - acc: 0.8846 - val_loss: 0.4677 - val_acc: 0.7911\n",
      "9639/9639 [==============================] - 3s 265us/step\n",
      "TN:105,FP:66,FN:8,TP:156,Macc:0.782627248307,F1:0.808284733586\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2576 - acc: 0.8846 - val_loss: 0.4132 - val_acc: 0.8218\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:112,FP:59,FN:6,TP:158,Macc:0.809192643814,F1:0.829390884033\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2545 - acc: 0.8864 - val_loss: 0.5072 - val_acc: 0.7850\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:105,FP:66,FN:14,TP:150,Macc:0.764334566607,F1:0.789468241533\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2513 - acc: 0.8881 - val_loss: 0.4791 - val_acc: 0.7920\n",
      "9639/9639 [==============================] - 3s 264us/step\n",
      "TN:108,FP:63,FN:15,TP:149,Macc:0.770057715584,F1:0.792547734787\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2495 - acc: 0.8900 - val_loss: 0.4241 - val_acc: 0.8197\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:112,FP:59,FN:12,TP:152,Macc:0.790899962114,F1:0.81066120554\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2488 - acc: 0.8892 - val_loss: 0.3902 - val_acc: 0.8319\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:132,FP:39,FN:23,TP:141,Macc:0.8158429074,F1:0.819761901369\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2466 - acc: 0.8902 - val_loss: 0.3967 - val_acc: 0.8310\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:120,FP:51,FN:14,TP:150,Macc:0.808194212908,F1:0.821912315232\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2445 - acc: 0.8911 - val_loss: 0.3903 - val_acc: 0.8318\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:121,FP:50,FN:14,TP:150,Macc:0.811118189329,F1:0.824170328197\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2448 - acc: 0.8903 - val_loss: 0.4028 - val_acc: 0.8265\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:116,FP:55,FN:12,TP:152,Macc:0.802595867795,F1:0.819401533416\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2433 - acc: 0.8897 - val_loss: 0.3953 - val_acc: 0.8365\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:121,FP:50,FN:12,TP:152,Macc:0.817215749895,F1:0.830595602293\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2418 - acc: 0.8907 - val_loss: 0.4345 - val_acc: 0.8051\n",
      "9639/9639 [==============================] - 2s 257us/step\n",
      "TN:105,FP:66,FN:11,TP:153,Macc:0.773480907457,F1:0.798950181141\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2433 - acc: 0.8906 - val_loss: 0.3847 - val_acc: 0.8363\n",
      "9639/9639 [==============================] - 3s 262us/step\n",
      "TN:120,FP:51,FN:12,TP:152,Macc:0.814291773475,F1:0.82833238715\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2378 - acc: 0.8939 - val_loss: 0.3738 - val_acc: 0.8402\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:116,FP:55,FN:9,TP:155,Macc:0.811742208645,F1:0.828871539828\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2398 - acc: 0.8933 - val_loss: 0.4012 - val_acc: 0.8273\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:112,FP:59,FN:9,TP:155,Macc:0.800046302964,F1:0.820100368642\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2382 - acc: 0.8939 - val_loss: 0.3734 - val_acc: 0.8500\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:141,FP:30,FN:23,TP:141,Macc:0.842158695181,F1:0.841785491932\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2394 - acc: 0.8914 - val_loss: 0.3706 - val_acc: 0.8468\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:125,FP:46,FN:11,TP:153,Macc:0.831960435859,F1:0.842969706689\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2358 - acc: 0.8952 - val_loss: 0.3685 - val_acc: 0.8440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:122,FP:49,FN:12,TP:152,Macc:0.820139726315,F1:0.832871218681\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2380 - acc: 0.8927 - val_loss: 0.3707 - val_acc: 0.8405\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:114,FP:57,FN:5,TP:159,Macc:0.818089376938,F1:0.836836659841\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2355 - acc: 0.8947 - val_loss: 0.3657 - val_acc: 0.8524\n",
      "9639/9639 [==============================] - 2s 254us/step\n",
      "TN:131,FP:40,FN:14,TP:150,Macc:0.840357953529,F1:0.847452104156\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2365 - acc: 0.8930 - val_loss: 0.3719 - val_acc: 0.8538\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:136,FP:35,FN:18,TP:146,Macc:0.842782714497,F1:0.846371271012\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2335 - acc: 0.8961 - val_loss: 0.3768 - val_acc: 0.8483\n",
      "9639/9639 [==============================] - 2s 255us/step\n",
      "TN:137,FP:34,FN:22,TP:142,Macc:0.833511569784,F1:0.835288570486\n",
      "Loss: 0.370485\n",
      "Iteration No: 44 ended. Search finished for the next optimal point.\n",
      "Time taken: 2108.4628\n",
      "Function value obtained: 0.3705\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 45 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.7485 - acc: 0.8504 - val_loss: 0.5722 - val_acc: 0.7837\n",
      "9639/9639 [==============================] - 14s 1ms/step\n",
      "TN:114,FP:57,FN:21,TP:143,Macc:0.769308892405,F1:0.785708792062\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2958 - acc: 0.8666 - val_loss: 0.4727 - val_acc: 0.8185\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:108,FP:63,FN:7,TP:157,Macc:0.79444795785,F1:0.817702903491\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2799 - acc: 0.8738 - val_loss: 0.5436 - val_acc: 0.7507\n",
      "9639/9639 [==============================] - 3s 288us/step\n",
      "TN:86,FP:85,FN:2,TP:162,Macc:0.745364378025,F1:0.788315850024\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2712 - acc: 0.8806 - val_loss: 0.4213 - val_acc: 0.8483\n",
      "9639/9639 [==============================] - 3s 286us/step\n",
      "TN:119,FP:52,FN:8,TP:156,Macc:0.823562918188,F1:0.838704204799\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2645 - acc: 0.8833 - val_loss: 0.4523 - val_acc: 0.8270\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:115,FP:56,FN:10,TP:154,Macc:0.805769451941,F1:0.82352394656\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2586 - acc: 0.8857 - val_loss: 0.4139 - val_acc: 0.8444\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:121,FP:50,FN:12,TP:152,Macc:0.817215749895,F1:0.830595602293\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 40ms/step - loss: 0.2564 - acc: 0.8874 - val_loss: 0.3853 - val_acc: 0.8542\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:125,FP:46,FN:9,TP:155,Macc:0.838057996425,F1:0.849309573854\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2540 - acc: 0.8880 - val_loss: 0.3849 - val_acc: 0.8597\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:128,FP:43,FN:10,TP:154,Macc:0.843781145402,F1:0.853180089568\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2502 - acc: 0.8881 - val_loss: 0.3988 - val_acc: 0.8396\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:118,FP:53,FN:8,TP:156,Macc:0.820638941768,F1:0.836455656757\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2487 - acc: 0.8888 - val_loss: 0.3943 - val_acc: 0.8439\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:121,FP:50,FN:10,TP:154,Macc:0.823313310462,F1:0.83695103672\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2473 - acc: 0.8897 - val_loss: 0.3745 - val_acc: 0.8566\n",
      "9639/9639 [==============================] - 3s 288us/step\n",
      "TN:119,FP:52,FN:5,TP:159,Macc:0.832709259038,F1:0.847994536682\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2470 - acc: 0.8911 - val_loss: 0.3660 - val_acc: 0.8588\n",
      "9639/9639 [==============================] - 3s 286us/step\n",
      "TN:130,FP:41,FN:15,TP:149,Macc:0.834385196826,F1:0.841802386993\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2457 - acc: 0.8907 - val_loss: 0.3772 - val_acc: 0.8456\n",
      "9639/9639 [==============================] - 3s 283us/step\n",
      "TN:115,FP:56,FN:6,TP:158,Macc:0.817964573074,F1:0.835973383591\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2437 - acc: 0.8906 - val_loss: 0.3913 - val_acc: 0.8341\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:111,FP:60,FN:6,TP:158,Macc:0.806268667394,F1:0.82721969319\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2441 - acc: 0.8913 - val_loss: 0.3605 - val_acc: 0.8636\n",
      "9639/9639 [==============================] - 3s 290us/step\n",
      "TN:137,FP:34,FN:19,TP:145,Macc:0.842657910633,F1:0.845475505888\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2403 - acc: 0.8934 - val_loss: 0.3796 - val_acc: 0.8431\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:123,FP:48,FN:9,TP:155,Macc:0.832210043585,F1:0.844681160011\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2414 - acc: 0.8931 - val_loss: 0.3633 - val_acc: 0.8647\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:142,FP:29,FN:24,TP:140,Macc:0.842033891317,F1:0.840835286566\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2398 - acc: 0.8926 - val_loss: 0.3611 - val_acc: 0.8535\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:128,FP:43,FN:11,TP:153,Macc:0.840732365119,F1:0.849994491547\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2398 - acc: 0.8931 - val_loss: 0.3829 - val_acc: 0.8370\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:116,FP:55,FN:6,TP:158,Macc:0.820888549495,F1:0.838190830506\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2379 - acc: 0.8929 - val_loss: 0.3680 - val_acc: 0.8550\n",
      "9639/9639 [==============================] - 3s 288us/step\n",
      "TN:136,FP:35,FN:20,TP:144,Macc:0.83668515393,F1:0.839644602472\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2366 - acc: 0.8946 - val_loss: 0.3679 - val_acc: 0.8496\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:128,FP:43,FN:8,TP:156,Macc:0.849878705969,F1:0.859498631306\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2372 - acc: 0.8937 - val_loss: 0.3869 - val_acc: 0.8486\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:140,FP:31,FN:23,TP:141,Macc:0.839234718761,F1:0.839280162486\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2359 - acc: 0.8953 - val_loss: 0.3895 - val_acc: 0.8329\n",
      "9639/9639 [==============================] - 3s 288us/step\n",
      "TN:124,FP:47,FN:10,TP:154,Macc:0.832085239722,F1:0.843830122129\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2356 - acc: 0.8956 - val_loss: 0.3815 - val_acc: 0.8481\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:139,FP:32,FN:21,TP:143,Macc:0.842408302907,F1:0.843652268185\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2358 - acc: 0.8941 - val_loss: 0.3665 - val_acc: 0.8614\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:141,FP:30,FN:20,TP:144,Macc:0.851305036031,F1:0.852065455307\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2345 - acc: 0.8959 - val_loss: 0.3989 - val_acc: 0.8238\n",
      "9639/9639 [==============================] - 3s 286us/step\n",
      "TN:117,FP:54,FN:9,TP:155,Macc:0.814666185065,F1:0.831093726778\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2339 - acc: 0.8961 - val_loss: 0.3869 - val_acc: 0.8532\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:140,FP:31,FN:23,TP:141,Macc:0.839234718761,F1:0.839280162486\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2333 - acc: 0.8966 - val_loss: 0.3796 - val_acc: 0.8549\n",
      "9639/9639 [==============================] - 3s 288us/step\n",
      "TN:138,FP:33,FN:20,TP:144,Macc:0.84253310677,F1:0.844569233599\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2345 - acc: 0.8953 - val_loss: 0.4079 - val_acc: 0.8487\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:152,FP:19,FN:31,TP:133,Macc:0.849932193535,F1:0.841766601262\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2317 - acc: 0.8972 - val_loss: 0.3707 - val_acc: 0.8516\n",
      "9639/9639 [==============================] - 3s 293us/step\n",
      "TN:134,FP:37,FN:16,TP:148,Macc:0.843032322223,F1:0.848132002301\n",
      "Loss: 0.363944\n",
      "Iteration No: 45 ended. Search finished for the next optimal point.\n",
      "Time taken: 2169.3289\n",
      "Function value obtained: 0.3639\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 46 started. Searching for the next optimal point.\n",
      "args [1, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.7642 - acc: 0.8449 - val_loss: 0.5631 - val_acc: 0.7752\n",
      "9639/9639 [==============================] - 14s 1ms/step\n",
      "TN:102,FP:69,FN:13,TP:151,Macc:0.75861141763,F1:0.786452905283\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.3063 - acc: 0.8619 - val_loss: 0.5313 - val_acc: 0.7664\n",
      "9639/9639 [==============================] - 3s 272us/step\n",
      "TN:110,FP:61,FN:24,TP:140,Macc:0.748466645874,F1:0.767117797989\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2890 - acc: 0.8704 - val_loss: 0.5162 - val_acc: 0.7910\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:112,FP:59,FN:16,TP:148,Macc:0.778704840981,F1:0.797838192378\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2803 - acc: 0.8752 - val_loss: 0.5945 - val_acc: 0.7060\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:76,FP:95,FN:4,TP:160,Macc:0.710027053257,F1:0.763717869722\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2753 - acc: 0.8786 - val_loss: 0.5476 - val_acc: 0.7440\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:94,FP:77,FN:13,TP:151,Macc:0.735219606269,F1:0.770402766671\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2698 - acc: 0.8795 - val_loss: 0.5561 - val_acc: 0.7378\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:91,FP:80,FN:9,TP:155,Macc:0.738642798142,F1:0.776936987241\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2647 - acc: 0.8822 - val_loss: 0.5265 - val_acc: 0.7690\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:115,FP:56,FN:22,TP:142,Macc:0.769184088541,F1:0.784524887618\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2646 - acc: 0.8818 - val_loss: 0.5332 - val_acc: 0.7549\n",
      "9639/9639 [==============================] - 3s 266us/step\n",
      "TN:93,FP:78,FN:7,TP:157,Macc:0.750588311549,F1:0.786962049345\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2612 - acc: 0.8837 - val_loss: 0.5078 - val_acc: 0.7759\n",
      "9639/9639 [==============================] - 3s 271us/step\n",
      "TN:95,FP:76,FN:6,TP:158,Macc:0.759485044673,F1:0.793964475529\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2604 - acc: 0.8840 - val_loss: 0.5395 - val_acc: 0.7405\n",
      "9639/9639 [==============================] - 3s 268us/step\n",
      "TN:80,FP:91,FN:0,TP:164,Macc:0.733918080071,F1:0.782810947478\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2577 - acc: 0.8838 - val_loss: 0.5168 - val_acc: 0.7661\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:92,FP:79,FN:7,TP:157,Macc:0.747664335129,F1:0.784994635061\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2575 - acc: 0.8834 - val_loss: 0.4690 - val_acc: 0.7999\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:105,FP:66,FN:7,TP:157,Macc:0.78567602859,F1:0.811364090789\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2560 - acc: 0.8853 - val_loss: 0.4483 - val_acc: 0.8237\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:130,FP:41,FN:21,TP:143,Macc:0.816092515126,F1:0.821833546707\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2548 - acc: 0.8857 - val_loss: 0.4897 - val_acc: 0.7788\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:95,FP:76,FN:3,TP:161,Macc:0.768631385523,F1:0.802987156956\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2556 - acc: 0.8840 - val_loss: 0.5112 - val_acc: 0.7637\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:92,FP:79,FN:7,TP:157,Macc:0.747664335129,F1:0.784994635061\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2537 - acc: 0.8861 - val_loss: 0.4877 - val_acc: 0.7831\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:99,FP:72,FN:4,TP:160,Macc:0.77727851092,F1:0.8080754254\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2530 - acc: 0.8853 - val_loss: 0.4223 - val_acc: 0.8388\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:125,FP:46,FN:11,TP:153,Macc:0.831960435859,F1:0.842969706689\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2518 - acc: 0.8855 - val_loss: 0.4602 - val_acc: 0.8105\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:117,FP:54,FN:13,TP:151,Macc:0.802471063931,F1:0.818422703424\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2501 - acc: 0.8871 - val_loss: 0.4892 - val_acc: 0.7804\n",
      "9639/9639 [==============================] - 2s 257us/step\n",
      "TN:96,FP:75,FN:7,TP:157,Macc:0.759360240809,F1:0.792923911091\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2509 - acc: 0.8845 - val_loss: 0.4357 - val_acc: 0.8224\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:116,FP:55,FN:9,TP:155,Macc:0.811742208645,F1:0.828871539828\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2507 - acc: 0.8862 - val_loss: 0.4561 - val_acc: 0.8099\n",
      "9639/9639 [==============================] - 2s 259us/step\n",
      "TN:115,FP:56,FN:9,TP:155,Macc:0.808818232225,F1:0.826661204601\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2489 - acc: 0.8878 - val_loss: 0.4761 - val_acc: 0.7875\n",
      "9639/9639 [==============================] - 3s 280us/step\n",
      "TN:99,FP:72,FN:5,TP:159,Macc:0.774229730636,F1:0.805057904576\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2474 - acc: 0.8881 - val_loss: 0.4352 - val_acc: 0.8226\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:119,FP:52,FN:8,TP:156,Macc:0.823562918188,F1:0.838704204799\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2489 - acc: 0.8877 - val_loss: 0.4434 - val_acc: 0.8137\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:110,FP:61,FN:6,TP:158,Macc:0.803344690974,F1:0.825059840214\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2481 - acc: 0.8878 - val_loss: 0.4443 - val_acc: 0.8239\n",
      "9639/9639 [==============================] - 3s 263us/step\n",
      "TN:124,FP:47,FN:18,TP:146,Macc:0.807694997456,F1:0.817921656793\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2464 - acc: 0.8886 - val_loss: 0.5153 - val_acc: 0.7543\n",
      "9639/9639 [==============================] - 3s 260us/step\n",
      "TN:86,FP:85,FN:3,TP:161,Macc:0.742315597742,F1:0.785360531552\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2463 - acc: 0.8896 - val_loss: 0.4241 - val_acc: 0.8332\n",
      "9639/9639 [==============================] - 3s 261us/step\n",
      "TN:122,FP:49,FN:12,TP:152,Macc:0.820139726315,F1:0.832871218681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2452 - acc: 0.8889 - val_loss: 0.4305 - val_acc: 0.8217\n",
      "9639/9639 [==============================] - 2s 256us/step\n",
      "TN:113,FP:58,FN:10,TP:154,Macc:0.799921499101,F1:0.81914347791\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 46s 38ms/step - loss: 0.2450 - acc: 0.8886 - val_loss: 0.4107 - val_acc: 0.8384\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:120,FP:51,FN:9,TP:155,Macc:0.823438114325,F1:0.837832358925\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 46s 39ms/step - loss: 0.2443 - acc: 0.8892 - val_loss: 0.4258 - val_acc: 0.8176\n",
      "9639/9639 [==============================] - 2s 258us/step\n",
      "TN:112,FP:59,FN:4,TP:160,Macc:0.815290204381,F1:0.835503703843\n",
      "Loss: 0.419668\n",
      "Iteration No: 46 ended. Search finished for the next optimal point.\n",
      "Time taken: 2142.8973\n",
      "Function value obtained: 0.4197\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 47 started. Searching for the next optimal point.\n",
      "args [5, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.7508 - acc: 0.8394 - val_loss: 0.5941 - val_acc: 0.7499\n",
      "9639/9639 [==============================] - 15s 2ms/step\n",
      "TN:123,FP:48,FN:51,TP:113,Macc:0.704161271686,F1:0.695379068825\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2997 - acc: 0.8591 - val_loss: 0.5936 - val_acc: 0.7294\n",
      "9639/9639 [==============================] - 3s 302us/step\n",
      "TN:75,FP:96,FN:0,TP:164,Macc:0.719298197971,F1:0.773579647508\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2835 - acc: 0.8733 - val_loss: 0.5409 - val_acc: 0.7516\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:106,FP:65,FN:19,TP:145,Macc:0.752014641611,F1:0.775395607147\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2756 - acc: 0.8754 - val_loss: 0.5408 - val_acc: 0.7423\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:83,FP:88,FN:4,TP:160,Macc:0.730494888198,F1:0.776693716314\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2707 - acc: 0.8797 - val_loss: 0.5225 - val_acc: 0.7657\n",
      "9639/9639 [==============================] - 3s 290us/step\n",
      "TN:90,FP:81,FN:4,TP:160,Macc:0.750962723139,F1:0.790118112713\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2665 - acc: 0.8816 - val_loss: 0.4945 - val_acc: 0.7852\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:111,FP:60,FN:17,TP:147,Macc:0.772732084278,F1:0.792447357118\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2653 - acc: 0.8831 - val_loss: 0.5097 - val_acc: 0.7631\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:90,FP:81,FN:1,TP:163,Macc:0.760109063989,F1:0.799014276251\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2639 - acc: 0.8824 - val_loss: 0.4553 - val_acc: 0.8131\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:117,FP:54,FN:12,TP:152,Macc:0.805519844215,F1:0.821616143674\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2629 - acc: 0.8826 - val_loss: 0.4430 - val_acc: 0.8196\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:115,FP:56,FN:10,TP:154,Macc:0.805769451941,F1:0.82352394656\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2597 - acc: 0.8854 - val_loss: 0.5146 - val_acc: 0.7566\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:90,FP:81,FN:3,TP:161,Macc:0.754011503422,F1:0.793098108337\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2610 - acc: 0.8845 - val_loss: 0.4385 - val_acc: 0.8177\n",
      "9639/9639 [==============================] - 3s 288us/step\n",
      "TN:112,FP:59,FN:7,TP:157,Macc:0.806143863531,F1:0.826310344661\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2556 - acc: 0.8876 - val_loss: 0.4252 - val_acc: 0.8156\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:127,FP:44,FN:26,TP:138,Macc:0.792076684449,F1:0.797682325475\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2569 - acc: 0.8861 - val_loss: 0.4478 - val_acc: 0.8110\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:107,FP:64,FN:7,TP:157,Macc:0.79152398143,F1:0.815578989572\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2556 - acc: 0.8874 - val_loss: 0.4253 - val_acc: 0.8218\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:110,FP:61,FN:7,TP:157,Macc:0.800295910691,F1:0.821984091398\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2575 - acc: 0.8847 - val_loss: 0.4242 - val_acc: 0.8178\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:114,FP:57,FN:7,TP:157,Macc:0.811991816371,F1:0.830682378608\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2543 - acc: 0.8857 - val_loss: 0.4048 - val_acc: 0.8281\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:120,FP:51,FN:18,TP:146,Macc:0.795999091775,F1:0.808858762631\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2537 - acc: 0.8874 - val_loss: 0.4269 - val_acc: 0.8197\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:134,FP:37,FN:27,TP:137,Macc:0.809495739107,F1:0.810645339661\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2529 - acc: 0.8877 - val_loss: 0.4299 - val_acc: 0.8148\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:109,FP:62,FN:5,TP:159,Macc:0.803469494837,F1:0.825968599367\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2534 - acc: 0.8867 - val_loss: 0.4162 - val_acc: 0.8297\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:135,FP:36,FN:25,TP:139,Macc:0.818517276093,F1:0.820053449659\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2494 - acc: 0.8884 - val_loss: 0.4058 - val_acc: 0.8271\n",
      "9639/9639 [==============================] - 3s 286us/step\n",
      "TN:123,FP:48,FN:20,TP:144,Macc:0.798673460469,F1:0.808983248112\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2515 - acc: 0.8879 - val_loss: 0.4199 - val_acc: 0.8293\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:142,FP:29,FN:29,TP:135,Macc:0.826789989901,F1:0.823165176531\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2494 - acc: 0.8894 - val_loss: 0.4017 - val_acc: 0.8336\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:128,FP:43,FN:22,TP:142,Macc:0.807195782003,F1:0.813748050316\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 47s 40ms/step - loss: 0.2505 - acc: 0.8879 - val_loss: 0.4250 - val_acc: 0.8160\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:109,FP:62,FN:3,TP:161,Macc:0.809567055404,F1:0.832035924238\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2485 - acc: 0.8892 - val_loss: 0.3992 - val_acc: 0.8333\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:127,FP:44,FN:14,TP:150,Macc:0.828662047849,F1:0.837983313977\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2470 - acc: 0.8894 - val_loss: 0.3930 - val_acc: 0.8336\n",
      "9639/9639 [==============================] - 3s 306us/step\n",
      "TN:126,FP:45,FN:18,TP:146,Macc:0.813542950296,F1:0.822529692145\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2493 - acc: 0.8881 - val_loss: 0.3881 - val_acc: 0.8399\n",
      "9639/9639 [==============================] - 3s 288us/step\n",
      "TN:125,FP:46,FN:16,TP:148,Macc:0.816716534442,F1:0.826810130307\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2459 - acc: 0.8908 - val_loss: 0.4128 - val_acc: 0.8331\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:142,FP:29,FN:28,TP:136,Macc:0.829838770184,F1:0.826742165168\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2470 - acc: 0.8903 - val_loss: 0.3960 - val_acc: 0.8314\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:114,FP:57,FN:7,TP:157,Macc:0.811991816371,F1:0.830682378608\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2474 - acc: 0.8902 - val_loss: 0.4037 - val_acc: 0.8292\n",
      "9639/9639 [==============================] - 3s 302us/step\n",
      "TN:136,FP:35,FN:25,TP:139,Macc:0.821441252514,F1:0.822479658417\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2455 - acc: 0.8908 - val_loss: 0.3987 - val_acc: 0.8320\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:115,FP:56,FN:3,TP:161,Macc:0.827110913924,F1:0.845138914619\n",
      "Loss: 0.392367\n",
      "Iteration No: 47 ended. Search finished for the next optimal point.\n",
      "Time taken: 2222.7725\n",
      "Function value obtained: 0.3924\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 48 started. Searching for the next optimal point.\n",
      "args [5, 1, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.8054 - acc: 0.8382 - val_loss: 0.5826 - val_acc: 0.8128\n",
      "9639/9639 [==============================] - 16s 2ms/step\n",
      "TN:105,FP:66,FN:0,TP:164,Macc:0.807017490573,F1:0.832481917528\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.3191 - acc: 0.8531 - val_loss: 0.6107 - val_acc: 0.7097\n",
      "9639/9639 [==============================] - 3s 356us/step\n",
      "TN:64,FP:107,FN:0,TP:164,Macc:0.68713445735,F1:0.754017783182\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.3010 - acc: 0.8618 - val_loss: 0.5546 - val_acc: 0.7630\n",
      "9639/9639 [==============================] - 3s 351us/step\n",
      "TN:90,FP:81,FN:2,TP:162,Macc:0.757060283705,F1:0.796063460291\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2890 - acc: 0.8712 - val_loss: 0.6020 - val_acc: 0.6920\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:66,FP:105,FN:1,TP:163,Macc:0.689933629906,F1:0.754624409994\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2789 - acc: 0.8780 - val_loss: 0.6006 - val_acc: 0.6838\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:64,FP:107,FN:0,TP:164,Macc:0.68713445735,F1:0.754017783182\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2726 - acc: 0.8811 - val_loss: 0.4656 - val_acc: 0.8235\n",
      "9639/9639 [==============================] - 4s 367us/step\n",
      "TN:114,FP:57,FN:6,TP:158,Macc:0.815040596654,F1:0.8337676383\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2681 - acc: 0.8825 - val_loss: 0.5943 - val_acc: 0.6530\n",
      "9639/9639 [==============================] - 3s 355us/step\n",
      "TN:147,FP:24,FN:80,TP:84,Macc:0.685922077552,F1:0.617641742043\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2659 - acc: 0.8845 - val_loss: 1.2247 - val_acc: 0.5299\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:154,FP:17,FN:122,TP:42,Macc:0.578341140594,F1:0.376677296596\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2622 - acc: 0.8847 - val_loss: 1.6757 - val_acc: 0.5002\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:163,FP:8,FN:143,TP:21,Macc:0.540632542425,F1:0.217613746552\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2595 - acc: 0.8869 - val_loss: 2.0367 - val_acc: 0.4922\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:164,FP:7,FN:148,TP:16,Macc:0.528312617429,F1:0.171120601479\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2581 - acc: 0.8866 - val_loss: 1.9287 - val_acc: 0.4931\n",
      "9639/9639 [==============================] - 3s 352us/step\n",
      "TN:163,FP:8,FN:147,TP:17,Macc:0.528437421292,F1:0.179891633864\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2551 - acc: 0.8879 - val_loss: 2.0790 - val_acc: 0.4926\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:163,FP:8,FN:147,TP:17,Macc:0.528437421292,F1:0.179891633864\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2531 - acc: 0.8895 - val_loss: 2.9693 - val_acc: 0.4740\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:168,FP:3,FN:158,TP:6,Macc:0.509520720276,F1:0.0693630680769\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2529 - acc: 0.8886 - val_loss: 1.7344 - val_acc: 0.4949\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:162,FP:9,FN:147,TP:17,Macc:0.525513444872,F1:0.178944749178\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2509 - acc: 0.8887 - val_loss: 2.7042 - val_acc: 0.4793\n",
      "9639/9639 [==============================] - 3s 352us/step\n",
      "TN:169,FP:2,FN:156,TP:8,Macc:0.518542257263,F1:0.0919528196735\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2472 - acc: 0.8908 - val_loss: 2.9534 - val_acc: 0.4828\n",
      "9639/9639 [==============================] - 3s 358us/step\n",
      "TN:165,FP:6,FN:152,TP:12,Macc:0.519041472715,F1:0.131866155324\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2477 - acc: 0.8917 - val_loss: 1.9356 - val_acc: 0.4939\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:163,FP:8,FN:144,TP:20,Macc:0.537583762142,F1:0.208330569046\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2455 - acc: 0.8928 - val_loss: 1.9351 - val_acc: 0.4885\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:165,FP:6,FN:151,TP:13,Macc:0.522090252999,F1:0.14207443868\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2449 - acc: 0.8921 - val_loss: 2.9110 - val_acc: 0.4738\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:171,FP:0,FN:162,TP:2,Macc:0.506097528403,F1:0.0240961204834\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2427 - acc: 0.8938 - val_loss: 3.2814 - val_acc: 0.4719\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2412 - acc: 0.8951 - val_loss: 1.4971 - val_acc: 0.5089\n",
      "9639/9639 [==============================] - 3s 355us/step\n",
      "TN:163,FP:8,FN:142,TP:22,Macc:0.543681322708,F1:0.226801222057\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2425 - acc: 0.8941 - val_loss: 1.8745 - val_acc: 0.4831\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:171,FP:0,FN:155,TP:9,Macc:0.527438990386,F1:0.104045144586\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2396 - acc: 0.8959 - val_loss: 1.8948 - val_acc: 0.4869\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:169,FP:2,FN:153,TP:11,Macc:0.527688598113,F1:0.124292272736\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2397 - acc: 0.8950 - val_loss: 1.3778 - val_acc: 0.5046\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:166,FP:5,FN:147,TP:17,Macc:0.537209350552,F1:0.182793382962\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2397 - acc: 0.8965 - val_loss: 1.4192 - val_acc: 0.5231\n",
      "9639/9639 [==============================] - 3s 352us/step\n",
      "TN:161,FP:10,FN:129,TP:35,Macc:0.577467513551,F1:0.334924477502\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2382 - acc: 0.8956 - val_loss: 2.1885 - val_acc: 0.4894\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:167,FP:4,FN:152,TP:12,Macc:0.524889425556,F1:0.133331535332\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2365 - acc: 0.8965 - val_loss: 1.8009 - val_acc: 0.4980\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:167,FP:4,FN:144,TP:20,Macc:0.549279667822,F1:0.212763482599\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2361 - acc: 0.8969 - val_loss: 1.8690 - val_acc: 0.5024\n",
      "9639/9639 [==============================] - 3s 362us/step\n",
      "TN:168,FP:3,FN:146,TP:18,Macc:0.546106083676,F1:0.194592357658\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2376 - acc: 0.8956 - val_loss: 2.5007 - val_acc: 0.4835\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:170,FP:1,FN:156,TP:8,Macc:0.521466233683,F1:0.0924844524156\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2346 - acc: 0.8970 - val_loss: 2.2544 - val_acc: 0.4900\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:169,FP:2,FN:151,TP:13,Macc:0.533786158679,F1:0.145249689729\n",
      "Loss: 2.244130\n",
      "Iteration No: 48 ended. Search finished for the next optimal point.\n",
      "Time taken: 2293.9292\n",
      "Function value obtained: 2.2441\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 49 started. Searching for the next optimal point.\n",
      "args [4, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.7664 - acc: 0.8323 - val_loss: 0.6056 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 16s 2ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.3149 - acc: 0.8326 - val_loss: 0.6097 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 3s 302us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2993 - acc: 0.8368 - val_loss: 0.5492 - val_acc: 0.8127\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:104,FP:67,FN:2,TP:162,Macc:0.797995953587,F1:0.824422085254\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2912 - acc: 0.8414 - val_loss: 0.5285 - val_acc: 0.8187\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:111,FP:60,FN:4,TP:160,Macc:0.812366227961,F1:0.833327902596\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2901 - acc: 0.8446 - val_loss: 0.5262 - val_acc: 0.8212\n",
      "9639/9639 [==============================] - 3s 293us/step\n",
      "TN:118,FP:53,FN:11,TP:153,Macc:0.811492600918,F1:0.827021548757\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2857 - acc: 0.8474 - val_loss: 0.5248 - val_acc: 0.8127\n",
      "9639/9639 [==============================] - 3s 303us/step\n",
      "TN:118,FP:53,FN:16,TP:148,Macc:0.796248699502,F1:0.810953411784\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2839 - acc: 0.8500 - val_loss: 0.5161 - val_acc: 0.8196\n",
      "9639/9639 [==============================] - 3s 290us/step\n",
      "TN:113,FP:58,FN:4,TP:160,Macc:0.818214180801,F1:0.837690896775\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2824 - acc: 0.8569 - val_loss: 0.5282 - val_acc: 0.7945\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:98,FP:73,FN:0,TP:164,Macc:0.786549655633,F1:0.817949749651\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2783 - acc: 0.8678 - val_loss: 0.5117 - val_acc: 0.7949\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:105,FP:66,FN:3,TP:161,Macc:0.797871149723,F1:0.82352400825\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2750 - acc: 0.8718 - val_loss: 0.5037 - val_acc: 0.7887\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:107,FP:64,FN:13,TP:151,Macc:0.773231299731,F1:0.796828326461\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2732 - acc: 0.8730 - val_loss: 0.5042 - val_acc: 0.7867\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:100,FP:71,FN:1,TP:163,Macc:0.78934882819,F1:0.81909010228\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2716 - acc: 0.8738 - val_loss: 0.5041 - val_acc: 0.7890\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:101,FP:70,FN:10,TP:154,Macc:0.76483378206,F1:0.793809019539\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2701 - acc: 0.8752 - val_loss: 0.4718 - val_acc: 0.8113\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:114,FP:57,FN:17,TP:147,Macc:0.781504013538,F1:0.798907560735\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 47s 40ms/step - loss: 0.2676 - acc: 0.8772 - val_loss: 0.4928 - val_acc: 0.7869\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:117,FP:54,FN:16,TP:148,Macc:0.793324723082,F1:0.808737680111\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2673 - acc: 0.8776 - val_loss: 0.4744 - val_acc: 0.8013\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:112,FP:59,FN:14,TP:150,Macc:0.784802401548,F1:0.804284076886\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2658 - acc: 0.8780 - val_loss: 0.4709 - val_acc: 0.8053\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:104,FP:67,FN:3,TP:161,Macc:0.794947173303,F1:0.821423171968\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2641 - acc: 0.8808 - val_loss: 0.4732 - val_acc: 0.8062\n",
      "9639/9639 [==============================] - 3s 293us/step\n",
      "TN:123,FP:48,FN:19,TP:145,Macc:0.801722240752,F1:0.812319416242\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2641 - acc: 0.8796 - val_loss: 0.4951 - val_acc: 0.7869\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:96,FP:75,FN:0,TP:164,Macc:0.780701702793,F1:0.813890427733\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2634 - acc: 0.8796 - val_loss: 0.4658 - val_acc: 0.8097\n",
      "9639/9639 [==============================] - 3s 290us/step\n",
      "TN:112,FP:59,FN:15,TP:149,Macc:0.781753621264,F1:0.801069798424\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 47s 40ms/step - loss: 0.2634 - acc: 0.8799 - val_loss: 0.4618 - val_acc: 0.8121\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:130,FP:41,FN:24,TP:140,Macc:0.806946174276,F1:0.811588664536\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2625 - acc: 0.8810 - val_loss: 0.4948 - val_acc: 0.7907\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:126,FP:45,FN:34,TP:130,Macc:0.764762465763,F1:0.766956107974\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2617 - acc: 0.8813 - val_loss: 0.4626 - val_acc: 0.8099\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:131,FP:40,FN:26,TP:138,Macc:0.80377259013,F1:0.8070120012\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2618 - acc: 0.8806 - val_loss: 0.4745 - val_acc: 0.7965\n",
      "9639/9639 [==============================] - 3s 290us/step\n",
      "TN:129,FP:42,FN:28,TP:136,Macc:0.791827076723,F1:0.79531609552\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2616 - acc: 0.8801 - val_loss: 0.4606 - val_acc: 0.8113\n",
      "9639/9639 [==============================] - 3s 305us/step\n",
      "TN:124,FP:47,FN:18,TP:146,Macc:0.807694997456,F1:0.817921656793\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2598 - acc: 0.8811 - val_loss: 0.4761 - val_acc: 0.8051\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:123,FP:48,FN:24,TP:140,Macc:0.786478339336,F1:0.795449021344\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2612 - acc: 0.8813 - val_loss: 0.4584 - val_acc: 0.8109\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:119,FP:52,FN:15,TP:149,Macc:0.802221456205,F1:0.816432863508\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2595 - acc: 0.8826 - val_loss: 0.4674 - val_acc: 0.8063\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:113,FP:58,FN:18,TP:146,Macc:0.775531256835,F1:0.793472778452\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2572 - acc: 0.8854 - val_loss: 0.4330 - val_acc: 0.8288\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:123,FP:48,FN:21,TP:143,Macc:0.795624680186,F1:0.805628284743\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2589 - acc: 0.8826 - val_loss: 0.4375 - val_acc: 0.8245\n",
      "9639/9639 [==============================] - 3s 290us/step\n",
      "TN:130,FP:41,FN:22,TP:142,Macc:0.813043734843,F1:0.818438268671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2582 - acc: 0.8830 - val_loss: 0.4347 - val_acc: 0.8266\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:133,FP:38,FN:27,TP:137,Macc:0.806571762687,F1:0.808254040396\n",
      "Loss: 0.428900\n",
      "Iteration No: 49 ended. Search finished for the next optimal point.\n",
      "Time taken: 2277.2371\n",
      "Function value obtained: 0.4289\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 50 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 58s 48ms/step - loss: 0.7573 - acc: 0.8486 - val_loss: 0.5840 - val_acc: 0.7509\n",
      "9639/9639 [==============================] - 16s 2ms/step\n",
      "TN:102,FP:69,FN:17,TP:147,Macc:0.746416296497,F1:0.773678768763\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2990 - acc: 0.8656 - val_loss: 0.5033 - val_acc: 0.7994\n",
      "9639/9639 [==============================] - 3s 307us/step\n",
      "TN:105,FP:66,FN:8,TP:156,Macc:0.782627248307,F1:0.808284733586\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2845 - acc: 0.8711 - val_loss: 0.5554 - val_acc: 0.7284\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:76,FP:95,FN:6,TP:158,Macc:0.703929492691,F1:0.757788475581\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2731 - acc: 0.8776 - val_loss: 0.5482 - val_acc: 0.7256\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:78,FP:93,FN:4,TP:160,Macc:0.715875006098,F1:0.767380801214\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2665 - acc: 0.8799 - val_loss: 0.4642 - val_acc: 0.8021\n",
      "9639/9639 [==============================] - 3s 284us/step\n",
      "TN:104,FP:67,FN:5,TP:159,Macc:0.788849612737,F1:0.815379208424\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2600 - acc: 0.8826 - val_loss: 0.5368 - val_acc: 0.7321\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:78,FP:93,FN:6,TP:158,Macc:0.709777445531,F1:0.761440484519\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2595 - acc: 0.8837 - val_loss: 0.5351 - val_acc: 0.7333\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:79,FP:92,FN:5,TP:159,Macc:0.715750202234,F1:0.766259761372\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2573 - acc: 0.8849 - val_loss: 0.5488 - val_acc: 0.7193\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:72,FP:99,FN:4,TP:160,Macc:0.698331147577,F1:0.756495920138\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2552 - acc: 0.8852 - val_loss: 0.4340 - val_acc: 0.8172\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:123,FP:48,FN:16,TP:148,Macc:0.810868581602,F1:0.822216715468\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2538 - acc: 0.8860 - val_loss: 0.4316 - val_acc: 0.8138\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:105,FP:66,FN:8,TP:156,Macc:0.782627248307,F1:0.808284733586\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2527 - acc: 0.8861 - val_loss: 0.4747 - val_acc: 0.7788\n",
      "9639/9639 [==============================] - 3s 286us/step\n",
      "TN:94,FP:77,FN:4,TP:160,Macc:0.762658628819,F1:0.797999626058\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2489 - acc: 0.8889 - val_loss: 0.4601 - val_acc: 0.7913\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:104,FP:67,FN:10,TP:154,Macc:0.77360571132,F1:0.799994574878\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2494 - acc: 0.8883 - val_loss: 0.4013 - val_acc: 0.8358\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:122,FP:49,FN:11,TP:153,Macc:0.823188506599,F1:0.836060082839\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2487 - acc: 0.8881 - val_loss: 0.4160 - val_acc: 0.8266\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:117,FP:54,FN:11,TP:153,Macc:0.808568624498,F1:0.824792368676\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2483 - acc: 0.8887 - val_loss: 0.4378 - val_acc: 0.8034\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:103,FP:68,FN:4,TP:160,Macc:0.7889744166,F1:0.816321131439\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2465 - acc: 0.8903 - val_loss: 0.4165 - val_acc: 0.8164\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:110,FP:61,FN:7,TP:157,Macc:0.800295910691,F1:0.821984091398\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2454 - acc: 0.8901 - val_loss: 0.4173 - val_acc: 0.8210\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:123,FP:48,FN:16,TP:148,Macc:0.810868581602,F1:0.822216715468\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2451 - acc: 0.8905 - val_loss: 0.3883 - val_acc: 0.8468\n",
      "9639/9639 [==============================] - 3s 302us/step\n",
      "TN:133,FP:38,FN:17,TP:147,Macc:0.83705956552,F1:0.842401343637\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2428 - acc: 0.8915 - val_loss: 0.3997 - val_acc: 0.8336\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:125,FP:46,FN:16,TP:148,Macc:0.816716534442,F1:0.826810130307\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2439 - acc: 0.8907 - val_loss: 0.3788 - val_acc: 0.8411\n",
      "9639/9639 [==============================] - 3s 289us/step\n",
      "TN:122,FP:49,FN:10,TP:154,Macc:0.826237286882,F1:0.839231569057\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2421 - acc: 0.8918 - val_loss: 0.4153 - val_acc: 0.8169\n",
      "9639/9639 [==============================] - 3s 287us/step\n",
      "TN:110,FP:61,FN:6,TP:158,Macc:0.803344690974,F1:0.825059840214\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2400 - acc: 0.8928 - val_loss: 0.4355 - val_acc: 0.8061\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:105,FP:66,FN:4,TP:160,Macc:0.79482236944,F1:0.820507413263\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2410 - acc: 0.8932 - val_loss: 0.3890 - val_acc: 0.8440\n",
      "9639/9639 [==============================] - 3s 286us/step\n",
      "TN:120,FP:51,FN:10,TP:154,Macc:0.820389334042,F1:0.834682865055\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2405 - acc: 0.8922 - val_loss: 0.4103 - val_acc: 0.8196\n",
      "9639/9639 [==============================] - 3s 290us/step\n",
      "TN:109,FP:62,FN:6,TP:158,Macc:0.800420714554,F1:0.822911236526\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2398 - acc: 0.8925 - val_loss: 0.3896 - val_acc: 0.8410\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:122,FP:49,FN:10,TP:154,Macc:0.826237286882,F1:0.839231569057\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 47s 39ms/step - loss: 0.2398 - acc: 0.8923 - val_loss: 0.3748 - val_acc: 0.8526\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:138,FP:33,FN:16,TP:148,Macc:0.854728227904,F1:0.857965473171\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2393 - acc: 0.8930 - val_loss: 0.3676 - val_acc: 0.8506\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:132,FP:39,FN:15,TP:149,Macc:0.840233149666,F1:0.846585381783\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 47s 40ms/step - loss: 0.2370 - acc: 0.8943 - val_loss: 0.3804 - val_acc: 0.8445\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:125,FP:46,FN:11,TP:153,Macc:0.831960435859,F1:0.842969706689\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2388 - acc: 0.8925 - val_loss: 0.3827 - val_acc: 0.8474\n",
      "9639/9639 [==============================] - 3s 285us/step\n",
      "TN:135,FP:36,FN:19,TP:145,Macc:0.836809957793,F1:0.840574169933\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2370 - acc: 0.8951 - val_loss: 0.3781 - val_acc: 0.8438\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:119,FP:52,FN:9,TP:155,Macc:0.820514137905,F1:0.835574039195\n",
      "Loss: 0.372591\n",
      "Iteration No: 50 ended. Search finished for the next optimal point.\n",
      "Time taken: 2280.3332\n",
      "Function value obtained: 0.3726\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 51 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.7587 - acc: 0.8465 - val_loss: 0.5761 - val_acc: 0.7507\n",
      "9639/9639 [==============================] - 16s 2ms/step\n",
      "TN:113,FP:58,FN:34,TP:130,Macc:0.726750772302,F1:0.73863084308\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2990 - acc: 0.8674 - val_loss: 0.5175 - val_acc: 0.7826\n",
      "9639/9639 [==============================] - 3s 304us/step\n",
      "TN:98,FP:73,FN:4,TP:160,Macc:0.774354534499,F1:0.80603996156\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2818 - acc: 0.8751 - val_loss: 0.4967 - val_acc: 0.7827\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:98,FP:73,FN:4,TP:160,Macc:0.774354534499,F1:0.80603996156\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2721 - acc: 0.8787 - val_loss: 0.4703 - val_acc: 0.8029\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:103,FP:68,FN:2,TP:162,Macc:0.795071977167,F1:0.822329633831\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2702 - acc: 0.8791 - val_loss: 0.4542 - val_acc: 0.8135\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:116,FP:55,FN:16,TP:148,Macc:0.790400746661,F1:0.806534023335\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2647 - acc: 0.8817 - val_loss: 0.4440 - val_acc: 0.8137\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:106,FP:65,FN:3,TP:161,Macc:0.800795126144,F1:0.825635618102\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2614 - acc: 0.8825 - val_loss: 0.4393 - val_acc: 0.8174\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:110,FP:61,FN:4,TP:160,Macc:0.809442251541,F1:0.831163404265\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2591 - acc: 0.8852 - val_loss: 0.4218 - val_acc: 0.8248\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:113,FP:58,FN:3,TP:161,Macc:0.821262961084,F1:0.840725635658\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2575 - acc: 0.8848 - val_loss: 0.4203 - val_acc: 0.8260\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:111,FP:60,FN:5,TP:159,Macc:0.809317447677,F1:0.830281772029\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2549 - acc: 0.8853 - val_loss: 0.4050 - val_acc: 0.8287\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:114,FP:57,FN:7,TP:157,Macc:0.811991816371,F1:0.830682378608\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2530 - acc: 0.8865 - val_loss: 0.4379 - val_acc: 0.8069\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:98,FP:73,FN:4,TP:160,Macc:0.774354534499,F1:0.80603996156\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2533 - acc: 0.8873 - val_loss: 0.4472 - val_acc: 0.8025\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:104,FP:67,FN:4,TP:160,Macc:0.79189839302,F1:0.818408919024\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2517 - acc: 0.8877 - val_loss: 0.4081 - val_acc: 0.8245\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:109,FP:62,FN:5,TP:159,Macc:0.803469494837,F1:0.825968599367\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2513 - acc: 0.8888 - val_loss: 0.4055 - val_acc: 0.8258\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:109,FP:62,FN:3,TP:161,Macc:0.809567055404,F1:0.832035924238\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2511 - acc: 0.8879 - val_loss: 0.3855 - val_acc: 0.8406\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:116,FP:55,FN:3,TP:161,Macc:0.830034890345,F1:0.847362975021\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2483 - acc: 0.8890 - val_loss: 0.3848 - val_acc: 0.8422\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:121,FP:50,FN:8,TP:156,Macc:0.829410871028,F1:0.843237764009\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2477 - acc: 0.8902 - val_loss: 0.3971 - val_acc: 0.8307\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:111,FP:60,FN:4,TP:160,Macc:0.812366227961,F1:0.833327902596\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2478 - acc: 0.8906 - val_loss: 0.3792 - val_acc: 0.8422\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:121,FP:50,FN:6,TP:158,Macc:0.835508431595,F1:0.849456892334\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2474 - acc: 0.8899 - val_loss: 0.4040 - val_acc: 0.8240\n",
      "9639/9639 [==============================] - 3s 310us/step\n",
      "TN:110,FP:61,FN:1,TP:163,Macc:0.818588592391,F1:0.840200769484\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2464 - acc: 0.8892 - val_loss: 0.3782 - val_acc: 0.8383\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:115,FP:56,FN:4,TP:160,Macc:0.824062133641,F1:0.842099817431\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2446 - acc: 0.8911 - val_loss: 0.3850 - val_acc: 0.8424\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:130,FP:41,FN:18,TP:146,Macc:0.825238855976,F1:0.831903303418\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2447 - acc: 0.8910 - val_loss: 0.3683 - val_acc: 0.8517\n",
      "9639/9639 [==============================] - 3s 300us/step\n",
      "TN:128,FP:43,FN:12,TP:152,Macc:0.837683584836,F1:0.846791146535\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2461 - acc: 0.8900 - val_loss: 0.3769 - val_acc: 0.8378\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:114,FP:57,FN:4,TP:160,Macc:0.821138157221,F1:0.83988957109\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2454 - acc: 0.8912 - val_loss: 0.3836 - val_acc: 0.8396\n",
      "9639/9639 [==============================] - 3s 293us/step\n",
      "TN:119,FP:52,FN:3,TP:161,Macc:0.838806819605,F1:0.85410594894\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2434 - acc: 0.8899 - val_loss: 0.3677 - val_acc: 0.8502\n",
      "9639/9639 [==============================] - 3s 305us/step\n",
      "TN:126,FP:45,FN:13,TP:151,Macc:0.828786851712,F1:0.838883381116\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2445 - acc: 0.8910 - val_loss: 0.3713 - val_acc: 0.8470\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:132,FP:39,FN:14,TP:150,Macc:0.84328192995,F1:0.849852831596\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2418 - acc: 0.8912 - val_loss: 0.3614 - val_acc: 0.8541\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:126,FP:45,FN:9,TP:155,Macc:0.840981972846,F1:0.851642854008\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2428 - acc: 0.8910 - val_loss: 0.3672 - val_acc: 0.8515\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:130,FP:41,FN:10,TP:154,Macc:0.849629098243,F1:0.857933207133\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2427 - acc: 0.8912 - val_loss: 0.3638 - val_acc: 0.8541\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:125,FP:46,FN:8,TP:156,Macc:0.841106776709,F1:0.852453524476\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2423 - acc: 0.8903 - val_loss: 0.3631 - val_acc: 0.8494\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:121,FP:50,FN:6,TP:158,Macc:0.835508431595,F1:0.849456892334\n",
      "Loss: 0.357368\n",
      "Iteration No: 51 ended. Search finished for the next optimal point.\n",
      "Time taken: 2318.3959\n",
      "Function value obtained: 0.3574\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 52 started. Searching for the next optimal point.\n",
      "args [2, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.7645 - acc: 0.8432 - val_loss: 0.5602 - val_acc: 0.7538\n",
      "9639/9639 [==============================] - 17s 2ms/step\n",
      "TN:128,FP:43,FN:43,TP:121,Macc:0.743171396053,F1:0.737799328603\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.3087 - acc: 0.8606 - val_loss: 0.5637 - val_acc: 0.7397\n",
      "9639/9639 [==============================] - 3s 309us/step\n",
      "TN:95,FP:76,FN:17,TP:147,Macc:0.725948461556,F1:0.759684507166\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2961 - acc: 0.8637 - val_loss: 0.5967 - val_acc: 0.6861\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:65,FP:106,FN:2,TP:162,Macc:0.683960873203,F1:0.7499947806\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2866 - acc: 0.8695 - val_loss: 0.5485 - val_acc: 0.7422\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:91,FP:80,FN:12,TP:152,Macc:0.729496457292,F1:0.767671387242\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2826 - acc: 0.8723 - val_loss: 0.5755 - val_acc: 0.7091\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:75,FP:96,FN:8,TP:156,Macc:0.694907955704,F1:0.74999470649\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2776 - acc: 0.8756 - val_loss: 0.6576 - val_acc: 0.6120\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:29,FP:142,FN:0,TP:164,Macc:0.584795282646,F1:0.697867309841\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2725 - acc: 0.8778 - val_loss: 0.6322 - val_acc: 0.6423\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:44,FP:127,FN:6,TP:158,Macc:0.610362247248,F1:0.70378105652\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2688 - acc: 0.8800 - val_loss: 0.6544 - val_acc: 0.6266\n",
      "9639/9639 [==============================] - 3s 308us/step\n",
      "TN:32,FP:139,FN:0,TP:164,Macc:0.593567211906,F1:0.702350414591\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2696 - acc: 0.8796 - val_loss: 0.6600 - val_acc: 0.6173\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:29,FP:142,FN:0,TP:164,Macc:0.584795282646,F1:0.697867309841\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2624 - acc: 0.8836 - val_loss: 0.6228 - val_acc: 0.6585\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:50,FP:121,FN:0,TP:164,Macc:0.646198787468,F1:0.730507113115\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2622 - acc: 0.8836 - val_loss: 0.6750 - val_acc: 0.6081\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:26,FP:145,FN:0,TP:164,Macc:0.576023353386,F1:0.693441073442\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2606 - acc: 0.8849 - val_loss: 0.6641 - val_acc: 0.6234\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:31,FP:140,FN:0,TP:164,Macc:0.590643235486,F1:0.700849660128\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2587 - acc: 0.8837 - val_loss: 0.6571 - val_acc: 0.6249\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:32,FP:139,FN:2,TP:162,Macc:0.58746965134,F1:0.696769138036\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2579 - acc: 0.8859 - val_loss: 0.6456 - val_acc: 0.6322\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:40,FP:131,FN:1,TP:163,Macc:0.613910242984,F1:0.711785301975\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2570 - acc: 0.8857 - val_loss: 0.6289 - val_acc: 0.6412\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:38,FP:133,FN:3,TP:161,Macc:0.601964729577,F1:0.703051677941\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2552 - acc: 0.8862 - val_loss: 0.6214 - val_acc: 0.6500\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:44,FP:127,FN:5,TP:159,Macc:0.613411027531,F1:0.706661536412\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2556 - acc: 0.8865 - val_loss: 0.6680 - val_acc: 0.6176\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:26,FP:145,FN:3,TP:161,Macc:0.566877012536,F1:0.685101352993\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2536 - acc: 0.8878 - val_loss: 0.7052 - val_acc: 0.5992\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:19,FP:152,FN:0,TP:164,Macc:0.555555518445,F1:0.683328353577\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2520 - acc: 0.8878 - val_loss: 0.6634 - val_acc: 0.6109\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:23,FP:148,FN:3,TP:161,Macc:0.558105083276,F1:0.680756084603\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2511 - acc: 0.8882 - val_loss: 0.5858 - val_acc: 0.6849\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:60,FP:111,FN:3,TP:161,Macc:0.666292210819,F1:0.738526910334\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2520 - acc: 0.8877 - val_loss: 0.6001 - val_acc: 0.6684\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:52,FP:119,FN:3,TP:161,Macc:0.642900399458,F1:0.725220064743\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2499 - acc: 0.8880 - val_loss: 0.6744 - val_acc: 0.6216\n",
      "9639/9639 [==============================] - 3s 300us/step\n",
      "TN:28,FP:143,FN:3,TP:161,Macc:0.572724965376,F1:0.688029147911\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2494 - acc: 0.8877 - val_loss: 0.6827 - val_acc: 0.6128\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:24,FP:147,FN:2,TP:162,Macc:0.564077839979,F1:0.684984414216\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2496 - acc: 0.8893 - val_loss: 0.5500 - val_acc: 0.7059\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:71,FP:100,FN:6,TP:158,Macc:0.68930961059,F1:0.748809899769\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2482 - acc: 0.8895 - val_loss: 0.5746 - val_acc: 0.6960\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:61,FP:110,FN:3,TP:161,Macc:0.669216187239,F1:0.740224680432\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2483 - acc: 0.8886 - val_loss: 0.5880 - val_acc: 0.6792\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:50,FP:121,FN:3,TP:161,Macc:0.637052446618,F1:0.72196794359\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2480 - acc: 0.8889 - val_loss: 0.5858 - val_acc: 0.6799\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:50,FP:121,FN:3,TP:161,Macc:0.637052446618,F1:0.72196794359\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2480 - acc: 0.8892 - val_loss: 0.5462 - val_acc: 0.6975\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:59,FP:112,FN:3,TP:161,Macc:0.663368234399,F1:0.736836910383\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2469 - acc: 0.8896 - val_loss: 0.5179 - val_acc: 0.7292\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:77,FP:94,FN:3,TP:161,Macc:0.715999809961,F1:0.768491139161\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2458 - acc: 0.8889 - val_loss: 0.5701 - val_acc: 0.6758\n",
      "9639/9639 [==============================] - 3s 304us/step\n",
      "TN:49,FP:122,FN:4,TP:160,Macc:0.631079689915,F1:0.717483638879\n",
      "Loss: 0.564391\n",
      "Iteration No: 52 ended. Search finished for the next optimal point.\n",
      "Time taken: 2357.3690\n",
      "Function value obtained: 0.5644\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 53 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.7592 - acc: 0.8431 - val_loss: 0.6062 - val_acc: 0.7391\n",
      "9639/9639 [==============================] - 17s 2ms/step\n",
      "TN:98,FP:73,FN:17,TP:147,Macc:0.734720390816,F1:0.765619573144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.3132 - acc: 0.8549 - val_loss: 0.5545 - val_acc: 0.7955\n",
      "9639/9639 [==============================] - 3s 316us/step\n",
      "TN:123,FP:48,FN:24,TP:140,Macc:0.786478339336,F1:0.795449021344\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.3014 - acc: 0.8579 - val_loss: 0.6358 - val_acc: 0.6352\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:146,FP:25,FN:95,TP:69,Macc:0.637266396883,F1:0.534878580242\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2946 - acc: 0.8603 - val_loss: 0.5869 - val_acc: 0.6649\n",
      "9639/9639 [==============================] - 3s 304us/step\n",
      "TN:143,FP:28,FN:69,TP:95,Macc:0.707762754988,F1:0.662015467465\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2914 - acc: 0.8624 - val_loss: 0.5924 - val_acc: 0.6910\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:146,FP:25,FN:70,TP:94,Macc:0.713485903965,F1:0.664305541528\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2880 - acc: 0.8664 - val_loss: 0.5303 - val_acc: 0.7235\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:133,FP:38,FN:51,TP:113,Macc:0.733401035887,F1:0.717454776762\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2826 - acc: 0.8693 - val_loss: 0.5135 - val_acc: 0.7688\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:125,FP:46,FN:29,TP:135,Macc:0.777082390759,F1:0.782603159139\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2796 - acc: 0.8713 - val_loss: 0.4685 - val_acc: 0.7814\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:128,FP:43,FN:30,TP:134,Macc:0.782805539736,F1:0.785918210993\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2763 - acc: 0.8740 - val_loss: 0.5047 - val_acc: 0.7634\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:137,FP:34,FN:46,TP:118,Macc:0.760340842984,F1:0.746829899015\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2726 - acc: 0.8768 - val_loss: 0.4545 - val_acc: 0.8091\n",
      "9639/9639 [==============================] - 3s 306us/step\n",
      "TN:121,FP:50,FN:16,TP:148,Macc:0.805020628762,F1:0.817674056872\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2701 - acc: 0.8778 - val_loss: 0.4374 - val_acc: 0.8118\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:133,FP:38,FN:27,TP:137,Macc:0.806571762687,F1:0.808254040396\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2680 - acc: 0.8787 - val_loss: 0.4648 - val_acc: 0.7926\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:132,FP:39,FN:34,TP:130,Macc:0.782306324283,F1:0.780775230476\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2660 - acc: 0.8812 - val_loss: 0.4197 - val_acc: 0.8261\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:126,FP:45,FN:20,TP:144,Macc:0.807445389729,F1:0.815858499439\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2635 - acc: 0.8822 - val_loss: 0.4544 - val_acc: 0.7910\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:136,FP:35,FN:38,TP:126,Macc:0.78180710883,F1:0.775379063405\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2659 - acc: 0.8808 - val_loss: 0.4733 - val_acc: 0.7777\n",
      "9639/9639 [==============================] - 3s 302us/step\n",
      "TN:139,FP:32,FN:46,TP:118,Macc:0.766188795824,F1:0.751586815002\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2609 - acc: 0.8819 - val_loss: 0.4421 - val_acc: 0.7994\n",
      "9639/9639 [==============================] - 3s 308us/step\n",
      "TN:133,FP:38,FN:32,TP:132,Macc:0.79132786127,F1:0.790413611426\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2621 - acc: 0.8821 - val_loss: 0.4203 - val_acc: 0.8129\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:139,FP:32,FN:32,TP:132,Macc:0.808871719791,F1:0.804872494832\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2600 - acc: 0.8838 - val_loss: 0.4157 - val_acc: 0.8156\n",
      "9639/9639 [==============================] - 3s 300us/step\n",
      "TN:131,FP:40,FN:23,TP:141,Macc:0.81291893098,F1:0.817385765615\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2582 - acc: 0.8840 - val_loss: 0.4373 - val_acc: 0.8038\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:135,FP:36,FN:32,TP:132,Macc:0.79717581411,F1:0.795175171035\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2569 - acc: 0.8852 - val_loss: 0.4586 - val_acc: 0.7833\n",
      "9639/9639 [==============================] - 3s 303us/step\n",
      "TN:140,FP:31,FN:37,TP:127,Macc:0.796551794794,F1:0.78881432383\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2552 - acc: 0.8859 - val_loss: 0.4156 - val_acc: 0.8192\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:126,FP:45,FN:20,TP:144,Macc:0.807445389729,F1:0.815858499439\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2547 - acc: 0.8859 - val_loss: 0.4098 - val_acc: 0.8211\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:113,FP:58,FN:10,TP:154,Macc:0.799921499101,F1:0.81914347791\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2552 - acc: 0.8853 - val_loss: 0.3941 - val_acc: 0.8328\n",
      "9639/9639 [==============================] - 3s 301us/step\n",
      "TN:128,FP:43,FN:16,TP:148,Macc:0.825488463703,F1:0.83379729708\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2524 - acc: 0.8874 - val_loss: 0.3871 - val_acc: 0.8390\n",
      "9639/9639 [==============================] - 3s 301us/step\n",
      "TN:113,FP:58,FN:3,TP:161,Macc:0.821262961084,F1:0.840725635658\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2541 - acc: 0.8877 - val_loss: 0.3927 - val_acc: 0.8318\n",
      "9639/9639 [==============================] - 3s 302us/step\n",
      "TN:111,FP:60,FN:6,TP:158,Macc:0.806268667394,F1:0.82721969319\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2519 - acc: 0.8874 - val_loss: 0.3682 - val_acc: 0.8484\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:113,FP:58,FN:3,TP:161,Macc:0.821262961084,F1:0.840725635658\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2508 - acc: 0.8877 - val_loss: 0.3876 - val_acc: 0.8332\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:111,FP:60,FN:7,TP:157,Macc:0.803219887111,F1:0.824141540504\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2507 - acc: 0.8886 - val_loss: 0.3771 - val_acc: 0.8399\n",
      "9639/9639 [==============================] - 3s 300us/step\n",
      "TN:115,FP:56,FN:6,TP:158,Macc:0.817964573074,F1:0.835973383591\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2498 - acc: 0.8867 - val_loss: 0.3762 - val_acc: 0.8403\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:119,FP:52,FN:12,TP:152,Macc:0.811367797055,F1:0.826081472153\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2484 - acc: 0.8893 - val_loss: 0.3980 - val_acc: 0.8251\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:112,FP:59,FN:9,TP:155,Macc:0.800046302964,F1:0.820100368642\n",
      "Loss: 0.391244\n",
      "Iteration No: 53 ended. Search finished for the next optimal point.\n",
      "Time taken: 2350.0632\n",
      "Function value obtained: 0.3912\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 54 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 58s 48ms/step - loss: 0.7585 - acc: 0.8455 - val_loss: 0.6118 - val_acc: 0.7449\n",
      "9639/9639 [==============================] - 18s 2ms/step\n",
      "TN:111,FP:60,FN:35,TP:129,Macc:0.717854039178,F1:0.730872669046\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.3059 - acc: 0.8591 - val_loss: 0.5301 - val_acc: 0.7824\n",
      "9639/9639 [==============================] - 3s 319us/step\n",
      "TN:101,FP:70,FN:19,TP:145,Macc:0.73739475951,F1:0.765166059172\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2931 - acc: 0.8635 - val_loss: 0.5330 - val_acc: 0.7720\n",
      "9639/9639 [==============================] - 3s 312us/step\n",
      "TN:93,FP:78,FN:10,TP:154,Macc:0.741441970699,F1:0.777772396782\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2860 - acc: 0.8678 - val_loss: 0.5310 - val_acc: 0.7678\n",
      "9639/9639 [==============================] - 3s 301us/step\n",
      "TN:91,FP:80,FN:6,TP:158,Macc:0.747789138992,F1:0.786064295128\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2818 - acc: 0.8712 - val_loss: 0.4850 - val_acc: 0.7927\n",
      "9639/9639 [==============================] - 3s 309us/step\n",
      "TN:97,FP:74,FN:7,TP:157,Macc:0.762284217229,F1:0.794931322862\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2758 - acc: 0.8750 - val_loss: 0.4939 - val_acc: 0.8181\n",
      "9639/9639 [==============================] - 3s 310us/step\n",
      "TN:118,FP:53,FN:15,TP:149,Macc:0.799297479785,F1:0.814202160656\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2704 - acc: 0.8774 - val_loss: 0.5921 - val_acc: 0.6822\n",
      "9639/9639 [==============================] - 3s 302us/step\n",
      "TN:58,FP:113,FN:1,TP:163,Macc:0.666541818546,F1:0.740903910263\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2684 - acc: 0.8783 - val_loss: 0.4705 - val_acc: 0.7922\n",
      "9639/9639 [==============================] - 3s 310us/step\n",
      "TN:98,FP:73,FN:3,TP:161,Macc:0.777403314783,F1:0.809039851579\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2651 - acc: 0.8792 - val_loss: 0.5514 - val_acc: 0.7214\n",
      "9639/9639 [==============================] - 3s 302us/step\n",
      "TN:71,FP:100,FN:3,TP:161,Macc:0.69845595144,F1:0.757641806142\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2628 - acc: 0.8812 - val_loss: 0.5370 - val_acc: 0.7289\n",
      "9639/9639 [==============================] - 3s 308us/step\n",
      "TN:70,FP:101,FN:1,TP:163,Macc:0.701629535587,F1:0.761677004119\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2607 - acc: 0.8815 - val_loss: 0.4355 - val_acc: 0.8409\n",
      "9639/9639 [==============================] - 3s 301us/step\n",
      "TN:132,FP:39,FN:12,TP:152,Macc:0.849379490516,F1:0.85633250695\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2610 - acc: 0.8810 - val_loss: 0.4505 - val_acc: 0.8341\n",
      "9639/9639 [==============================] - 3s 311us/step\n",
      "TN:147,FP:24,FN:31,TP:133,Macc:0.835312311435,F1:0.828654881996\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2571 - acc: 0.8836 - val_loss: 0.4342 - val_acc: 0.8303\n",
      "9639/9639 [==============================] - 3s 301us/step\n",
      "TN:127,FP:44,FN:13,TP:151,Macc:0.831710828132,F1:0.841220116236\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2555 - acc: 0.8846 - val_loss: 0.5016 - val_acc: 0.7781\n",
      "9639/9639 [==============================] - 3s 309us/step\n",
      "TN:136,FP:35,FN:34,TP:130,Macc:0.794002229964,F1:0.790268003475\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2564 - acc: 0.8846 - val_loss: 0.5729 - val_acc: 0.6625\n",
      "9639/9639 [==============================] - 3s 300us/step\n",
      "TN:154,FP:17,FN:90,TP:74,Macc:0.67590210966,F1:0.580387057576\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2536 - acc: 0.8853 - val_loss: 0.6672 - val_acc: 0.5921\n",
      "9639/9639 [==============================] - 3s 307us/step\n",
      "TN:160,FP:11,FN:114,TP:50,Macc:0.620275241381,F1:0.444440053613\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2515 - acc: 0.8866 - val_loss: 0.5439 - val_acc: 0.6772\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:151,FP:20,FN:78,TP:86,Macc:0.703715543799,F1:0.637031738973\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2517 - acc: 0.8861 - val_loss: 0.5468 - val_acc: 0.6866\n",
      "9639/9639 [==============================] - 3s 317us/step\n",
      "TN:146,FP:25,FN:72,TP:92,Macc:0.707388343399,F1:0.654798873108\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2492 - acc: 0.8861 - val_loss: 0.5461 - val_acc: 0.7106\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:138,FP:33,FN:52,TP:112,Macc:0.744972137704,F1:0.724913563075\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2501 - acc: 0.8867 - val_loss: 0.6996 - val_acc: 0.5948\n",
      "9639/9639 [==============================] - 3s 306us/step\n",
      "TN:159,FP:12,FN:110,TP:54,Macc:0.629546386094,F1:0.469560671045\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2490 - acc: 0.8873 - val_loss: 0.6766 - val_acc: 0.6017\n",
      "9639/9639 [==============================] - 3s 300us/step\n",
      "TN:161,FP:10,FN:108,TP:56,Macc:0.641491899501,F1:0.486951973728\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2476 - acc: 0.8873 - val_loss: 0.7845 - val_acc: 0.5764\n",
      "9639/9639 [==============================] - 3s 308us/step\n",
      "TN:161,FP:10,FN:115,TP:49,Macc:0.620150437518,F1:0.439457559453\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2483 - acc: 0.8873 - val_loss: 0.6491 - val_acc: 0.6215\n",
      "9639/9639 [==============================] - 3s 310us/step\n",
      "TN:154,FP:17,FN:99,TP:65,Macc:0.64846308711,F1:0.528450348449\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2458 - acc: 0.8888 - val_loss: 0.7134 - val_acc: 0.5975\n",
      "9639/9639 [==============================] - 3s 307us/step\n",
      "TN:159,FP:12,FN:108,TP:56,Macc:0.635643946661,F1:0.48275401669\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2472 - acc: 0.8879 - val_loss: 0.6517 - val_acc: 0.5939\n",
      "9639/9639 [==============================] - 3s 300us/step\n",
      "TN:161,FP:10,FN:111,TP:53,Macc:0.632345558651,F1:0.466955896022\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2450 - acc: 0.8892 - val_loss: 0.6609 - val_acc: 0.6141\n",
      "9639/9639 [==============================] - 3s 309us/step\n",
      "TN:156,FP:15,FN:102,TP:62,Macc:0.6451646991,F1:0.514517991401\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2443 - acc: 0.8884 - val_loss: 0.6260 - val_acc: 0.6257\n",
      "9639/9639 [==============================] - 3s 300us/step\n",
      "TN:159,FP:12,FN:102,TP:62,Macc:0.653936628361,F1:0.521003641734\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2452 - acc: 0.8889 - val_loss: 0.6730 - val_acc: 0.6098\n",
      "9639/9639 [==============================] - 3s 308us/step\n",
      "TN:157,FP:14,FN:103,TP:61,Macc:0.645039895237,F1:0.51045546679\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2434 - acc: 0.8899 - val_loss: 0.5625 - val_acc: 0.6728\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:153,FP:18,FN:81,TP:83,Macc:0.70041715579,F1:0.626409853229\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2452 - acc: 0.8893 - val_loss: 0.6707 - val_acc: 0.6140\n",
      "9639/9639 [==============================] - 3s 307us/step\n",
      "TN:158,FP:13,FN:105,TP:59,Macc:0.641866311091,F1:0.499995289256\n",
      "Loss: 0.665023\n",
      "Iteration No: 54 ended. Search finished for the next optimal point.\n",
      "Time taken: 2402.4678\n",
      "Function value obtained: 0.6650\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 55 started. Searching for the next optimal point.\n",
      "args [1, 1, 7]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.7628 - acc: 0.8429 - val_loss: 0.6624 - val_acc: 0.6891\n",
      "9639/9639 [==============================] - 18s 2ms/step\n",
      "TN:104,FP:67,FN:44,TP:120,Macc:0.669947181688,F1:0.683755164564\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.3062 - acc: 0.8586 - val_loss: 0.5149 - val_acc: 0.7738\n",
      "9639/9639 [==============================] - 3s 303us/step\n",
      "TN:113,FP:58,FN:23,TP:141,Macc:0.760287355418,F1:0.776854008219\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2934 - acc: 0.8622 - val_loss: 0.5005 - val_acc: 0.8026\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:100,FP:71,FN:0,TP:164,Macc:0.792397608473,F1:0.822049766707\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2918 - acc: 0.8623 - val_loss: 0.5784 - val_acc: 0.6584\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:152,FP:19,FN:92,TP:72,Macc:0.663956596253,F1:0.56470078442\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2841 - acc: 0.8664 - val_loss: 0.5956 - val_acc: 0.6749\n",
      "9639/9639 [==============================] - 3s 293us/step\n",
      "TN:142,FP:29,FN:71,TP:93,Macc:0.698741218002,F1:0.650344218979\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2771 - acc: 0.8719 - val_loss: 0.4779 - val_acc: 0.7792\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:126,FP:45,FN:30,TP:134,Macc:0.776957586896,F1:0.781335568314\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2732 - acc: 0.8729 - val_loss: 0.6156 - val_acc: 0.6845\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:142,FP:29,FN:71,TP:93,Macc:0.698741218002,F1:0.650344218979\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 49s 40ms/step - loss: 0.2706 - acc: 0.8751 - val_loss: 0.5050 - val_acc: 0.7552\n",
      "9639/9639 [==============================] - 3s 299us/step\n",
      "TN:129,FP:42,FN:37,TP:127,Macc:0.764388054173,F1:0.76275721365\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2653 - acc: 0.8782 - val_loss: 0.5960 - val_acc: 0.6935\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:143,FP:28,FN:71,TP:93,Macc:0.701665194422,F1:0.652626153815\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2658 - acc: 0.8778 - val_loss: 0.5516 - val_acc: 0.7223\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:141,FP:30,FN:59,TP:105,Macc:0.732402604981,F1:0.702335637228\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2623 - acc: 0.8795 - val_loss: 0.5173 - val_acc: 0.7446\n",
      "9639/9639 [==============================] - 3s 298us/step\n",
      "TN:135,FP:36,FN:43,TP:121,Macc:0.763639230994,F1:0.753888531984\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2623 - acc: 0.8789 - val_loss: 0.6285 - val_acc: 0.6918\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:142,FP:29,FN:62,TP:102,Macc:0.726180240551,F1:0.691519941025\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2619 - acc: 0.8803 - val_loss: 0.5035 - val_acc: 0.7536\n",
      "9639/9639 [==============================] - 3s 300us/step\n",
      "TN:136,FP:35,FN:43,TP:121,Macc:0.766563207414,F1:0.756244451485\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2603 - acc: 0.8812 - val_loss: 0.5850 - val_acc: 0.7033\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:142,FP:29,FN:60,TP:104,Macc:0.732277801118,F1:0.700331208422\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2593 - acc: 0.8826 - val_loss: 0.7575 - val_acc: 0.6545\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:143,FP:28,FN:82,TP:82,Macc:0.668128611305,F1:0.598534811598\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2574 - acc: 0.8794 - val_loss: 0.5623 - val_acc: 0.7401\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:141,FP:30,FN:52,TP:112,Macc:0.753744066965,F1:0.732020619632\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2582 - acc: 0.8807 - val_loss: 0.4975 - val_acc: 0.7696\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:144,FP:27,FN:44,TP:120,Macc:0.786906238491,F1:0.771698641947\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2567 - acc: 0.8824 - val_loss: 0.5050 - val_acc: 0.7598\n",
      "9639/9639 [==============================] - 3s 306us/step\n",
      "TN:137,FP:34,FN:43,TP:121,Macc:0.769487183834,F1:0.758615141754\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2544 - acc: 0.8829 - val_loss: 0.5674 - val_acc: 0.7222\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:144,FP:27,FN:61,TP:103,Macc:0.735076973675,F1:0.700674793276\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2545 - acc: 0.8832 - val_loss: 0.5851 - val_acc: 0.7099\n",
      "9639/9639 [==============================] - 3s 297us/step\n",
      "TN:141,FP:30,FN:59,TP:105,Macc:0.732402604981,F1:0.702335637228\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2536 - acc: 0.8839 - val_loss: 0.5210 - val_acc: 0.7424\n",
      "9639/9639 [==============================] - 3s 293us/step\n",
      "TN:135,FP:36,FN:48,TP:116,Macc:0.748395329577,F1:0.734171672049\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2521 - acc: 0.8852 - val_loss: 0.6262 - val_acc: 0.6960\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:143,FP:28,FN:68,TP:96,Macc:0.710811535272,F1:0.66666122188\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2536 - acc: 0.8851 - val_loss: 0.5731 - val_acc: 0.7191\n",
      "9639/9639 [==============================] - 3s 295us/step\n",
      "TN:143,FP:28,FN:60,TP:104,Macc:0.735201777538,F1:0.702697214797\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2512 - acc: 0.8847 - val_loss: 0.4883 - val_acc: 0.7664\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:138,FP:33,FN:43,TP:121,Macc:0.772411160254,F1:0.761000742138\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2511 - acc: 0.8845 - val_loss: 0.4970 - val_acc: 0.7532\n",
      "9639/9639 [==============================] - 3s 296us/step\n",
      "TN:138,FP:33,FN:47,TP:117,Macc:0.760216039121,F1:0.745217388697\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2512 - acc: 0.8852 - val_loss: 0.5424 - val_acc: 0.7425\n",
      "9639/9639 [==============================] - 3s 294us/step\n",
      "TN:139,FP:32,FN:49,TP:115,Macc:0.757042454974,F1:0.739544303387\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2515 - acc: 0.8853 - val_loss: 0.6979 - val_acc: 0.6610\n",
      "9639/9639 [==============================] - 3s 290us/step\n",
      "TN:143,FP:28,FN:78,TP:86,Macc:0.680323732439,F1:0.61869966497\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2495 - acc: 0.8867 - val_loss: 0.5966 - val_acc: 0.7047\n",
      "9639/9639 [==============================] - 3s 291us/step\n",
      "TN:142,FP:29,FN:59,TP:105,Macc:0.735326581401,F1:0.704692490335\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2499 - acc: 0.8860 - val_loss: 0.5199 - val_acc: 0.7545\n",
      "9639/9639 [==============================] - 3s 301us/step\n",
      "TN:140,FP:31,FN:46,TP:118,Macc:0.769112772244,F1:0.753988069899\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 48s 40ms/step - loss: 0.2493 - acc: 0.8867 - val_loss: 0.6501 - val_acc: 0.6947\n",
      "9639/9639 [==============================] - 3s 292us/step\n",
      "TN:144,FP:27,FN:65,TP:99,Macc:0.722881852542,F1:0.682753163372\n",
      "Loss: 0.644291\n",
      "Iteration No: 55 ended. Search finished for the next optimal point.\n",
      "Time taken: 2393.8271\n",
      "Function value obtained: 0.6443\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 56 started. Searching for the next optimal point.\n",
      "args [5, 2, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 63s 53ms/step - loss: 0.7498 - acc: 0.8538 - val_loss: 3.1051 - val_acc: 0.4730\n",
      "9639/9639 [==============================] - 19s 2ms/step\n",
      "TN:168,FP:3,FN:159,TP:5,Macc:0.506471939993,F1:0.0581385518\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2983 - acc: 0.8731 - val_loss: 3.2457 - val_acc: 0.4739\n",
      "9639/9639 [==============================] - 4s 377us/step\n",
      "TN:171,FP:0,FN:161,TP:3,Macc:0.509146308686,F1:0.0359277508724\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2815 - acc: 0.8784 - val_loss: 4.1674 - val_acc: 0.4708\n",
      "9639/9639 [==============================] - 3s 352us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2745 - acc: 0.8813 - val_loss: 5.5991 - val_acc: 0.4712\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2701 - acc: 0.8833 - val_loss: 2.8631 - val_acc: 0.4720\n",
      "9639/9639 [==============================] - 3s 361us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2624 - acc: 0.8866 - val_loss: 3.3970 - val_acc: 0.4798\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:171,FP:0,FN:158,TP:6,Macc:0.518292649536,F1:0.0705874771001\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2599 - acc: 0.8877 - val_loss: 5.2459 - val_acc: 0.4716\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2579 - acc: 0.8887 - val_loss: 4.1265 - val_acc: 0.4724\n",
      "9639/9639 [==============================] - 3s 355us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2553 - acc: 0.8899 - val_loss: 3.9354 - val_acc: 0.4700\n",
      "9639/9639 [==============================] - 3s 358us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2538 - acc: 0.8903 - val_loss: 3.5306 - val_acc: 0.4724\n",
      "9639/9639 [==============================] - 3s 351us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2518 - acc: 0.8895 - val_loss: 2.7701 - val_acc: 0.4754\n",
      "9639/9639 [==============================] - 3s 362us/step\n",
      "TN:171,FP:0,FN:161,TP:3,Macc:0.509146308686,F1:0.0359277508724\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2503 - acc: 0.8919 - val_loss: 3.6400 - val_acc: 0.4706\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2481 - acc: 0.8919 - val_loss: 4.3819 - val_acc: 0.4698\n",
      "9639/9639 [==============================] - 3s 358us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2474 - acc: 0.8914 - val_loss: 3.6723 - val_acc: 0.4711\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2464 - acc: 0.8928 - val_loss: 4.3483 - val_acc: 0.4702\n",
      "9639/9639 [==============================] - 3s 360us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2450 - acc: 0.8933 - val_loss: 4.1248 - val_acc: 0.4698\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2436 - acc: 0.8944 - val_loss: 3.4113 - val_acc: 0.4704\n",
      "9639/9639 [==============================] - 3s 360us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2434 - acc: 0.8934 - val_loss: 4.3073 - val_acc: 0.4700\n",
      "9639/9639 [==============================] - 4s 367us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2431 - acc: 0.8942 - val_loss: 4.3051 - val_acc: 0.4702\n",
      "9639/9639 [==============================] - 3s 356us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2409 - acc: 0.8945 - val_loss: 4.2847 - val_acc: 0.4700\n",
      "9639/9639 [==============================] - 3s 357us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2405 - acc: 0.8951 - val_loss: 4.2606 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2392 - acc: 0.8955 - val_loss: 3.9022 - val_acc: 0.4698\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2377 - acc: 0.8960 - val_loss: 3.3436 - val_acc: 0.4704\n",
      "9639/9639 [==============================] - 3s 361us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2389 - acc: 0.8965 - val_loss: 3.7036 - val_acc: 0.4703\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2377 - acc: 0.8956 - val_loss: 3.8807 - val_acc: 0.4694\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2366 - acc: 0.8962 - val_loss: 3.9427 - val_acc: 0.4697\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2359 - acc: 0.8967 - val_loss: 3.8462 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 3s 358us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2347 - acc: 0.8982 - val_loss: 3.9686 - val_acc: 0.4702\n",
      "9639/9639 [==============================] - 3s 352us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2354 - acc: 0.8966 - val_loss: 4.4175 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 3s 360us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2345 - acc: 0.8969 - val_loss: 3.5197 - val_acc: 0.4703\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Loss: 3.510651\n",
      "Iteration No: 56 ended. Search finished for the next optimal point.\n",
      "Time taken: 2571.7162\n",
      "Function value obtained: 3.5107\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 57 started. Searching for the next optimal point.\n",
      "args [1, 3, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 60s 50ms/step - loss: 0.7726 - acc: 0.8503 - val_loss: 0.5598 - val_acc: 0.8017\n",
      "9639/9639 [==============================] - 20s 2ms/step\n",
      "TN:116,FP:55,FN:22,TP:142,Macc:0.772108064962,F1:0.786698099162\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.3154 - acc: 0.8667 - val_loss: 1.3047 - val_acc: 0.5315\n",
      "9639/9639 [==============================] - 3s 352us/step\n",
      "TN:153,FP:18,FN:139,TP:25,Macc:0.523587899357,F1:0.241542247388\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2936 - acc: 0.8760 - val_loss: 0.4846 - val_acc: 0.8111\n",
      "9639/9639 [==============================] - 3s 337us/step\n",
      "TN:115,FP:56,FN:16,TP:148,Macc:0.787476770241,F1:0.804342343019\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2783 - acc: 0.8817 - val_loss: 0.5350 - val_acc: 0.8032\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:119,FP:52,FN:21,TP:143,Macc:0.783928774505,F1:0.796651873843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2717 - acc: 0.8835 - val_loss: 0.4499 - val_acc: 0.8255\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:117,FP:54,FN:8,TP:156,Macc:0.817714965348,F1:0.834219133096\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2625 - acc: 0.8879 - val_loss: 0.4564 - val_acc: 0.8159\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:107,FP:64,FN:7,TP:157,Macc:0.79152398143,F1:0.815578989572\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2576 - acc: 0.8898 - val_loss: 0.5206 - val_acc: 0.7600\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:94,FP:77,FN:3,TP:161,Macc:0.765707409102,F1:0.800989667444\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2530 - acc: 0.8913 - val_loss: 0.4854 - val_acc: 0.8016\n",
      "9639/9639 [==============================] - 3s 337us/step\n",
      "TN:108,FP:63,FN:9,TP:155,Macc:0.788350397284,F1:0.811512887813\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2503 - acc: 0.8922 - val_loss: 0.4719 - val_acc: 0.7845\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:135,FP:36,FN:37,TP:127,Macc:0.781931912694,F1:0.776752857617\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2475 - acc: 0.8942 - val_loss: 0.7867 - val_acc: 0.6976\n",
      "9639/9639 [==============================] - 3s 337us/step\n",
      "TN:149,FP:22,FN:68,TP:96,Macc:0.728355393792,F1:0.680845657102\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2441 - acc: 0.8956 - val_loss: 0.8879 - val_acc: 0.6810\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:142,FP:29,FN:65,TP:99,Macc:0.717033899702,F1:0.678076724335\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2402 - acc: 0.8971 - val_loss: 0.6068 - val_acc: 0.7760\n",
      "9639/9639 [==============================] - 3s 338us/step\n",
      "TN:145,FP:26,FN:47,TP:117,Macc:0.780683874061,F1:0.762209454866\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2383 - acc: 0.8978 - val_loss: 0.8622 - val_acc: 0.6894\n",
      "9639/9639 [==============================] - 3s 334us/step\n",
      "TN:152,FP:19,FN:72,TP:92,Macc:0.724932201919,F1:0.669085559896\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2355 - acc: 0.8987 - val_loss: 0.4469 - val_acc: 0.8215\n",
      "9639/9639 [==============================] - 3s 337us/step\n",
      "TN:138,FP:33,FN:23,TP:141,Macc:0.83338676592,F1:0.834313977173\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2328 - acc: 0.9003 - val_loss: 0.4244 - val_acc: 0.8152\n",
      "9639/9639 [==============================] - 3s 339us/step\n",
      "TN:134,FP:37,FN:25,TP:139,Macc:0.815593299673,F1:0.817641512805\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2326 - acc: 0.8994 - val_loss: 0.4807 - val_acc: 0.7914\n",
      "9639/9639 [==============================] - 3s 339us/step\n",
      "TN:137,FP:34,FN:36,TP:128,Macc:0.790828645817,F1:0.785270520871\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2300 - acc: 0.9023 - val_loss: 0.6633 - val_acc: 0.7439\n",
      "9639/9639 [==============================] - 3s 334us/step\n",
      "TN:141,FP:30,FN:61,TP:103,Macc:0.726305044415,F1:0.693597202188\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2301 - acc: 0.9018 - val_loss: 0.4072 - val_acc: 0.8494\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:130,FP:41,FN:13,TP:151,Macc:0.840482757393,F1:0.848309088376\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2270 - acc: 0.9028 - val_loss: 0.4714 - val_acc: 0.8038\n",
      "9639/9639 [==============================] - 3s 334us/step\n",
      "TN:140,FP:31,FN:30,TP:134,Macc:0.817893256777,F1:0.81458411127\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2260 - acc: 0.9029 - val_loss: 0.3216 - val_acc: 0.8771\n",
      "9639/9639 [==============================] - 3s 338us/step\n",
      "TN:127,FP:44,FN:5,TP:159,Macc:0.856101070399,F1:0.866479523826\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2241 - acc: 0.9047 - val_loss: 0.5197 - val_acc: 0.7801\n",
      "9639/9639 [==============================] - 3s 337us/step\n",
      "TN:139,FP:32,FN:40,TP:124,Macc:0.784481477524,F1:0.774994450195\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2239 - acc: 0.9042 - val_loss: 0.4217 - val_acc: 0.8119\n",
      "9639/9639 [==============================] - 3s 339us/step\n",
      "TN:141,FP:30,FN:26,TP:138,Macc:0.833012354331,F1:0.831319746952\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2220 - acc: 0.9062 - val_loss: 0.3593 - val_acc: 0.8564\n",
      "9639/9639 [==============================] - 3s 339us/step\n",
      "TN:135,FP:36,FN:8,TP:156,Macc:0.87034654091,F1:0.87639897428\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2205 - acc: 0.9054 - val_loss: 0.6752 - val_acc: 0.7557\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:140,FP:31,FN:51,TP:113,Macc:0.753868870828,F1:0.733760704586\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2210 - acc: 0.9050 - val_loss: 0.6572 - val_acc: 0.7435\n",
      "9639/9639 [==============================] - 3s 336us/step\n",
      "TN:149,FP:22,FN:57,TP:107,Macc:0.761891976909,F1:0.730369950302\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2188 - acc: 0.9058 - val_loss: 0.3334 - val_acc: 0.8628\n",
      "9639/9639 [==============================] - 3s 337us/step\n",
      "TN:134,FP:37,FN:6,TP:158,Macc:0.873520125056,F1:0.880217328329\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2182 - acc: 0.9066 - val_loss: 0.3425 - val_acc: 0.8670\n",
      "9639/9639 [==============================] - 3s 337us/step\n",
      "TN:124,FP:47,FN:3,TP:161,Macc:0.853426701705,F1:0.865585923638\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2169 - acc: 0.9072 - val_loss: 0.4246 - val_acc: 0.8171\n",
      "9639/9639 [==============================] - 3s 339us/step\n",
      "TN:137,FP:34,FN:25,TP:139,Macc:0.824365228934,F1:0.82492026613\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2176 - acc: 0.9074 - val_loss: 0.3351 - val_acc: 0.8641\n",
      "9639/9639 [==============================] - 3s 338us/step\n",
      "TN:131,FP:40,FN:5,TP:159,Macc:0.867796976079,F1:0.876027555923\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2152 - acc: 0.9076 - val_loss: 0.6332 - val_acc: 0.7589\n",
      "9639/9639 [==============================] - 3s 336us/step\n",
      "TN:155,FP:16,FN:62,TP:102,Macc:0.764191934013,F1:0.723398845269\n",
      "Loss: 0.618654\n",
      "Iteration No: 57 ended. Search finished for the next optimal point.\n",
      "Time taken: 2549.6450\n",
      "Function value obtained: 0.6187\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 58 started. Searching for the next optimal point.\n",
      "args [1, 3, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 60s 50ms/step - loss: 0.7524 - acc: 0.8509 - val_loss: 0.5807 - val_acc: 0.7684\n",
      "9639/9639 [==============================] - 20s 2ms/step\n",
      "TN:127,FP:44,FN:47,TP:117,Macc:0.7280522985,F1:0.719994451772\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2985 - acc: 0.8672 - val_loss: 0.9351 - val_acc: 0.5455\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:151,FP:20,FN:121,TP:43,Macc:0.572617991617,F1:0.378850177699\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2844 - acc: 0.8705 - val_loss: 0.5792 - val_acc: 0.6588\n",
      "9639/9639 [==============================] - 3s 326us/step\n",
      "TN:145,FP:26,FN:85,TP:79,Macc:0.664830223296,F1:0.587355311388\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2733 - acc: 0.8799 - val_loss: 1.2480 - val_acc: 0.4990\n",
      "9639/9639 [==============================] - 3s 327us/step\n",
      "TN:163,FP:8,FN:144,TP:20,Macc:0.537583762142,F1:0.208330569046\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2670 - acc: 0.8832 - val_loss: 1.1810 - val_acc: 0.5107\n",
      "9639/9639 [==============================] - 3s 326us/step\n",
      "TN:167,FP:4,FN:143,TP:21,Macc:0.552328448105,F1:0.222219671258\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2593 - acc: 0.8882 - val_loss: 1.9115 - val_acc: 0.4792\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:171,FP:0,FN:157,TP:7,Macc:0.521341429819,F1:0.0818704707853\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2540 - acc: 0.8894 - val_loss: 1.8465 - val_acc: 0.4887\n",
      "9639/9639 [==============================] - 3s 334us/step\n",
      "TN:171,FP:0,FN:152,TP:12,Macc:0.536585331236,F1:0.136362221604\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2500 - acc: 0.8923 - val_loss: 2.7629 - val_acc: 0.4730\n",
      "9639/9639 [==============================] - 3s 327us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2487 - acc: 0.8924 - val_loss: 3.5357 - val_acc: 0.4702\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2441 - acc: 0.8940 - val_loss: 1.7249 - val_acc: 0.4993\n",
      "9639/9639 [==============================] - 3s 324us/step\n",
      "TN:166,FP:5,FN:144,TP:20,Macc:0.546355691402,F1:0.211637661909\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2417 - acc: 0.8952 - val_loss: 1.7916 - val_acc: 0.4935\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:168,FP:3,FN:148,TP:16,Macc:0.540008523109,F1:0.174861319979\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2389 - acc: 0.8969 - val_loss: 2.2176 - val_acc: 0.4866\n",
      "9639/9639 [==============================] - 3s 326us/step\n",
      "TN:165,FP:6,FN:151,TP:13,Macc:0.522090252999,F1:0.14207443868\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2390 - acc: 0.8957 - val_loss: 1.9046 - val_acc: 0.4835\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:170,FP:1,FN:154,TP:10,Macc:0.527563794249,F1:0.114284403997\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2369 - acc: 0.8976 - val_loss: 1.4259 - val_acc: 0.4905\n",
      "9639/9639 [==============================] - 3s 324us/step\n",
      "TN:170,FP:1,FN:149,TP:15,Macc:0.542807695666,F1:0.166664864586\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2369 - acc: 0.8961 - val_loss: 1.7027 - val_acc: 0.4934\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:167,FP:4,FN:147,TP:17,Macc:0.540133326972,F1:0.183781548134\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2334 - acc: 0.8974 - val_loss: 1.1699 - val_acc: 0.5155\n",
      "9639/9639 [==============================] - 3s 324us/step\n",
      "TN:165,FP:6,FN:143,TP:21,Macc:0.546480495265,F1:0.21989259234\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2337 - acc: 0.8991 - val_loss: 2.2500 - val_acc: 0.4766\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:171,FP:0,FN:160,TP:4,Macc:0.51219508897,F1:0.0476185300495\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2320 - acc: 0.8997 - val_loss: 1.6824 - val_acc: 0.4911\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:164,FP:7,FN:148,TP:16,Macc:0.528312617429,F1:0.171120601479\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2314 - acc: 0.8991 - val_loss: 1.5141 - val_acc: 0.5146\n",
      "9639/9639 [==============================] - 3s 322us/step\n",
      "TN:160,FP:11,FN:138,TP:26,Macc:0.547104514581,F1:0.258703135113\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2292 - acc: 0.9001 - val_loss: 2.6226 - val_acc: 0.4767\n",
      "9639/9639 [==============================] - 3s 324us/step\n",
      "TN:171,FP:0,FN:158,TP:6,Macc:0.518292649536,F1:0.0705874771001\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2284 - acc: 0.9005 - val_loss: 1.7467 - val_acc: 0.5132\n",
      "9639/9639 [==============================] - 3s 323us/step\n",
      "TN:161,FP:10,FN:136,TP:28,Macc:0.556126051568,F1:0.277224332557\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2284 - acc: 0.9002 - val_loss: 2.2121 - val_acc: 0.4915\n",
      "9639/9639 [==============================] - 3s 328us/step\n",
      "TN:167,FP:4,FN:146,TP:18,Macc:0.543182107256,F1:0.193546069861\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2268 - acc: 0.9014 - val_loss: 1.6669 - val_acc: 0.5046\n",
      "9639/9639 [==============================] - 3s 323us/step\n",
      "TN:161,FP:10,FN:142,TP:22,Macc:0.537833369868,F1:0.224486765346\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2274 - acc: 0.9007 - val_loss: 2.0471 - val_acc: 0.4921\n",
      "9639/9639 [==============================] - 3s 327us/step\n",
      "TN:171,FP:0,FN:155,TP:9,Macc:0.527438990386,F1:0.104045144586\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2251 - acc: 0.9012 - val_loss: 2.3053 - val_acc: 0.5015\n",
      "9639/9639 [==============================] - 3s 326us/step\n",
      "TN:166,FP:5,FN:144,TP:20,Macc:0.546355691402,F1:0.211637661909\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2254 - acc: 0.9011 - val_loss: 0.8716 - val_acc: 0.6011\n",
      "9639/9639 [==============================] - 3s 328us/step\n",
      "TN:153,FP:18,FN:97,TP:67,Macc:0.651636671257,F1:0.538147616568\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2239 - acc: 0.9019 - val_loss: 1.6060 - val_acc: 0.5202\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:162,FP:9,FN:133,TP:31,Macc:0.568196368838,F1:0.303918067994\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2241 - acc: 0.9039 - val_loss: 1.7062 - val_acc: 0.5088\n",
      "9639/9639 [==============================] - 3s 327us/step\n",
      "TN:166,FP:5,FN:142,TP:22,Macc:0.552453251969,F1:0.230363795321\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2242 - acc: 0.9019 - val_loss: 1.3854 - val_acc: 0.5368\n",
      "9639/9639 [==============================] - 3s 324us/step\n",
      "TN:163,FP:8,FN:132,TP:32,Macc:0.574169125542,F1:0.313721988504\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2219 - acc: 0.9032 - val_loss: 1.9618 - val_acc: 0.5037\n",
      "9639/9639 [==============================] - 3s 326us/step\n",
      "TN:165,FP:6,FN:142,TP:22,Macc:0.549529275549,F1:0.229163899989\n",
      "Loss: 1.953056\n",
      "Iteration No: 58 ended. Search finished for the next optimal point.\n",
      "Time taken: 2587.7844\n",
      "Function value obtained: 1.9531\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 59 started. Searching for the next optimal point.\n",
      "args [5, 3, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 104s 87ms/step - loss: 0.9122 - acc: 0.8404 - val_loss: 0.6393 - val_acc: 0.8085\n",
      "9639/9639 [==============================] - 24s 2ms/step\n",
      "TN:106,FP:65,FN:5,TP:159,Macc:0.794697565577,F1:0.819582213953\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.3497 - acc: 0.8590 - val_loss: 0.6702 - val_acc: 0.6475\n",
      "9639/9639 [==============================] - 6s 619us/step\n",
      "TN:49,FP:122,FN:2,TP:162,Macc:0.637177250481,F1:0.723209144846\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 89s 74ms/step - loss: 0.3122 - acc: 0.8700 - val_loss: 0.6357 - val_acc: 0.6732\n",
      "9639/9639 [==============================] - 6s 639us/step\n",
      "TN:63,FP:108,FN:8,TP:156,Macc:0.659820238663,F1:0.728966725428\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2973 - acc: 0.8747 - val_loss: 0.6902 - val_acc: 0.5978\n",
      "9639/9639 [==============================] - 6s 631us/step\n",
      "TN:23,FP:148,FN:0,TP:164,Macc:0.567251424126,F1:0.689070630147\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2881 - acc: 0.8784 - val_loss: 0.6464 - val_acc: 0.6503\n",
      "9639/9639 [==============================] - 6s 649us/step\n",
      "TN:46,FP:125,FN:0,TP:164,Macc:0.634502881788,F1:0.724056693806\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2797 - acc: 0.8816 - val_loss: 1.4549 - val_acc: 0.4971\n",
      "9639/9639 [==============================] - 6s 637us/step\n",
      "TN:171,FP:0,FN:155,TP:9,Macc:0.527438990386,F1:0.104045144586\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2695 - acc: 0.8867 - val_loss: 0.5973 - val_acc: 0.7674\n",
      "9639/9639 [==============================] - 6s 644us/step\n",
      "TN:159,FP:12,FN:56,TP:108,Macc:0.794180521393,F1:0.76055795342\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2609 - acc: 0.8905 - val_loss: 1.8486 - val_acc: 0.4955\n",
      "9639/9639 [==============================] - 6s 634us/step\n",
      "TN:171,FP:0,FN:153,TP:11,Macc:0.533536550953,F1:0.125712973988\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2564 - acc: 0.8921 - val_loss: 1.5260 - val_acc: 0.4942\n",
      "9639/9639 [==============================] - 6s 631us/step\n",
      "TN:171,FP:0,FN:156,TP:8,Macc:0.524390210103,F1:0.0930222682622\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2521 - acc: 0.8949 - val_loss: 0.4510 - val_acc: 0.8082\n",
      "9639/9639 [==============================] - 6s 637us/step\n",
      "TN:106,FP:65,FN:1,TP:163,Macc:0.80689268671,F1:0.831627253028\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2487 - acc: 0.8956 - val_loss: 1.6215 - val_acc: 0.5374\n",
      "9639/9639 [==============================] - 6s 644us/step\n",
      "TN:170,FP:1,FN:133,TP:31,Macc:0.591588180199,F1:0.31632348972\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2459 - acc: 0.8977 - val_loss: 0.9269 - val_acc: 0.6110\n",
      "9639/9639 [==============================] - 6s 646us/step\n",
      "TN:170,FP:1,FN:115,TP:49,Macc:0.646466225299,F1:0.45793993898\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2424 - acc: 0.8974 - val_loss: 1.5012 - val_acc: 0.4931\n",
      "9639/9639 [==============================] - 6s 647us/step\n",
      "TN:171,FP:0,FN:154,TP:10,Macc:0.530487770669,F1:0.114941322511\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2409 - acc: 0.8993 - val_loss: 1.9781 - val_acc: 0.5104\n",
      "9639/9639 [==============================] - 6s 649us/step\n",
      "TN:171,FP:0,FN:148,TP:16,Macc:0.548780452369,F1:0.177775974338\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2388 - acc: 0.8995 - val_loss: 1.7815 - val_acc: 0.4946\n",
      "9639/9639 [==============================] - 6s 626us/step\n",
      "TN:171,FP:0,FN:153,TP:11,Macc:0.533536550953,F1:0.125712973988\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2374 - acc: 0.9000 - val_loss: 1.8428 - val_acc: 0.4966\n",
      "9639/9639 [==============================] - 6s 643us/step\n",
      "TN:171,FP:0,FN:153,TP:11,Macc:0.533536550953,F1:0.125712973988\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2347 - acc: 0.9023 - val_loss: 0.4293 - val_acc: 0.8225\n",
      "9639/9639 [==============================] - 6s 652us/step\n",
      "TN:161,FP:10,FN:40,TP:124,Macc:0.848808958766,F1:0.832209259438\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2343 - acc: 0.9016 - val_loss: 0.6257 - val_acc: 0.7241\n",
      "9639/9639 [==============================] - 6s 644us/step\n",
      "TN:169,FP:2,FN:78,TP:86,Macc:0.756347119361,F1:0.682534623241\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2309 - acc: 0.9028 - val_loss: 2.6669 - val_acc: 0.4930\n",
      "9639/9639 [==============================] - 6s 654us/step\n",
      "TN:171,FP:0,FN:157,TP:7,Macc:0.521341429819,F1:0.0818704707853\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 91s 75ms/step - loss: 0.2304 - acc: 0.9033 - val_loss: 3.3109 - val_acc: 0.4867\n",
      "9639/9639 [==============================] - 6s 647us/step\n",
      "TN:171,FP:0,FN:159,TP:5,Macc:0.515243869253,F1:0.0591709583053\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2302 - acc: 0.9038 - val_loss: 1.8679 - val_acc: 0.4968\n",
      "9639/9639 [==============================] - 6s 639us/step\n",
      "TN:171,FP:0,FN:152,TP:12,Macc:0.536585331236,F1:0.136362221604\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2268 - acc: 0.9040 - val_loss: 0.7987 - val_acc: 0.5626\n",
      "9639/9639 [==============================] - 6s 641us/step\n",
      "TN:171,FP:0,FN:135,TP:29,Macc:0.588414596052,F1:0.300515291497\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2267 - acc: 0.9041 - val_loss: 0.6499 - val_acc: 0.7318\n",
      "9639/9639 [==============================] - 6s 636us/step\n",
      "TN:165,FP:6,FN:80,TP:84,Macc:0.738553653114,F1:0.661412232415\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2260 - acc: 0.9058 - val_loss: 1.2078 - val_acc: 0.5483\n",
      "9639/9639 [==============================] - 6s 647us/step\n",
      "TN:170,FP:1,FN:138,TP:26,Macc:0.576344278782,F1:0.272248607246\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2257 - acc: 0.9056 - val_loss: 0.9310 - val_acc: 0.5414\n",
      "9639/9639 [==============================] - 6s 626us/step\n",
      "TN:171,FP:0,FN:140,TP:24,Macc:0.573170694636,F1:0.255316669104\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2248 - acc: 0.9060 - val_loss: 2.2205 - val_acc: 0.4981\n",
      "9639/9639 [==============================] - 6s 649us/step\n",
      "TN:171,FP:0,FN:152,TP:12,Macc:0.536585331236,F1:0.136362221604\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2226 - acc: 0.9066 - val_loss: 1.6236 - val_acc: 0.4993\n",
      "9639/9639 [==============================] - 6s 647us/step\n",
      "TN:171,FP:0,FN:152,TP:12,Macc:0.536585331236,F1:0.136362221604\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 91s 75ms/step - loss: 0.2213 - acc: 0.9079 - val_loss: 0.7703 - val_acc: 0.5793\n",
      "9639/9639 [==============================] - 6s 623us/step\n",
      "TN:171,FP:0,FN:130,TP:34,Macc:0.603658497469,F1:0.343431176234\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2212 - acc: 0.9083 - val_loss: 1.3952 - val_acc: 0.5050\n",
      "9639/9639 [==============================] - 6s 637us/step\n",
      "TN:171,FP:0,FN:152,TP:12,Macc:0.536585331236,F1:0.136362221604\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.2197 - acc: 0.9083 - val_loss: 1.8577 - val_acc: 0.5097\n",
      "9639/9639 [==============================] - 6s 637us/step\n",
      "TN:171,FP:0,FN:152,TP:12,Macc:0.536585331236,F1:0.136362221604\n",
      "Loss: 1.842236\n",
      "Iteration No: 59 ended. Search finished for the next optimal point.\n",
      "Time taken: 3943.6634\n",
      "Function value obtained: 1.8422\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 60 started. Searching for the next optimal point.\n",
      "args [1, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 61s 50ms/step - loss: 0.7587 - acc: 0.8419 - val_loss: 0.5890 - val_acc: 0.6988\n",
      "9639/9639 [==============================] - 21s 2ms/step\n",
      "TN:152,FP:19,FN:85,TP:79,Macc:0.685298058236,F1:0.603048233538\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.3095 - acc: 0.8560 - val_loss: 0.5844 - val_acc: 0.6674\n",
      "9639/9639 [==============================] - 3s 333us/step\n",
      "TN:145,FP:26,FN:82,TP:82,Macc:0.673976564145,F1:0.602935860881\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2879 - acc: 0.8697 - val_loss: 0.4758 - val_acc: 0.7948\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:157,FP:14,FN:53,TP:111,Macc:0.797478909403,F1:0.768160631687\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2785 - acc: 0.8743 - val_loss: 0.4480 - val_acc: 0.8408\n",
      "9639/9639 [==============================] - 3s 336us/step\n",
      "TN:146,FP:25,FN:32,TP:132,Macc:0.829339554731,F1:0.822424352828\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2766 - acc: 0.8749 - val_loss: 0.7121 - val_acc: 0.6350\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:150,FP:21,FN:98,TP:66,Macc:0.639815961713,F1:0.525891385898\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2688 - acc: 0.8792 - val_loss: 0.5723 - val_acc: 0.6923\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:147,FP:24,FN:75,TP:89,Macc:0.701165978969,F1:0.642593913427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2635 - acc: 0.8807 - val_loss: 0.5572 - val_acc: 0.7201\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:143,FP:28,FN:68,TP:96,Macc:0.710811535272,F1:0.66666122188\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2624 - acc: 0.8818 - val_loss: 0.5788 - val_acc: 0.7035\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:159,FP:12,FN:89,TP:75,Macc:0.693570772044,F1:0.597604527017\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2578 - acc: 0.8822 - val_loss: 0.4744 - val_acc: 0.7698\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:147,FP:24,FN:57,TP:107,Macc:0.756044024069,F1:0.72541824358\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2561 - acc: 0.8837 - val_loss: 0.5779 - val_acc: 0.6990\n",
      "9639/9639 [==============================] - 3s 336us/step\n",
      "TN:156,FP:15,FN:84,TP:80,Macc:0.7000427442,F1:0.617755455685\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2558 - acc: 0.8845 - val_loss: 0.4686 - val_acc: 0.7713\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:149,FP:22,FN:54,TP:110,Macc:0.771038317759,F1:0.743237752322\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2535 - acc: 0.8844 - val_loss: 0.7422 - val_acc: 0.6172\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:161,FP:10,FN:115,TP:49,Macc:0.620150437518,F1:0.439457559453\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2538 - acc: 0.8853 - val_loss: 0.5060 - val_acc: 0.7409\n",
      "9639/9639 [==============================] - 3s 336us/step\n",
      "TN:151,FP:20,FN:69,TP:95,Macc:0.731154566349,F1:0.680998200219\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2504 - acc: 0.8859 - val_loss: 0.4813 - val_acc: 0.7573\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:145,FP:26,FN:52,TP:112,Macc:0.765439972645,F1:0.741716341078\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2507 - acc: 0.8868 - val_loss: 0.5183 - val_acc: 0.7234\n",
      "9639/9639 [==============================] - 3s 326us/step\n",
      "TN:159,FP:12,FN:76,TP:88,Macc:0.733204915727,F1:0.666661434384\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2470 - acc: 0.8891 - val_loss: 0.5603 - val_acc: 0.7041\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:150,FP:21,FN:67,TP:97,Macc:0.734328150496,F1:0.687937855129\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2466 - acc: 0.8890 - val_loss: 0.6069 - val_acc: 0.6944\n",
      "9639/9639 [==============================] - 3s 327us/step\n",
      "TN:154,FP:17,FN:83,TP:81,Macc:0.697243571643,F1:0.618315407828\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2451 - acc: 0.8892 - val_loss: 0.4687 - val_acc: 0.7628\n",
      "9639/9639 [==============================] - 3s 326us/step\n",
      "TN:146,FP:25,FN:49,TP:115,Macc:0.777510289915,F1:0.756573426935\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2457 - acc: 0.8903 - val_loss: 0.4707 - val_acc: 0.7688\n",
      "9639/9639 [==============================] - 3s 324us/step\n",
      "TN:145,FP:26,FN:45,TP:119,Macc:0.786781434628,F1:0.770221003212\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2426 - acc: 0.8906 - val_loss: 0.5044 - val_acc: 0.7459\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:155,FP:16,FN:72,TP:92,Macc:0.73370413118,F1:0.676465266693\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2425 - acc: 0.8904 - val_loss: 0.4677 - val_acc: 0.7616\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:153,FP:18,FN:57,TP:107,Macc:0.773587882589,F1:0.740478972896\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2405 - acc: 0.8926 - val_loss: 0.4435 - val_acc: 0.7818\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:151,FP:20,FN:55,TP:109,Macc:0.773837490316,F1:0.744021826409\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2389 - acc: 0.8934 - val_loss: 0.4459 - val_acc: 0.7784\n",
      "9639/9639 [==============================] - 3s 326us/step\n",
      "TN:147,FP:24,FN:54,TP:110,Macc:0.765190364918,F1:0.738249534836\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2391 - acc: 0.8920 - val_loss: 0.6823 - val_acc: 0.6372\n",
      "9639/9639 [==============================] - 3s 327us/step\n",
      "TN:157,FP:14,FN:108,TP:56,Macc:0.62979599382,F1:0.478627821215\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2391 - acc: 0.8922 - val_loss: 0.4526 - val_acc: 0.7896\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:141,FP:30,FN:42,TP:122,Macc:0.784231869798,F1:0.772146352947\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2360 - acc: 0.8932 - val_loss: 0.4362 - val_acc: 0.7930\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:150,FP:21,FN:48,TP:116,Macc:0.792254975879,F1:0.77075860756\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2365 - acc: 0.8927 - val_loss: 0.5902 - val_acc: 0.6702\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:168,FP:3,FN:106,TP:58,Macc:0.668057295008,F1:0.515551157765\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2345 - acc: 0.8947 - val_loss: 0.4662 - val_acc: 0.7606\n",
      "9639/9639 [==============================] - 3s 324us/step\n",
      "TN:157,FP:14,FN:66,TP:98,Macc:0.75784476572,F1:0.710139566202\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2365 - acc: 0.8928 - val_loss: 0.5268 - val_acc: 0.7124\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:154,FP:17,FN:78,TP:86,Macc:0.71248747306,F1:0.644189490595\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2355 - acc: 0.8950 - val_loss: 0.5619 - val_acc: 0.6961\n",
      "9639/9639 [==============================] - 3s 325us/step\n",
      "TN:155,FP:16,FN:84,TP:80,Macc:0.69711876778,F1:0.615379439569\n",
      "Loss: 0.556490\n",
      "Iteration No: 60 ended. Search finished for the next optimal point.\n",
      "Time taken: 2599.0534\n",
      "Function value obtained: 0.5565\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 61 started. Searching for the next optimal point.\n",
      "args [1, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 59s 49ms/step - loss: 0.7478 - acc: 0.8476 - val_loss: 0.5431 - val_acc: 0.7583\n",
      "9639/9639 [==============================] - 21s 2ms/step\n",
      "TN:139,FP:32,FN:49,TP:115,Macc:0.757042454974,F1:0.739544303387\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.3010 - acc: 0.8630 - val_loss: 0.4976 - val_acc: 0.8079\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:130,FP:41,FN:28,TP:136,Macc:0.794751053143,F1:0.797648415514\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2877 - acc: 0.8707 - val_loss: 0.4782 - val_acc: 0.7868\n",
      "9639/9639 [==============================] - 3s 333us/step\n",
      "TN:140,FP:31,FN:44,TP:120,Macc:0.775210332811,F1:0.7618992181\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2764 - acc: 0.8761 - val_loss: 0.4714 - val_acc: 0.7929\n",
      "9639/9639 [==============================] - 3s 334us/step\n",
      "TN:143,FP:28,FN:44,TP:120,Macc:0.783982262071,F1:0.769225229493\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2749 - acc: 0.8781 - val_loss: 0.4330 - val_acc: 0.8320\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:132,FP:39,FN:20,TP:144,Macc:0.82498924825,F1:0.829965645461\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2686 - acc: 0.8790 - val_loss: 0.4329 - val_acc: 0.8312\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:131,FP:40,FN:19,TP:145,Macc:0.825114052113,F1:0.830940026309\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2660 - acc: 0.8812 - val_loss: 0.4267 - val_acc: 0.8497\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:131,FP:40,FN:15,TP:149,Macc:0.837309173246,F1:0.84418710957\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2637 - acc: 0.8819 - val_loss: 0.4259 - val_acc: 0.8445\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:128,FP:43,FN:14,TP:150,Macc:0.831586024269,F1:0.840330618997\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2600 - acc: 0.8838 - val_loss: 0.4039 - val_acc: 0.8459\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:140,FP:31,FN:22,TP:142,Macc:0.842283499044,F1:0.84272441927\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2583 - acc: 0.8838 - val_loss: 0.4149 - val_acc: 0.8488\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:128,FP:43,FN:13,TP:151,Macc:0.834634804553,F1:0.843569905812\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2587 - acc: 0.8840 - val_loss: 0.4629 - val_acc: 0.7960\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:139,FP:32,FN:38,TP:126,Macc:0.790579038091,F1:0.78260314413\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2569 - acc: 0.8835 - val_loss: 0.3921 - val_acc: 0.8512\n",
      "9639/9639 [==============================] - 3s 313us/step\n",
      "TN:125,FP:46,FN:7,TP:157,Macc:0.844155556992,F1:0.855580341918\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2575 - acc: 0.8849 - val_loss: 0.4790 - val_acc: 0.7827\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:139,FP:32,FN:45,TP:119,Macc:0.769237576108,F1:0.755550012194\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2546 - acc: 0.8848 - val_loss: 0.3937 - val_acc: 0.8472\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:131,FP:40,FN:17,TP:147,Macc:0.83121161268,F1:0.837601308759\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2561 - acc: 0.8839 - val_loss: 0.4209 - val_acc: 0.8136\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:136,FP:35,FN:28,TP:136,Macc:0.812294911664,F1:0.811934747625\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2528 - acc: 0.8860 - val_loss: 0.4141 - val_acc: 0.8257\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:136,FP:35,FN:25,TP:139,Macc:0.821441252514,F1:0.822479658417\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2515 - acc: 0.8852 - val_loss: 0.3852 - val_acc: 0.8471\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:132,FP:39,FN:16,TP:148,Macc:0.837184369383,F1:0.8432993141\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2527 - acc: 0.8853 - val_loss: 0.4212 - val_acc: 0.8161\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:135,FP:36,FN:33,TP:131,Macc:0.794127033827,F1:0.791535233379\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2516 - acc: 0.8861 - val_loss: 0.3946 - val_acc: 0.8431\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:133,FP:38,FN:20,TP:144,Macc:0.82791322467,F1:0.832364404193\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2493 - acc: 0.8883 - val_loss: 0.3869 - val_acc: 0.8582\n",
      "9639/9639 [==============================] - 3s 327us/step\n",
      "TN:133,FP:38,FN:15,TP:149,Macc:0.843157126086,F1:0.84899731944\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2487 - acc: 0.8879 - val_loss: 0.3936 - val_acc: 0.8368\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:137,FP:34,FN:26,TP:138,Macc:0.82131644865,F1:0.821423020799\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2476 - acc: 0.8883 - val_loss: 0.3753 - val_acc: 0.8466\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:133,FP:38,FN:19,TP:145,Macc:0.830962004953,F1:0.835729333857\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2486 - acc: 0.8875 - val_loss: 0.3880 - val_acc: 0.8607\n",
      "9639/9639 [==============================] - 3s 328us/step\n",
      "TN:144,FP:27,FN:23,TP:141,Macc:0.850930624441,F1:0.84939203491\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2471 - acc: 0.8885 - val_loss: 0.3844 - val_acc: 0.8553\n",
      "9639/9639 [==============================] - 3s 333us/step\n",
      "TN:131,FP:40,FN:14,TP:150,Macc:0.840357953529,F1:0.847452104156\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2475 - acc: 0.8866 - val_loss: 0.3806 - val_acc: 0.8587\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:131,FP:40,FN:13,TP:151,Macc:0.843406733813,F1:0.850698704483\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2470 - acc: 0.8872 - val_loss: 0.4100 - val_acc: 0.8254\n",
      "9639/9639 [==============================] - 3s 328us/step\n",
      "TN:139,FP:32,FN:28,TP:136,Macc:0.821066840924,F1:0.81927155498\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2449 - acc: 0.8899 - val_loss: 0.3838 - val_acc: 0.8417\n",
      "9639/9639 [==============================] - 3s 334us/step\n",
      "TN:137,FP:34,FN:21,TP:143,Macc:0.836560350067,F1:0.838704131339\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2473 - acc: 0.8879 - val_loss: 0.4500 - val_acc: 0.7995\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:139,FP:32,FN:35,TP:129,Macc:0.799725378941,F1:0.793840600616\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2451 - acc: 0.8894 - val_loss: 0.3904 - val_acc: 0.8599\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:130,FP:41,FN:9,TP:155,Macc:0.852677878526,F1:0.861105601979\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2452 - acc: 0.8888 - val_loss: 0.3708 - val_acc: 0.8621\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:130,FP:41,FN:11,TP:153,Macc:0.846580317959,F1:0.854743089483\n",
      "Loss: 0.365652\n",
      "Iteration No: 61 ended. Search finished for the next optimal point.\n",
      "Time taken: 2575.6121\n",
      "Function value obtained: 0.3657\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 62 started. Searching for the next optimal point.\n",
      "args [1, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 61s 51ms/step - loss: 0.7590 - acc: 0.8441 - val_loss: 0.5702 - val_acc: 0.7571\n",
      "9639/9639 [==============================] - 21s 2ms/step\n",
      "TN:146,FP:25,FN:56,TP:108,Macc:0.756168827932,F1:0.727267233362\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2977 - acc: 0.8645 - val_loss: 0.5529 - val_acc: 0.7035\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:142,FP:29,FN:58,TP:106,Macc:0.738375361685,F1:0.709024599946\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2855 - acc: 0.8675 - val_loss: 0.5290 - val_acc: 0.7370\n",
      "9639/9639 [==============================] - 3s 336us/step\n",
      "TN:138,FP:33,FN:52,TP:112,Macc:0.744972137704,F1:0.724913563075\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2783 - acc: 0.8704 - val_loss: 0.5249 - val_acc: 0.7122\n",
      "9639/9639 [==============================] - 3s 339us/step\n",
      "TN:143,FP:28,FN:57,TP:107,Macc:0.744348118388,F1:0.715713562664\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2771 - acc: 0.8712 - val_loss: 0.5197 - val_acc: 0.7564\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:140,FP:31,FN:38,TP:126,Macc:0.793503014511,F1:0.785041177822\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2696 - acc: 0.8752 - val_loss: 0.5401 - val_acc: 0.6232\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:157,FP:14,FN:102,TP:62,Macc:0.64808867552,F1:0.516661858794\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2678 - acc: 0.8767 - val_loss: 0.5432 - val_acc: 0.5955\n",
      "9639/9639 [==============================] - 3s 338us/step\n",
      "TN:157,FP:14,FN:110,TP:54,Macc:0.623698433254,F1:0.465512639016\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2656 - acc: 0.8782 - val_loss: 0.5466 - val_acc: 0.5641\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:165,FP:6,FN:130,TP:34,Macc:0.586114638948,F1:0.333329829524\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2596 - acc: 0.8815 - val_loss: 0.6266 - val_acc: 0.5296\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:163,FP:8,FN:135,TP:29,Macc:0.565022784692,F1:0.28855387811\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2587 - acc: 0.8815 - val_loss: 0.5872 - val_acc: 0.5408\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:162,FP:9,FN:133,TP:31,Macc:0.568196368838,F1:0.303918067994\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2578 - acc: 0.8827 - val_loss: 0.6371 - val_acc: 0.5059\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:166,FP:5,FN:147,TP:17,Macc:0.537209350552,F1:0.182793382962\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2557 - acc: 0.8834 - val_loss: 0.6325 - val_acc: 0.4998\n",
      "9639/9639 [==============================] - 3s 336us/step\n",
      "TN:168,FP:3,FN:150,TP:14,Macc:0.533910962542,F1:0.154694241589\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2543 - acc: 0.8834 - val_loss: 0.6881 - val_acc: 0.5109\n",
      "9639/9639 [==============================] - 3s 337us/step\n",
      "TN:165,FP:6,FN:145,TP:19,Macc:0.540382934699,F1:0.20105565256\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2503 - acc: 0.8842 - val_loss: 0.7020 - val_acc: 0.4933\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:167,FP:4,FN:149,TP:15,Macc:0.534035766406,F1:0.163932359546\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2522 - acc: 0.8843 - val_loss: 0.6975 - val_acc: 0.5001\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:164,FP:7,FN:148,TP:16,Macc:0.528312617429,F1:0.171120601479\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2496 - acc: 0.8850 - val_loss: 0.7019 - val_acc: 0.4925\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:168,FP:3,FN:151,TP:13,Macc:0.530862182259,F1:0.144442645083\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2481 - acc: 0.8861 - val_loss: 0.6727 - val_acc: 0.5007\n",
      "9639/9639 [==============================] - 3s 333us/step\n",
      "TN:168,FP:3,FN:151,TP:13,Macc:0.530862182259,F1:0.144442645083\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2490 - acc: 0.8843 - val_loss: 0.6912 - val_acc: 0.4737\n",
      "9639/9639 [==============================] - 3s 329us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2467 - acc: 0.8856 - val_loss: 0.7684 - val_acc: 0.4766\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:171,FP:0,FN:162,TP:2,Macc:0.506097528403,F1:0.0240961204834\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2468 - acc: 0.8863 - val_loss: 0.7932 - val_acc: 0.4738\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:171,FP:0,FN:162,TP:2,Macc:0.506097528403,F1:0.0240961204834\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2462 - acc: 0.8868 - val_loss: 0.8088 - val_acc: 0.4730\n",
      "9639/9639 [==============================] - 3s 334us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2436 - acc: 0.8886 - val_loss: 0.7793 - val_acc: 0.4733\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:171,FP:0,FN:162,TP:2,Macc:0.506097528403,F1:0.0240961204834\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2441 - acc: 0.8897 - val_loss: 0.8557 - val_acc: 0.4758\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:171,FP:0,FN:162,TP:2,Macc:0.506097528403,F1:0.0240961204834\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2438 - acc: 0.8881 - val_loss: 0.7468 - val_acc: 0.4735\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:171,FP:0,FN:162,TP:2,Macc:0.506097528403,F1:0.0240961204834\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2450 - acc: 0.8870 - val_loss: 0.8454 - val_acc: 0.4724\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:171,FP:0,FN:162,TP:2,Macc:0.506097528403,F1:0.0240961204834\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2419 - acc: 0.8902 - val_loss: 0.8521 - val_acc: 0.4746\n",
      "9639/9639 [==============================] - 3s 327us/step\n",
      "TN:171,FP:0,FN:162,TP:2,Macc:0.506097528403,F1:0.0240961204834\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2437 - acc: 0.8865 - val_loss: 0.7772 - val_acc: 0.4724\n",
      "9639/9639 [==============================] - 3s 333us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2404 - acc: 0.8907 - val_loss: 0.8026 - val_acc: 0.4716\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2409 - acc: 0.8899 - val_loss: 0.7974 - val_acc: 0.4720\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:171,FP:0,FN:163,TP:1,Macc:0.50304874812,F1:0.0121210779798\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2403 - acc: 0.8908 - val_loss: 0.8344 - val_acc: 0.4758\n",
      "9639/9639 [==============================] - 3s 328us/step\n",
      "TN:171,FP:0,FN:161,TP:3,Macc:0.509146308686,F1:0.0359277508724\n",
      "Loss: 0.829478\n",
      "Iteration No: 62 ended. Search finished for the next optimal point.\n",
      "Time taken: 2596.3090\n",
      "Function value obtained: 0.8295\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 63 started. Searching for the next optimal point.\n",
      "args [1, 1, 10]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 59s 49ms/step - loss: 0.7561 - acc: 0.8313 - val_loss: 0.5779 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 22s 2ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.3091 - acc: 0.8442 - val_loss: 0.5489 - val_acc: 0.8135\n",
      "9639/9639 [==============================] - 3s 339us/step\n",
      "TN:122,FP:49,FN:18,TP:146,Macc:0.801847044615,F1:0.813364964741\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2951 - acc: 0.8533 - val_loss: 0.5013 - val_acc: 0.8230\n",
      "9639/9639 [==============================] - 3s 341us/step\n",
      "TN:118,FP:53,FN:14,TP:150,Macc:0.802346260068,F1:0.817433205242\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2863 - acc: 0.8586 - val_loss: 0.5240 - val_acc: 0.8052\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:134,FP:37,FN:32,TP:132,Macc:0.79425183769,F1:0.792787241694\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2852 - acc: 0.8603 - val_loss: 0.5209 - val_acc: 0.8052\n",
      "9639/9639 [==============================] - 3s 337us/step\n",
      "TN:134,FP:37,FN:29,TP:135,Macc:0.80339817854,F1:0.803565879112\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2780 - acc: 0.8643 - val_loss: 0.4960 - val_acc: 0.8006\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:103,FP:68,FN:3,TP:161,Macc:0.792023196883,F1:0.819333027015\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2748 - acc: 0.8680 - val_loss: 0.4846 - val_acc: 0.8138\n",
      "9639/9639 [==============================] - 3s 336us/step\n",
      "TN:113,FP:58,FN:8,TP:156,Macc:0.806019059668,F1:0.825391373625\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2739 - acc: 0.8699 - val_loss: 0.4630 - val_acc: 0.8242\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:123,FP:48,FN:12,TP:152,Macc:0.823063702735,F1:0.835159338521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2700 - acc: 0.8742 - val_loss: 0.4406 - val_acc: 0.8327\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:135,FP:36,FN:21,TP:143,Macc:0.830712397227,F1:0.833813699056\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2687 - acc: 0.8742 - val_loss: 0.4892 - val_acc: 0.8008\n",
      "9639/9639 [==============================] - 3s 336us/step\n",
      "TN:101,FP:70,FN:2,TP:162,Macc:0.789224024326,F1:0.818176434939\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2680 - acc: 0.8752 - val_loss: 0.4371 - val_acc: 0.8404\n",
      "9639/9639 [==============================] - 3s 334us/step\n",
      "TN:126,FP:45,FN:7,TP:157,Macc:0.847079533412,F1:0.857918005021\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2653 - acc: 0.8772 - val_loss: 0.4516 - val_acc: 0.8339\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:118,FP:53,FN:4,TP:160,Macc:0.832834062901,F1:0.848800909462\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2619 - acc: 0.8805 - val_loss: 0.4381 - val_acc: 0.8362\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:128,FP:43,FN:17,TP:147,Macc:0.822439683419,F1:0.830502952668\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2621 - acc: 0.8797 - val_loss: 0.4355 - val_acc: 0.8370\n",
      "9639/9639 [==============================] - 3s 333us/step\n",
      "TN:134,FP:37,FN:21,TP:143,Macc:0.827788420807,F1:0.831389807601\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2620 - acc: 0.8804 - val_loss: 0.4303 - val_acc: 0.8397\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:133,FP:38,FN:20,TP:144,Macc:0.82791322467,F1:0.832364404193\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2595 - acc: 0.8800 - val_loss: 0.4402 - val_acc: 0.8262\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:130,FP:41,FN:21,TP:143,Macc:0.816092515126,F1:0.821833546707\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2584 - acc: 0.8810 - val_loss: 0.4530 - val_acc: 0.8250\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:141,FP:30,FN:30,TP:134,Macc:0.820817233197,F1:0.817067615965\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2582 - acc: 0.8817 - val_loss: 0.4142 - val_acc: 0.8430\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:130,FP:41,FN:13,TP:151,Macc:0.840482757393,F1:0.848309088376\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2578 - acc: 0.8804 - val_loss: 0.4141 - val_acc: 0.8525\n",
      "9639/9639 [==============================] - 3s 333us/step\n",
      "TN:132,FP:39,FN:8,TP:156,Macc:0.861574611649,F1:0.869075267731\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2558 - acc: 0.8826 - val_loss: 0.4684 - val_acc: 0.8087\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:137,FP:34,FN:31,TP:133,Macc:0.806072547234,F1:0.80361982472\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2575 - acc: 0.8813 - val_loss: 0.4350 - val_acc: 0.8270\n",
      "9639/9639 [==============================] - 3s 342us/step\n",
      "TN:140,FP:31,FN:28,TP:136,Macc:0.823990817344,F1:0.821746711732\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2573 - acc: 0.8830 - val_loss: 0.5158 - val_acc: 0.7921\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:143,FP:28,FN:42,TP:122,Macc:0.790079822638,F1:0.777064520222\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 50s 41ms/step - loss: 0.2562 - acc: 0.8823 - val_loss: 0.4289 - val_acc: 0.8378\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:138,FP:33,FN:21,TP:143,Macc:0.839484326487,F1:0.841170923046\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2547 - acc: 0.8835 - val_loss: 0.4525 - val_acc: 0.8201\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:135,FP:36,FN:27,TP:137,Macc:0.812419715527,F1:0.813050830704\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2555 - acc: 0.8827 - val_loss: 0.4300 - val_acc: 0.8310\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:134,FP:37,FN:23,TP:141,Macc:0.82169086024,F1:0.82455585972\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 49s 41ms/step - loss: 0.2555 - acc: 0.8825 - val_loss: 0.4702 - val_acc: 0.8150\n",
      "9639/9639 [==============================] - 3s 332us/step\n",
      "TN:140,FP:31,FN:31,TP:133,Macc:0.814844476494,F1:0.810970055399\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2541 - acc: 0.8825 - val_loss: 0.4662 - val_acc: 0.8098\n",
      "9639/9639 [==============================] - 3s 331us/step\n",
      "TN:140,FP:31,FN:33,TP:131,Macc:0.808746915927,F1:0.803675427604\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2527 - acc: 0.8851 - val_loss: 0.5571 - val_acc: 0.7533\n",
      "9639/9639 [==============================] - 3s 351us/step\n",
      "TN:143,FP:28,FN:52,TP:112,Macc:0.759592019805,F1:0.736836586259\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2540 - acc: 0.8839 - val_loss: 0.4597 - val_acc: 0.8151\n",
      "9639/9639 [==============================] - 3s 335us/step\n",
      "TN:135,FP:36,FN:28,TP:136,Macc:0.809370935243,F1:0.809518259675\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2512 - acc: 0.8843 - val_loss: 0.5114 - val_acc: 0.8010\n",
      "9639/9639 [==============================] - 3s 330us/step\n",
      "TN:137,FP:34,FN:33,TP:131,Macc:0.799974986667,F1:0.796347030424\n",
      "Loss: 0.506264\n",
      "Iteration No: 63 ended. Search finished for the next optimal point.\n",
      "Time taken: 2636.6190\n",
      "Function value obtained: 0.5063\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 64 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.7553 - acc: 0.8468 - val_loss: 0.5560 - val_acc: 0.7706\n",
      "9639/9639 [==============================] - 22s 2ms/step\n",
      "TN:119,FP:52,FN:31,TP:133,Macc:0.753440971672,F1:0.762172122337\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.3040 - acc: 0.8629 - val_loss: 0.5410 - val_acc: 0.7629\n",
      "9639/9639 [==============================] - 3s 356us/step\n",
      "TN:91,FP:80,FN:5,TP:159,Macc:0.750837919276,F1:0.789076533307\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2910 - acc: 0.8661 - val_loss: 0.5357 - val_acc: 0.7624\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:87,FP:84,FN:5,TP:159,Macc:0.739142013595,F1:0.781321446347\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2826 - acc: 0.8713 - val_loss: 0.5515 - val_acc: 0.7391\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:79,FP:92,FN:2,TP:162,Macc:0.724896543084,F1:0.775114331438\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2783 - acc: 0.8728 - val_loss: 0.4871 - val_acc: 0.7924\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:99,FP:72,FN:6,TP:158,Macc:0.771180950353,F1:0.802025066438\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2698 - acc: 0.8779 - val_loss: 0.4890 - val_acc: 0.7806\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:96,FP:75,FN:1,TP:163,Macc:0.777652922509,F1:0.810939915656\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2672 - acc: 0.8787 - val_loss: 0.5072 - val_acc: 0.7679\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:94,FP:77,FN:7,TP:157,Macc:0.753512287969,F1:0.788939350179\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2628 - acc: 0.8816 - val_loss: 0.4676 - val_acc: 0.8107\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:119,FP:52,FN:19,TP:145,Macc:0.790026335072,F1:0.803318596764\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2643 - acc: 0.8813 - val_loss: 0.4895 - val_acc: 0.7910\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:111,FP:60,FN:12,TP:152,Macc:0.787975985694,F1:0.808505180661\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2612 - acc: 0.8820 - val_loss: 0.4361 - val_acc: 0.8254\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:116,FP:55,FN:10,TP:154,Macc:0.808693428361,F1:0.8257317968\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2599 - acc: 0.8839 - val_loss: 0.4787 - val_acc: 0.7970\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:102,FP:69,FN:5,TP:159,Macc:0.783001659897,F1:0.811219090909\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2574 - acc: 0.8845 - val_loss: 0.4492 - val_acc: 0.8183\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:127,FP:44,FN:20,TP:144,Macc:0.810369366149,F1:0.81817629265\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2585 - acc: 0.8839 - val_loss: 0.4549 - val_acc: 0.8295\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:143,FP:28,FN:35,TP:129,Macc:0.811421284621,F1:0.803732765325\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2562 - acc: 0.8861 - val_loss: 0.4594 - val_acc: 0.8192\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:133,FP:38,FN:22,TP:142,Macc:0.821815664103,F1:0.825575854485\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2564 - acc: 0.8854 - val_loss: 0.4447 - val_acc: 0.8253\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:140,FP:31,FN:34,TP:130,Macc:0.805698135644,F1:0.799994446353\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2529 - acc: 0.8870 - val_loss: 0.4159 - val_acc: 0.8432\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:140,FP:31,FN:27,TP:137,Macc:0.827039597627,F1:0.825295650966\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2529 - acc: 0.8863 - val_loss: 0.4282 - val_acc: 0.8467\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:149,FP:22,FN:23,TP:141,Macc:0.865550506541,F1:0.862379763168\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2522 - acc: 0.8861 - val_loss: 0.4329 - val_acc: 0.8453\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:154,FP:17,FN:34,TP:130,Macc:0.846633805525,F1:0.836007319067\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2509 - acc: 0.8870 - val_loss: 0.4757 - val_acc: 0.8240\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:157,FP:14,FN:47,TP:117,Macc:0.815771591103,F1:0.79321484869\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2514 - acc: 0.8867 - val_loss: 0.4276 - val_acc: 0.8465\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:143,FP:28,FN:26,TP:138,Macc:0.838860307171,F1:0.836358080844\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2485 - acc: 0.8877 - val_loss: 0.4753 - val_acc: 0.8239\n",
      "9639/9639 [==============================] - 3s 341us/step\n",
      "TN:158,FP:13,FN:48,TP:116,Macc:0.815646787239,F1:0.791803392785\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2482 - acc: 0.8879 - val_loss: 0.4485 - val_acc: 0.8393\n",
      "9639/9639 [==============================] - 3s 342us/step\n",
      "TN:156,FP:15,FN:42,TP:122,Macc:0.828091516099,F1:0.810625714278\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2481 - acc: 0.8892 - val_loss: 0.4290 - val_acc: 0.8537\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:151,FP:20,FN:35,TP:129,Macc:0.834813095982,F1:0.824275604891\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2477 - acc: 0.8892 - val_loss: 0.4121 - val_acc: 0.8524\n",
      "9639/9639 [==============================] - 3s 342us/step\n",
      "TN:137,FP:34,FN:19,TP:145,Macc:0.842657910633,F1:0.845475505888\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2468 - acc: 0.8898 - val_loss: 0.4413 - val_acc: 0.8378\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:151,FP:20,FN:39,TP:125,Macc:0.822617974849,F1:0.809055951902\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2448 - acc: 0.8909 - val_loss: 0.4397 - val_acc: 0.8397\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:155,FP:16,FN:37,TP:127,Macc:0.840411441096,F1:0.827356029999\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2471 - acc: 0.8887 - val_loss: 0.4453 - val_acc: 0.8433\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:154,FP:17,FN:36,TP:128,Macc:0.840536244959,F1:0.828473426247\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2443 - acc: 0.8898 - val_loss: 0.4515 - val_acc: 0.8389\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:154,FP:17,FN:40,TP:124,Macc:0.828341123826,F1:0.813109226761\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2450 - acc: 0.8901 - val_loss: 0.4775 - val_acc: 0.8235\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:160,FP:11,FN:44,TP:120,Macc:0.833689861213,F1:0.813553830223\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2434 - acc: 0.8917 - val_loss: 0.4521 - val_acc: 0.8375\n",
      "9639/9639 [==============================] - 3s 351us/step\n",
      "TN:157,FP:14,FN:42,TP:122,Macc:0.831015492519,F1:0.813327821637\n",
      "Loss: 0.446276\n",
      "Iteration No: 64 ended. Search finished for the next optimal point.\n",
      "Time taken: 2723.7295\n",
      "Function value obtained: 0.4463\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 65 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 61s 50ms/step - loss: 0.7666 - acc: 0.8373 - val_loss: 0.5829 - val_acc: 0.7890\n",
      "9639/9639 [==============================] - 23s 2ms/step\n",
      "TN:98,FP:73,FN:4,TP:160,Macc:0.774354534499,F1:0.80603996156\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.3061 - acc: 0.8560 - val_loss: 0.5618 - val_acc: 0.7756\n",
      "9639/9639 [==============================] - 3s 356us/step\n",
      "TN:97,FP:74,FN:0,TP:164,Macc:0.783625679213,F1:0.815915039761\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2918 - acc: 0.8615 - val_loss: 0.6004 - val_acc: 0.6969\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:68,FP:103,FN:1,TP:163,Macc:0.695781582747,F1:0.758134305606\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2824 - acc: 0.8704 - val_loss: 0.5398 - val_acc: 0.7612\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:90,FP:81,FN:1,TP:163,Macc:0.760109063989,F1:0.799014276251\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2773 - acc: 0.8758 - val_loss: 0.5620 - val_acc: 0.7288\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:75,FP:96,FN:0,TP:164,Macc:0.719298197971,F1:0.773579647508\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2697 - acc: 0.8781 - val_loss: 0.4931 - val_acc: 0.7884\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:101,FP:70,FN:2,TP:162,Macc:0.789224024326,F1:0.818176434939\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2702 - acc: 0.8793 - val_loss: 0.4684 - val_acc: 0.8011\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:112,FP:59,FN:7,TP:157,Macc:0.806143863531,F1:0.826310344661\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2656 - acc: 0.8818 - val_loss: 0.4857 - val_acc: 0.7865\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:98,FP:73,FN:1,TP:163,Macc:0.783500875349,F1:0.81499463341\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2631 - acc: 0.8811 - val_loss: 0.4612 - val_acc: 0.8015\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:105,FP:66,FN:0,TP:164,Macc:0.807017490573,F1:0.832481917528\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2598 - acc: 0.8828 - val_loss: 0.4144 - val_acc: 0.8345\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:115,FP:56,FN:4,TP:160,Macc:0.824062133641,F1:0.842099817431\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2590 - acc: 0.8834 - val_loss: 0.4198 - val_acc: 0.8273\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:117,FP:54,FN:10,TP:154,Macc:0.811617404781,F1:0.827951517263\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2575 - acc: 0.8852 - val_loss: 0.4303 - val_acc: 0.8149\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:108,FP:63,FN:1,TP:163,Macc:0.81274063955,F1:0.835892027779\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2550 - acc: 0.8850 - val_loss: 0.4512 - val_acc: 0.7987\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:106,FP:65,FN:1,TP:163,Macc:0.80689268671,F1:0.831627253028\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2566 - acc: 0.8840 - val_loss: 0.4328 - val_acc: 0.8131\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:108,FP:63,FN:1,TP:163,Macc:0.81274063955,F1:0.835892027779\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2555 - acc: 0.8847 - val_loss: 0.4140 - val_acc: 0.8265\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:114,FP:57,FN:3,TP:161,Macc:0.824186937504,F1:0.842926498568\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2540 - acc: 0.8867 - val_loss: 0.3845 - val_acc: 0.8496\n",
      "9639/9639 [==============================] - 3s 361us/step\n",
      "TN:127,FP:44,FN:5,TP:159,Macc:0.856101070399,F1:0.866479523826\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2519 - acc: 0.8869 - val_loss: 0.3909 - val_acc: 0.8416\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:119,FP:52,FN:2,TP:162,Macc:0.841855599888,F1:0.857137403522\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2529 - acc: 0.8856 - val_loss: 0.3966 - val_acc: 0.8392\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:122,FP:49,FN:8,TP:156,Macc:0.832334847449,F1:0.84552297281\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2506 - acc: 0.8868 - val_loss: 0.3951 - val_acc: 0.8583\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:145,FP:26,FN:19,TP:145,Macc:0.866049721994,F1:0.865666087377\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2494 - acc: 0.8865 - val_loss: 0.4021 - val_acc: 0.8385\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:120,FP:51,FN:4,TP:160,Macc:0.838682015742,F1:0.853327869702\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2506 - acc: 0.8869 - val_loss: 0.4007 - val_acc: 0.8434\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:125,FP:46,FN:14,TP:150,Macc:0.822814095009,F1:0.8333278259\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2481 - acc: 0.8885 - val_loss: 0.4024 - val_acc: 0.8446\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:135,FP:36,FN:20,TP:144,Macc:0.83376117751,F1:0.837203760717\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2487 - acc: 0.8880 - val_loss: 0.3745 - val_acc: 0.8513\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:122,FP:49,FN:6,TP:158,Macc:0.838432408015,F1:0.851746544974\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2487 - acc: 0.8877 - val_loss: 0.4269 - val_acc: 0.8104\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:105,FP:66,FN:0,TP:164,Macc:0.807017490573,F1:0.832481917528\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2475 - acc: 0.8881 - val_loss: 0.4216 - val_acc: 0.8126\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:112,FP:59,FN:4,TP:160,Macc:0.815290204381,F1:0.835503703843\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2466 - acc: 0.8895 - val_loss: 0.3991 - val_acc: 0.8314\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:113,FP:58,FN:2,TP:162,Macc:0.824311741368,F1:0.843744568665\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2461 - acc: 0.8891 - val_loss: 0.4023 - val_acc: 0.8431\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:124,FP:47,FN:10,TP:154,Macc:0.832085239722,F1:0.843830122129\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2446 - acc: 0.8901 - val_loss: 0.3829 - val_acc: 0.8432\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:120,FP:51,FN:4,TP:160,Macc:0.838682015742,F1:0.853327869702\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2457 - acc: 0.8896 - val_loss: 0.4231 - val_acc: 0.8111\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:109,FP:62,FN:5,TP:159,Macc:0.803469494837,F1:0.825968599367\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2453 - acc: 0.8895 - val_loss: 0.4806 - val_acc: 0.7673\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:90,FP:81,FN:1,TP:163,Macc:0.760109063989,F1:0.799014276251\n",
      "Loss: 0.475272\n",
      "Iteration No: 65 ended. Search finished for the next optimal point.\n",
      "Time taken: 2729.3454\n",
      "Function value obtained: 0.4753\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 66 started. Searching for the next optimal point.\n",
      "args [1, 1, 10]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 60s 50ms/step - loss: 0.7574 - acc: 0.8430 - val_loss: 0.6029 - val_acc: 0.7746\n",
      "9639/9639 [==============================] - 23s 2ms/step\n",
      "TN:109,FP:62,FN:15,TP:149,Macc:0.772981692004,F1:0.79466120648\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3020 - acc: 0.8601 - val_loss: 0.5205 - val_acc: 0.7979\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:102,FP:69,FN:7,TP:157,Macc:0.77690409933,F1:0.805122798747\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2879 - acc: 0.8668 - val_loss: 0.5206 - val_acc: 0.7918\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:96,FP:75,FN:1,TP:163,Macc:0.777652922509,F1:0.810939915656\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2795 - acc: 0.8737 - val_loss: 0.4799 - val_acc: 0.8084\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:99,FP:72,FN:3,TP:161,Macc:0.780327291203,F1:0.811077744656\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2750 - acc: 0.8751 - val_loss: 0.4709 - val_acc: 0.8093\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:104,FP:67,FN:7,TP:157,Macc:0.78275205217,F1:0.809272936187\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2695 - acc: 0.8779 - val_loss: 0.4391 - val_acc: 0.8295\n",
      "9639/9639 [==============================] - 3s 356us/step\n",
      "TN:108,FP:63,FN:1,TP:163,Macc:0.81274063955,F1:0.835892027779\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2686 - acc: 0.8773 - val_loss: 0.4526 - val_acc: 0.8127\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:105,FP:66,FN:5,TP:159,Macc:0.791773589157,F1:0.817475308842\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2632 - acc: 0.8818 - val_loss: 0.3993 - val_acc: 0.8505\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:125,FP:46,FN:12,TP:152,Macc:0.828911655576,F1:0.839773503042\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2615 - acc: 0.8814 - val_loss: 0.4015 - val_acc: 0.8485\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:120,FP:51,FN:9,TP:155,Macc:0.823438114325,F1:0.837832358925\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2594 - acc: 0.8836 - val_loss: 0.4061 - val_acc: 0.8432\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:115,FP:56,FN:4,TP:160,Macc:0.824062133641,F1:0.842099817431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2592 - acc: 0.8831 - val_loss: 0.4087 - val_acc: 0.8379\n",
      "9639/9639 [==============================] - 3s 342us/step\n",
      "TN:129,FP:42,FN:19,TP:145,Macc:0.819266099273,F1:0.826205298078\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2562 - acc: 0.8860 - val_loss: 0.4400 - val_acc: 0.8120\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:107,FP:64,FN:4,TP:160,Macc:0.80067032228,F1:0.824736852835\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2533 - acc: 0.8868 - val_loss: 0.4403 - val_acc: 0.8208\n",
      "9639/9639 [==============================] - 3s 346us/step\n",
      "TN:140,FP:31,FN:39,TP:125,Macc:0.790454234228,F1:0.781244449765\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2534 - acc: 0.8864 - val_loss: 0.4209 - val_acc: 0.8428\n",
      "9639/9639 [==============================] - 3s 345us/step\n",
      "TN:129,FP:42,FN:17,TP:147,Macc:0.825363659839,F1:0.832855665518\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2554 - acc: 0.8853 - val_loss: 0.4147 - val_acc: 0.8363\n",
      "9639/9639 [==============================] - 3s 344us/step\n",
      "TN:133,FP:38,FN:26,TP:138,Macc:0.80962054297,F1:0.811759160245\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2519 - acc: 0.8873 - val_loss: 0.4102 - val_acc: 0.8419\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:137,FP:34,FN:27,TP:137,Macc:0.818267668367,F1:0.817904896486\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2516 - acc: 0.8865 - val_loss: 0.4488 - val_acc: 0.8233\n",
      "9639/9639 [==============================] - 3s 352us/step\n",
      "TN:140,FP:31,FN:36,TP:128,Macc:0.799600575077,F1:0.792564106816\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2508 - acc: 0.8884 - val_loss: 0.4430 - val_acc: 0.8165\n",
      "9639/9639 [==============================] - 3s 341us/step\n",
      "TN:139,FP:32,FN:35,TP:129,Macc:0.799725378941,F1:0.793840600616\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2489 - acc: 0.8879 - val_loss: 0.4094 - val_acc: 0.8376\n",
      "9639/9639 [==============================] - 3s 341us/step\n",
      "TN:127,FP:44,FN:18,TP:146,Macc:0.816466926716,F1:0.824853235505\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2479 - acc: 0.8887 - val_loss: 0.4021 - val_acc: 0.8416\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:126,FP:45,FN:14,TP:150,Macc:0.825738071429,F1:0.835649085937\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2468 - acc: 0.8890 - val_loss: 0.4783 - val_acc: 0.8179\n",
      "9639/9639 [==============================] - 3s 338us/step\n",
      "TN:144,FP:27,FN:38,TP:126,Macc:0.805198920191,F1:0.794947132878\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2482 - acc: 0.8897 - val_loss: 0.4168 - val_acc: 0.8319\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:126,FP:45,FN:21,TP:143,Macc:0.804396609446,F1:0.812494474824\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2474 - acc: 0.8885 - val_loss: 0.4129 - val_acc: 0.8357\n",
      "9639/9639 [==============================] - 3s 338us/step\n",
      "TN:130,FP:41,FN:19,TP:145,Macc:0.822190075693,F1:0.828565898257\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2463 - acc: 0.8902 - val_loss: 0.4241 - val_acc: 0.8310\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:129,FP:42,FN:20,TP:144,Macc:0.816217318989,F1:0.822851612902\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2446 - acc: 0.8897 - val_loss: 0.3979 - val_acc: 0.8409\n",
      "9639/9639 [==============================] - 3s 341us/step\n",
      "TN:131,FP:40,FN:23,TP:141,Macc:0.81291893098,F1:0.817385765615\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2452 - acc: 0.8898 - val_loss: 0.4042 - val_acc: 0.8371\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:132,FP:39,FN:23,TP:141,Macc:0.8158429074,F1:0.819761901369\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2453 - acc: 0.8896 - val_loss: 0.4206 - val_acc: 0.8311\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:133,FP:38,FN:27,TP:137,Macc:0.806571762687,F1:0.808254040396\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2448 - acc: 0.8897 - val_loss: 0.4337 - val_acc: 0.8327\n",
      "9639/9639 [==============================] - 3s 343us/step\n",
      "TN:132,FP:39,FN:27,TP:137,Macc:0.803647786267,F1:0.805876807685\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2441 - acc: 0.8901 - val_loss: 0.4046 - val_acc: 0.8337\n",
      "9639/9639 [==============================] - 3s 340us/step\n",
      "TN:121,FP:50,FN:14,TP:150,Macc:0.811118189329,F1:0.824170328197\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2426 - acc: 0.8908 - val_loss: 0.3990 - val_acc: 0.8395\n",
      "9639/9639 [==============================] - 3s 342us/step\n",
      "TN:130,FP:41,FN:21,TP:143,Macc:0.816092515126,F1:0.821833546707\n",
      "Loss: 0.392634\n",
      "Iteration No: 66 ended. Search finished for the next optimal point.\n",
      "Time taken: 2725.8483\n",
      "Function value obtained: 0.3926\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 67 started. Searching for the next optimal point.\n",
      "args [4, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 62s 52ms/step - loss: 0.7888 - acc: 0.8320 - val_loss: 0.6520 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 24s 2ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3234 - acc: 0.8326 - val_loss: 0.6111 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3066 - acc: 0.8326 - val_loss: 0.6637 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.3030 - acc: 0.8326 - val_loss: 0.6419 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2966 - acc: 0.8347 - val_loss: 0.6575 - val_acc: 0.6214\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:43,FP:128,FN:13,TP:151,Macc:0.586096808844,F1:0.681710412443\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2887 - acc: 0.8596 - val_loss: 0.6616 - val_acc: 0.6112\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:41,FP:130,FN:8,TP:156,Macc:0.595492757421,F1:0.693328203731\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2832 - acc: 0.8647 - val_loss: 0.6450 - val_acc: 0.6229\n",
      "9639/9639 [==============================] - 3s 361us/step\n",
      "TN:43,FP:128,FN:5,TP:159,Macc:0.610487051111,F1:0.705094653003\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2782 - acc: 0.8668 - val_loss: 0.6036 - val_acc: 0.6763\n",
      "9639/9639 [==============================] - 3s 358us/step\n",
      "TN:61,FP:110,FN:6,TP:158,Macc:0.660069846389,F1:0.731476263025\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2778 - acc: 0.8691 - val_loss: 0.5973 - val_acc: 0.6831\n",
      "9639/9639 [==============================] - 3s 362us/step\n",
      "TN:61,FP:110,FN:2,TP:162,Macc:0.672264967523,F1:0.743114066066\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2746 - acc: 0.8700 - val_loss: 0.5332 - val_acc: 0.7401\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:84,FP:87,FN:4,TP:160,Macc:0.733418864618,F1:0.778583490448\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2745 - acc: 0.8705 - val_loss: 0.5837 - val_acc: 0.6923\n",
      "9639/9639 [==============================] - 4s 364us/step\n",
      "TN:60,FP:111,FN:1,TP:163,Macc:0.672389771386,F1:0.74428704699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2719 - acc: 0.8729 - val_loss: 0.5210 - val_acc: 0.7514\n",
      "9639/9639 [==============================] - 3s 355us/step\n",
      "TN:86,FP:85,FN:2,TP:162,Macc:0.745364378025,F1:0.788315850024\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2697 - acc: 0.8729 - val_loss: 0.5707 - val_acc: 0.7034\n",
      "9639/9639 [==============================] - 3s 360us/step\n",
      "TN:65,FP:106,FN:2,TP:162,Macc:0.683960873203,F1:0.7499947806\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2690 - acc: 0.8736 - val_loss: 0.5581 - val_acc: 0.7192\n",
      "9639/9639 [==============================] - 3s 356us/step\n",
      "TN:73,FP:98,FN:5,TP:159,Macc:0.698206343714,F1:0.755339147006\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2691 - acc: 0.8733 - val_loss: 0.4864 - val_acc: 0.7844\n",
      "9639/9639 [==============================] - 3s 357us/step\n",
      "TN:102,FP:69,FN:5,TP:159,Macc:0.783001659897,F1:0.811219090909\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2670 - acc: 0.8748 - val_loss: 0.5018 - val_acc: 0.7674\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:97,FP:74,FN:4,TP:160,Macc:0.771430558079,F1:0.804014726229\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2642 - acc: 0.8759 - val_loss: 0.5354 - val_acc: 0.7390\n",
      "9639/9639 [==============================] - 3s 356us/step\n",
      "TN:81,FP:90,FN:3,TP:161,Macc:0.727695715641,F1:0.775898315077\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2649 - acc: 0.8763 - val_loss: 0.5522 - val_acc: 0.7232\n",
      "9639/9639 [==============================] - 3s 355us/step\n",
      "TN:74,FP:97,FN:3,TP:161,Macc:0.707227880701,F1:0.763027908506\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2647 - acc: 0.8758 - val_loss: 0.5474 - val_acc: 0.7295\n",
      "9639/9639 [==============================] - 4s 364us/step\n",
      "TN:77,FP:94,FN:3,TP:161,Macc:0.715999809961,F1:0.768491139161\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2629 - acc: 0.8767 - val_loss: 0.5695 - val_acc: 0.7069\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:67,FP:104,FN:3,TP:161,Macc:0.68676004576,F1:0.750577516981\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2621 - acc: 0.8771 - val_loss: 0.5979 - val_acc: 0.6829\n",
      "9639/9639 [==============================] - 3s 357us/step\n",
      "TN:61,FP:110,FN:3,TP:161,Macc:0.669216187239,F1:0.740224680432\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2608 - acc: 0.8781 - val_loss: 0.5157 - val_acc: 0.7532\n",
      "9639/9639 [==============================] - 3s 360us/step\n",
      "TN:93,FP:78,FN:2,TP:162,Macc:0.765832212966,F1:0.80197484902\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2614 - acc: 0.8776 - val_loss: 0.5442 - val_acc: 0.7295\n",
      "9639/9639 [==============================] - 3s 358us/step\n",
      "TN:72,FP:99,FN:1,TP:163,Macc:0.707477488427,F1:0.765252967546\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2591 - acc: 0.8791 - val_loss: 0.4364 - val_acc: 0.8157\n",
      "9639/9639 [==============================] - 3s 358us/step\n",
      "TN:111,FP:60,FN:11,TP:153,Macc:0.791024765978,F1:0.811665633116\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2597 - acc: 0.8780 - val_loss: 0.5146 - val_acc: 0.7538\n",
      "9639/9639 [==============================] - 3s 361us/step\n",
      "TN:87,FP:84,FN:5,TP:159,Macc:0.739142013595,F1:0.781321446347\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2595 - acc: 0.8790 - val_loss: 0.4848 - val_acc: 0.7737\n",
      "9639/9639 [==============================] - 3s 357us/step\n",
      "TN:97,FP:74,FN:4,TP:160,Macc:0.771430558079,F1:0.804014726229\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2577 - acc: 0.8803 - val_loss: 0.5164 - val_acc: 0.7542\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:84,FP:87,FN:3,TP:161,Macc:0.736467644901,F1:0.781548084987\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2570 - acc: 0.8802 - val_loss: 0.4500 - val_acc: 0.8000\n",
      "9639/9639 [==============================] - 3s 355us/step\n",
      "TN:105,FP:66,FN:2,TP:162,Macc:0.800919930007,F1:0.826525212498\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2560 - acc: 0.8802 - val_loss: 0.4733 - val_acc: 0.7849\n",
      "9639/9639 [==============================] - 3s 361us/step\n",
      "TN:102,FP:69,FN:5,TP:159,Macc:0.783001659897,F1:0.811219090909\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2553 - acc: 0.8798 - val_loss: 0.4625 - val_acc: 0.7923\n",
      "9639/9639 [==============================] - 3s 359us/step\n",
      "TN:104,FP:67,FN:5,TP:159,Macc:0.788849612737,F1:0.815379208424\n",
      "Loss: 0.456463\n",
      "Iteration No: 67 ended. Search finished for the next optimal point.\n",
      "Time taken: 2822.4211\n",
      "Function value obtained: 0.4565\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 68 started. Searching for the next optimal point.\n",
      "args [1, 1, 9]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 62s 52ms/step - loss: 0.7536 - acc: 0.8465 - val_loss: 0.5600 - val_acc: 0.7619\n",
      "9639/9639 [==============================] - 24s 3ms/step\n",
      "TN:114,FP:57,FN:30,TP:134,Macc:0.741869869855,F1:0.754924062535\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2994 - acc: 0.8621 - val_loss: 0.5204 - val_acc: 0.7768\n",
      "9639/9639 [==============================] - 3s 362us/step\n",
      "TN:114,FP:57,FN:19,TP:145,Macc:0.775406452971,F1:0.792344238474\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2866 - acc: 0.8679 - val_loss: 0.5209 - val_acc: 0.7763\n",
      "9639/9639 [==============================] - 3s 351us/step\n",
      "TN:93,FP:78,FN:5,TP:159,Macc:0.756685872116,F1:0.793012095159\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2777 - acc: 0.8767 - val_loss: 0.5555 - val_acc: 0.7296\n",
      "9639/9639 [==============================] - 3s 357us/step\n",
      "TN:80,FP:91,FN:7,TP:157,Macc:0.712576618088,F1:0.762130610297\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2728 - acc: 0.8778 - val_loss: 0.4946 - val_acc: 0.7828\n",
      "9639/9639 [==============================] - 3s 355us/step\n",
      "TN:98,FP:73,FN:7,TP:157,Macc:0.76520819365,F1:0.796948924589\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2684 - acc: 0.8792 - val_loss: 0.4620 - val_acc: 0.8127\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:109,FP:62,FN:7,TP:157,Macc:0.797371934271,F1:0.819837908399\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2660 - acc: 0.8813 - val_loss: 0.4743 - val_acc: 0.7931\n",
      "9639/9639 [==============================] - 3s 355us/step\n",
      "TN:97,FP:74,FN:6,TP:158,Macc:0.765332997513,F1:0.797974415861\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2664 - acc: 0.8818 - val_loss: 0.4664 - val_acc: 0.7977\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:97,FP:74,FN:1,TP:163,Macc:0.780576898929,F1:0.812962218752\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2630 - acc: 0.8828 - val_loss: 0.4844 - val_acc: 0.7867\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:98,FP:73,FN:7,TP:157,Macc:0.76520819365,F1:0.796948924589\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2610 - acc: 0.8829 - val_loss: 0.4921 - val_acc: 0.7745\n",
      "9639/9639 [==============================] - 3s 356us/step\n",
      "TN:88,FP:83,FN:0,TP:164,Macc:0.757309891432,F1:0.7980482096\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2580 - acc: 0.8845 - val_loss: 0.4319 - val_acc: 0.8189\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:121,FP:50,FN:21,TP:143,Macc:0.789776727345,F1:0.801114935141\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2531 - acc: 0.8879 - val_loss: 0.4309 - val_acc: 0.8192\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:121,FP:50,FN:18,TP:146,Macc:0.798923068195,F1:0.811105605037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2534 - acc: 0.8877 - val_loss: 0.4146 - val_acc: 0.8327\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:120,FP:51,FN:8,TP:156,Macc:0.826486894608,F1:0.840964874455\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2534 - acc: 0.8867 - val_loss: 0.4102 - val_acc: 0.8388\n",
      "9639/9639 [==============================] - 3s 355us/step\n",
      "TN:143,FP:28,FN:28,TP:136,Macc:0.832762746604,F1:0.829262737098\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2531 - acc: 0.8863 - val_loss: 0.4144 - val_acc: 0.8416\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:145,FP:26,FN:27,TP:137,Macc:0.841659479728,F1:0.83791493301\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2513 - acc: 0.8866 - val_loss: 0.4549 - val_acc: 0.8077\n",
      "9639/9639 [==============================] - 3s 352us/step\n",
      "TN:156,FP:15,FN:48,TP:116,Macc:0.809798834399,F1:0.786435188179\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2511 - acc: 0.8869 - val_loss: 0.4771 - val_acc: 0.7816\n",
      "9639/9639 [==============================] - 3s 353us/step\n",
      "TN:162,FP:9,FN:59,TP:105,Macc:0.793806109803,F1:0.755390301626\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2497 - acc: 0.8877 - val_loss: 0.5021 - val_acc: 0.7208\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:160,FP:11,FN:71,TP:93,Macc:0.751372793563,F1:0.694024569487\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2481 - acc: 0.8882 - val_loss: 0.4884 - val_acc: 0.7367\n",
      "9639/9639 [==============================] - 3s 349us/step\n",
      "TN:160,FP:11,FN:70,TP:94,Macc:0.754421573847,F1:0.698879465829\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2489 - acc: 0.8862 - val_loss: 0.7662 - val_acc: 0.5106\n",
      "9639/9639 [==============================] - 3s 350us/step\n",
      "TN:171,FP:0,FN:151,TP:13,Macc:0.539634111519,F1:0.14689113998\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2465 - acc: 0.8896 - val_loss: 0.6663 - val_acc: 0.5314\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:171,FP:0,FN:136,TP:28,Macc:0.585365815769,F1:0.29166389282\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2437 - acc: 0.8898 - val_loss: 0.6481 - val_acc: 0.5400\n",
      "9639/9639 [==============================] - 3s 352us/step\n",
      "TN:165,FP:6,FN:129,TP:35,Macc:0.589163419232,F1:0.341459858025\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2457 - acc: 0.8896 - val_loss: 0.6248 - val_acc: 0.5466\n",
      "9639/9639 [==============================] - 3s 351us/step\n",
      "TN:167,FP:4,FN:130,TP:34,Macc:0.591962591789,F1:0.336630266674\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2433 - acc: 0.8906 - val_loss: 0.9071 - val_acc: 0.4978\n",
      "9639/9639 [==============================] - 3s 347us/step\n",
      "TN:171,FP:0,FN:153,TP:11,Macc:0.533536550953,F1:0.125712973988\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2450 - acc: 0.8894 - val_loss: 0.7341 - val_acc: 0.5246\n",
      "9639/9639 [==============================] - 3s 357us/step\n",
      "TN:169,FP:2,FN:139,TP:25,Macc:0.570371522079,F1:0.261777404264\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2449 - acc: 0.8889 - val_loss: 0.8161 - val_acc: 0.5106\n",
      "9639/9639 [==============================] - 3s 348us/step\n",
      "TN:171,FP:0,FN:149,TP:15,Macc:0.545731672086,F1:0.167596055695\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2434 - acc: 0.8904 - val_loss: 0.9606 - val_acc: 0.5058\n",
      "9639/9639 [==============================] - 3s 351us/step\n",
      "TN:171,FP:0,FN:151,TP:13,Macc:0.539634111519,F1:0.14689113998\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2433 - acc: 0.8893 - val_loss: 0.8820 - val_acc: 0.5090\n",
      "9639/9639 [==============================] - 3s 363us/step\n",
      "TN:171,FP:0,FN:148,TP:16,Macc:0.548780452369,F1:0.177775974338\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2400 - acc: 0.8920 - val_loss: 1.0001 - val_acc: 0.5024\n",
      "9639/9639 [==============================] - 3s 351us/step\n",
      "TN:171,FP:0,FN:152,TP:12,Macc:0.536585331236,F1:0.136362221604\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 50s 42ms/step - loss: 0.2423 - acc: 0.8902 - val_loss: 0.9321 - val_acc: 0.5122\n",
      "9639/9639 [==============================] - 3s 352us/step\n",
      "TN:171,FP:0,FN:148,TP:16,Macc:0.548780452369,F1:0.177775974338\n",
      "Loss: 0.926665\n",
      "Iteration No: 68 ended. Search finished for the next optimal point.\n",
      "Time taken: 2803.8110\n",
      "Function value obtained: 0.9267\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 69 started. Searching for the next optimal point.\n",
      "args [4, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 62s 51ms/step - loss: 0.7673 - acc: 0.8323 - val_loss: 0.6698 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 25s 3ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.3174 - acc: 0.8325 - val_loss: 0.5980 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.3057 - acc: 0.8326 - val_loss: 0.6233 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 365us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2998 - acc: 0.8327 - val_loss: 0.6039 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 365us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2977 - acc: 0.8325 - val_loss: 0.6015 - val_acc: 0.7283\n",
      "9639/9639 [==============================] - 4s 365us/step\n",
      "TN:81,FP:90,FN:0,TP:164,Macc:0.736842056491,F1:0.784683708924\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2945 - acc: 0.8329 - val_loss: 0.6252 - val_acc: 0.6823\n",
      "9639/9639 [==============================] - 4s 367us/step\n",
      "TN:57,FP:114,FN:0,TP:164,Macc:0.666666622409,F1:0.742076276934\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2933 - acc: 0.8313 - val_loss: 0.5714 - val_acc: 0.7677\n",
      "9639/9639 [==============================] - 4s 365us/step\n",
      "TN:93,FP:78,FN:0,TP:164,Macc:0.771929773532,F1:0.807876432659\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2901 - acc: 0.8329 - val_loss: 0.6293 - val_acc: 0.6736\n",
      "9639/9639 [==============================] - 4s 371us/step\n",
      "TN:52,FP:119,FN:0,TP:164,Macc:0.652046740308,F1:0.733775614347\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2889 - acc: 0.8410 - val_loss: 0.5606 - val_acc: 0.7782\n",
      "9639/9639 [==============================] - 4s 365us/step\n",
      "TN:96,FP:75,FN:0,TP:164,Macc:0.780701702793,F1:0.813890427733\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2873 - acc: 0.8407 - val_loss: 0.5619 - val_acc: 0.7713\n",
      "9639/9639 [==============================] - 4s 367us/step\n",
      "TN:94,FP:77,FN:0,TP:164,Macc:0.774853749952,F1:0.809871198059\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2876 - acc: 0.8370 - val_loss: 0.5742 - val_acc: 0.7538\n",
      "9639/9639 [==============================] - 4s 365us/step\n",
      "TN:85,FP:86,FN:0,TP:164,Macc:0.748537962171,F1:0.792265226668\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2857 - acc: 0.8448 - val_loss: 0.5732 - val_acc: 0.7535\n",
      "9639/9639 [==============================] - 4s 367us/step\n",
      "TN:84,FP:87,FN:0,TP:164,Macc:0.745613985751,F1:0.790356145635\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2865 - acc: 0.8453 - val_loss: 0.5445 - val_acc: 0.7866\n",
      "9639/9639 [==============================] - 4s 368us/step\n",
      "TN:102,FP:69,FN:6,TP:158,Macc:0.779952879613,F1:0.808178740572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2844 - acc: 0.8456 - val_loss: 0.5396 - val_acc: 0.7931\n",
      "9639/9639 [==============================] - 3s 363us/step\n",
      "TN:99,FP:72,FN:0,TP:164,Macc:0.789473632053,F1:0.819994633135\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2846 - acc: 0.8436 - val_loss: 0.5562 - val_acc: 0.7655\n",
      "9639/9639 [==============================] - 4s 366us/step\n",
      "TN:90,FP:81,FN:0,TP:164,Macc:0.763157844272,F1:0.801950662836\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2829 - acc: 0.8431 - val_loss: 0.5511 - val_acc: 0.7847\n",
      "9639/9639 [==============================] - 4s 365us/step\n",
      "TN:100,FP:71,FN:0,TP:164,Macc:0.792397608473,F1:0.822049766707\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2842 - acc: 0.8456 - val_loss: 0.5447 - val_acc: 0.7843\n",
      "9639/9639 [==============================] - 4s 367us/step\n",
      "TN:100,FP:71,FN:2,TP:162,Macc:0.786300047906,F1:0.816115527752\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2801 - acc: 0.8559 - val_loss: 0.5434 - val_acc: 0.7726\n",
      "9639/9639 [==============================] - 3s 360us/step\n",
      "TN:96,FP:75,FN:6,TP:158,Macc:0.762409021093,F1:0.795964395368\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2777 - acc: 0.8639 - val_loss: 0.5392 - val_acc: 0.7700\n",
      "9639/9639 [==============================] - 4s 366us/step\n",
      "TN:99,FP:72,FN:7,TP:157,Macc:0.76813217007,F1:0.798976794058\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2770 - acc: 0.8661 - val_loss: 0.5468 - val_acc: 0.7541\n",
      "9639/9639 [==============================] - 3s 363us/step\n",
      "TN:87,FP:84,FN:3,TP:161,Macc:0.745239574162,F1:0.787280736975\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2761 - acc: 0.8651 - val_loss: 0.5497 - val_acc: 0.7457\n",
      "9639/9639 [==============================] - 3s 363us/step\n",
      "TN:87,FP:84,FN:1,TP:163,Macc:0.751337134728,F1:0.793182029812\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2750 - acc: 0.8679 - val_loss: 0.5262 - val_acc: 0.7672\n",
      "9639/9639 [==============================] - 4s 364us/step\n",
      "TN:99,FP:72,FN:6,TP:158,Macc:0.771180950353,F1:0.802025066438\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2746 - acc: 0.8672 - val_loss: 0.5336 - val_acc: 0.7647\n",
      "9639/9639 [==============================] - 4s 365us/step\n",
      "TN:107,FP:64,FN:11,TP:153,Macc:0.779328860297,F1:0.803144166389\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2730 - acc: 0.8709 - val_loss: 0.5199 - val_acc: 0.7692\n",
      "9639/9639 [==============================] - 4s 366us/step\n",
      "TN:94,FP:77,FN:0,TP:164,Macc:0.774853749952,F1:0.809871198059\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2720 - acc: 0.8702 - val_loss: 0.5096 - val_acc: 0.7831\n",
      "9639/9639 [==============================] - 4s 365us/step\n",
      "TN:103,FP:68,FN:5,TP:159,Macc:0.785925636317,F1:0.813293829798\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 51s 42ms/step - loss: 0.2724 - acc: 0.8715 - val_loss: 0.5202 - val_acc: 0.7691\n",
      "9639/9639 [==============================] - 4s 364us/step\n",
      "TN:103,FP:68,FN:7,TP:157,Macc:0.77982807575,F1:0.807192533075\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2712 - acc: 0.8724 - val_loss: 0.5588 - val_acc: 0.7240\n",
      "9639/9639 [==============================] - 3s 363us/step\n",
      "TN:75,FP:96,FN:1,TP:163,Macc:0.716249417687,F1:0.770680316563\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2724 - acc: 0.8716 - val_loss: 0.5305 - val_acc: 0.7573\n",
      "9639/9639 [==============================] - 4s 364us/step\n",
      "TN:91,FP:80,FN:4,TP:160,Macc:0.753886699559,F1:0.792073859461\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2712 - acc: 0.8713 - val_loss: 0.5499 - val_acc: 0.7357\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:84,FP:87,FN:2,TP:162,Macc:0.739516425185,F1:0.784498323177\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2714 - acc: 0.8707 - val_loss: 0.5486 - val_acc: 0.7362\n",
      "9639/9639 [==============================] - 4s 364us/step\n",
      "TN:88,FP:83,FN:5,TP:159,Macc:0.742065990015,F1:0.783245892122\n",
      "Loss: 0.542394\n",
      "Iteration No: 69 ended. Search finished for the next optimal point.\n",
      "Time taken: 2854.2213\n",
      "Function value obtained: 0.5424\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 70 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.7590 - acc: 0.8454 - val_loss: 0.5487 - val_acc: 0.7584\n",
      "9639/9639 [==============================] - 25s 3ms/step\n",
      "TN:128,FP:43,FN:43,TP:121,Macc:0.743171396053,F1:0.737799328603\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3068 - acc: 0.8593 - val_loss: 0.5454 - val_acc: 0.7619\n",
      "9639/9639 [==============================] - 4s 389us/step\n",
      "TN:97,FP:74,FN:11,TP:153,Macc:0.750089096096,F1:0.782603294442\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2893 - acc: 0.8686 - val_loss: 0.5239 - val_acc: 0.7647\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:106,FP:65,FN:15,TP:149,Macc:0.764209762744,F1:0.788354338745\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2798 - acc: 0.8751 - val_loss: 0.5489 - val_acc: 0.7383\n",
      "9639/9639 [==============================] - 4s 377us/step\n",
      "TN:85,FP:86,FN:5,TP:159,Macc:0.733294060755,F1:0.777500786401\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2764 - acc: 0.8774 - val_loss: 0.5325 - val_acc: 0.7498\n",
      "9639/9639 [==============================] - 4s 380us/step\n",
      "TN:88,FP:83,FN:6,TP:158,Macc:0.739017209732,F1:0.780241570041\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2690 - acc: 0.8809 - val_loss: 0.5255 - val_acc: 0.7476\n",
      "9639/9639 [==============================] - 4s 371us/step\n",
      "TN:83,FP:88,FN:2,TP:162,Macc:0.736592448765,F1:0.782603391433\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2646 - acc: 0.8814 - val_loss: 0.5231 - val_acc: 0.7496\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:81,FP:90,FN:3,TP:161,Macc:0.727695715641,F1:0.775898315077\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2635 - acc: 0.8820 - val_loss: 0.4643 - val_acc: 0.8007\n",
      "9639/9639 [==============================] - 4s 379us/step\n",
      "TN:102,FP:69,FN:1,TP:163,Macc:0.79519678103,F1:0.823226939709\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2617 - acc: 0.8838 - val_loss: 0.4240 - val_acc: 0.8392\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:123,FP:48,FN:11,TP:153,Macc:0.826112483019,F1:0.838350670405\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2594 - acc: 0.8839 - val_loss: 0.5411 - val_acc: 0.7318\n",
      "9639/9639 [==============================] - 4s 381us/step\n",
      "TN:82,FP:89,FN:5,TP:159,Macc:0.724522131495,F1:0.771839347642\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2587 - acc: 0.8839 - val_loss: 0.4128 - val_acc: 0.8384\n",
      "9639/9639 [==============================] - 4s 371us/step\n",
      "TN:126,FP:45,FN:19,TP:145,Macc:0.810494170012,F1:0.819203518342\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2565 - acc: 0.8874 - val_loss: 0.4630 - val_acc: 0.8011\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:115,FP:56,FN:9,TP:155,Macc:0.808818232225,F1:0.826661204601\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2550 - acc: 0.8871 - val_loss: 0.4520 - val_acc: 0.8055\n",
      "9639/9639 [==============================] - 4s 369us/step\n",
      "TN:112,FP:59,FN:6,TP:158,Macc:0.809192643814,F1:0.829390884033\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2519 - acc: 0.8885 - val_loss: 0.4374 - val_acc: 0.8215\n",
      "9639/9639 [==============================] - 4s 371us/step\n",
      "TN:121,FP:50,FN:12,TP:152,Macc:0.817215749895,F1:0.830595602293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2548 - acc: 0.8859 - val_loss: 0.4430 - val_acc: 0.8256\n",
      "9639/9639 [==============================] - 4s 371us/step\n",
      "TN:120,FP:51,FN:7,TP:157,Macc:0.829535674892,F1:0.844080548566\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2524 - acc: 0.8874 - val_loss: 0.4407 - val_acc: 0.8122\n",
      "9639/9639 [==============================] - 4s 369us/step\n",
      "TN:108,FP:63,FN:4,TP:160,Macc:0.8035942987,F1:0.826867965876\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2512 - acc: 0.8873 - val_loss: 0.4173 - val_acc: 0.8451\n",
      "9639/9639 [==============================] - 4s 371us/step\n",
      "TN:133,FP:38,FN:16,TP:148,Macc:0.840108345803,F1:0.845708754321\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2479 - acc: 0.8888 - val_loss: 0.5159 - val_acc: 0.7448\n",
      "9639/9639 [==============================] - 4s 369us/step\n",
      "TN:83,FP:88,FN:4,TP:160,Macc:0.730494888198,F1:0.776693716314\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2511 - acc: 0.8876 - val_loss: 0.4172 - val_acc: 0.8343\n",
      "9639/9639 [==============================] - 4s 369us/step\n",
      "TN:119,FP:52,FN:5,TP:159,Macc:0.832709259038,F1:0.847994536682\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2498 - acc: 0.8871 - val_loss: 0.4139 - val_acc: 0.8537\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:135,FP:36,FN:17,TP:147,Macc:0.84290751836,F1:0.847256710647\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2484 - acc: 0.8877 - val_loss: 0.4547 - val_acc: 0.7955\n",
      "9639/9639 [==============================] - 4s 376us/step\n",
      "TN:111,FP:60,FN:7,TP:157,Macc:0.803219887111,F1:0.824141540504\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2464 - acc: 0.8891 - val_loss: 0.4038 - val_acc: 0.8472\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:125,FP:46,FN:8,TP:156,Macc:0.841106776709,F1:0.852453524476\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2456 - acc: 0.8905 - val_loss: 0.4268 - val_acc: 0.8127\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:107,FP:64,FN:3,TP:161,Macc:0.803719102564,F1:0.82775808461\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2455 - acc: 0.8896 - val_loss: 0.4803 - val_acc: 0.7887\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:106,FP:65,FN:6,TP:158,Macc:0.791648785294,F1:0.816532049151\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2456 - acc: 0.8903 - val_loss: 0.4399 - val_acc: 0.8111\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:115,FP:56,FN:6,TP:158,Macc:0.817964573074,F1:0.835973383591\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2452 - acc: 0.8904 - val_loss: 0.4370 - val_acc: 0.8232\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:123,FP:48,FN:12,TP:152,Macc:0.823063702735,F1:0.835159338521\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2449 - acc: 0.8890 - val_loss: 0.4102 - val_acc: 0.8509\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:136,FP:35,FN:14,TP:150,Macc:0.85497783563,F1:0.85959331963\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2442 - acc: 0.8909 - val_loss: 0.4277 - val_acc: 0.8465\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:141,FP:30,FN:24,TP:140,Macc:0.839109914897,F1:0.838317799885\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2450 - acc: 0.8906 - val_loss: 0.4383 - val_acc: 0.8399\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:127,FP:44,FN:11,TP:153,Macc:0.837808388699,F1:0.847639923701\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2442 - acc: 0.8891 - val_loss: 0.4507 - val_acc: 0.7984\n",
      "9639/9639 [==============================] - 4s 383us/step\n",
      "TN:107,FP:64,FN:4,TP:160,Macc:0.80067032228,F1:0.824736852835\n",
      "Loss: 0.445109\n",
      "Iteration No: 70 ended. Search finished for the next optimal point.\n",
      "Time taken: 2938.0481\n",
      "Function value obtained: 0.4451\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 71 started. Searching for the next optimal point.\n",
      "args [4, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 62s 52ms/step - loss: 0.7729 - acc: 0.8323 - val_loss: 0.6676 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 26s 3ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.3192 - acc: 0.8326 - val_loss: 0.5971 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3068 - acc: 0.8326 - val_loss: 0.6331 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2993 - acc: 0.8326 - val_loss: 0.5857 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 371us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2985 - acc: 0.8322 - val_loss: 0.5931 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2934 - acc: 0.8327 - val_loss: 0.5921 - val_acc: 0.7440\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:81,FP:90,FN:0,TP:164,Macc:0.736842056491,F1:0.784683708924\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2937 - acc: 0.8329 - val_loss: 0.5919 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2913 - acc: 0.8362 - val_loss: 0.6104 - val_acc: 0.7010\n",
      "9639/9639 [==============================] - 4s 376us/step\n",
      "TN:62,FP:109,FN:0,TP:164,Macc:0.681286504509,F1:0.750566886808\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2901 - acc: 0.8384 - val_loss: 0.5680 - val_acc: 0.7672\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:91,FP:80,FN:1,TP:163,Macc:0.763033040409,F1:0.80097746494\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2885 - acc: 0.8393 - val_loss: 0.5996 - val_acc: 0.7199\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:69,FP:102,FN:0,TP:164,Macc:0.70175433945,F1:0.762785468159\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2890 - acc: 0.8367 - val_loss: 0.5689 - val_acc: 0.7602\n",
      "9639/9639 [==============================] - 4s 377us/step\n",
      "TN:89,FP:82,FN:0,TP:164,Macc:0.760233867852,F1:0.799994677108\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2860 - acc: 0.8400 - val_loss: 0.5307 - val_acc: 0.8107\n",
      "9639/9639 [==============================] - 4s 369us/step\n",
      "TN:105,FP:66,FN:1,TP:163,Macc:0.80396871029,F1:0.829511143493\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2859 - acc: 0.8411 - val_loss: 0.5337 - val_acc: 0.8083\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:107,FP:64,FN:2,TP:162,Macc:0.806767882847,F1:0.830763822941\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2853 - acc: 0.8405 - val_loss: 0.5502 - val_acc: 0.7852\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:99,FP:72,FN:0,TP:164,Macc:0.789473632053,F1:0.819994633135\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2853 - acc: 0.8430 - val_loss: 0.5501 - val_acc: 0.7863\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:98,FP:73,FN:1,TP:163,Macc:0.783500875349,F1:0.81499463341\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2830 - acc: 0.8406 - val_loss: 0.5560 - val_acc: 0.7781\n",
      "9639/9639 [==============================] - 4s 368us/step\n",
      "TN:95,FP:76,FN:1,TP:163,Macc:0.774728946089,F1:0.808927648847\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2839 - acc: 0.8390 - val_loss: 0.5461 - val_acc: 0.7915\n",
      "9639/9639 [==============================] - 4s 381us/step\n",
      "TN:104,FP:67,FN:2,TP:162,Macc:0.797995953587,F1:0.824422085254\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2824 - acc: 0.8420 - val_loss: 0.5229 - val_acc: 0.8226\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:109,FP:62,FN:1,TP:163,Macc:0.81566461597,F1:0.838040860378\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2826 - acc: 0.8419 - val_loss: 0.5364 - val_acc: 0.7965\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:102,FP:69,FN:1,TP:163,Macc:0.79519678103,F1:0.823226939709\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2806 - acc: 0.8452 - val_loss: 0.5454 - val_acc: 0.7882\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:101,FP:70,FN:4,TP:160,Macc:0.78312646376,F1:0.812177350134\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2812 - acc: 0.8424 - val_loss: 0.5324 - val_acc: 0.8059\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:110,FP:61,FN:5,TP:159,Macc:0.806393471257,F1:0.828119569561\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2797 - acc: 0.8441 - val_loss: 0.5247 - val_acc: 0.8095\n",
      "9639/9639 [==============================] - 4s 369us/step\n",
      "TN:104,FP:67,FN:2,TP:162,Macc:0.797995953587,F1:0.824422085254\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2791 - acc: 0.8447 - val_loss: 0.5151 - val_acc: 0.8202\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:112,FP:59,FN:5,TP:159,Macc:0.812241424097,F1:0.832455294983\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2799 - acc: 0.8450 - val_loss: 0.5388 - val_acc: 0.7939\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:102,FP:69,FN:1,TP:163,Macc:0.79519678103,F1:0.823226939709\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2792 - acc: 0.8454 - val_loss: 0.5233 - val_acc: 0.8097\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:106,FP:65,FN:2,TP:162,Macc:0.803843906427,F1:0.828639097476\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2793 - acc: 0.8449 - val_loss: 0.5189 - val_acc: 0.8122\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:105,FP:66,FN:1,TP:163,Macc:0.80396871029,F1:0.829511143493\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2786 - acc: 0.8459 - val_loss: 0.5244 - val_acc: 0.8081\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:103,FP:68,FN:1,TP:163,Macc:0.79812075745,F1:0.825311068004\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2775 - acc: 0.8468 - val_loss: 0.5156 - val_acc: 0.8223\n",
      "9639/9639 [==============================] - 4s 369us/step\n",
      "TN:114,FP:57,FN:7,TP:157,Macc:0.811991816371,F1:0.830682378608\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2792 - acc: 0.8447 - val_loss: 0.5267 - val_acc: 0.7991\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:101,FP:70,FN:1,TP:163,Macc:0.79227280461,F1:0.821153310848\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2783 - acc: 0.8456 - val_loss: 0.5135 - val_acc: 0.8207\n",
      "9639/9639 [==============================] - 4s 368us/step\n",
      "TN:111,FP:60,FN:4,TP:160,Macc:0.812366227961,F1:0.833327902596\n",
      "Loss: 0.507467\n",
      "Iteration No: 71 ended. Search finished for the next optimal point.\n",
      "Time taken: 2924.1925\n",
      "Function value obtained: 0.5075\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 72 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 64s 53ms/step - loss: 0.7568 - acc: 0.8491 - val_loss: 0.5984 - val_acc: 0.7496\n",
      "9639/9639 [==============================] - 26s 3ms/step\n",
      "TN:99,FP:72,FN:12,TP:152,Macc:0.752888268653,F1:0.783499741773\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3011 - acc: 0.8661 - val_loss: 0.6112 - val_acc: 0.6815\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:60,FP:111,FN:4,TP:160,Macc:0.663243430536,F1:0.735626979516\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2855 - acc: 0.8711 - val_loss: 0.5934 - val_acc: 0.6982\n",
      "9639/9639 [==============================] - 4s 375us/step\n",
      "TN:63,FP:108,FN:0,TP:164,Macc:0.684210480929,F1:0.752288377529\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2792 - acc: 0.8742 - val_loss: 0.6191 - val_acc: 0.6697\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:54,FP:117,FN:0,TP:164,Macc:0.657894693149,F1:0.737073495483\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2735 - acc: 0.8782 - val_loss: 0.5439 - val_acc: 0.7369\n",
      "9639/9639 [==============================] - 4s 368us/step\n",
      "TN:80,FP:91,FN:9,TP:155,Macc:0.706479057521,F1:0.756092240441\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2682 - acc: 0.8806 - val_loss: 0.4718 - val_acc: 0.7919\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:101,FP:70,FN:6,TP:158,Macc:0.777028903193,F1:0.806117050379\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2649 - acc: 0.8827 - val_loss: 0.4749 - val_acc: 0.7895\n",
      "9639/9639 [==============================] - 4s 371us/step\n",
      "TN:98,FP:73,FN:5,TP:159,Macc:0.771305754216,F1:0.80302492063\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2615 - acc: 0.8834 - val_loss: 0.5040 - val_acc: 0.7593\n",
      "9639/9639 [==============================] - 4s 382us/step\n",
      "TN:87,FP:84,FN:3,TP:161,Macc:0.745239574162,F1:0.787280736975\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2578 - acc: 0.8845 - val_loss: 0.5053 - val_acc: 0.7605\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:87,FP:84,FN:5,TP:159,Macc:0.739142013595,F1:0.781321446347\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2543 - acc: 0.8881 - val_loss: 0.4460 - val_acc: 0.8072\n",
      "9639/9639 [==============================] - 4s 382us/step\n",
      "TN:104,FP:67,FN:5,TP:159,Macc:0.788849612737,F1:0.815379208424\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2551 - acc: 0.8878 - val_loss: 0.4095 - val_acc: 0.8330\n",
      "9639/9639 [==============================] - 4s 375us/step\n",
      "TN:118,FP:53,FN:8,TP:156,Macc:0.820638941768,F1:0.836455656757\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2531 - acc: 0.8883 - val_loss: 0.4583 - val_acc: 0.7938\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:97,FP:74,FN:1,TP:163,Macc:0.780576898929,F1:0.812962218752\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2513 - acc: 0.8882 - val_loss: 0.4358 - val_acc: 0.8100\n",
      "9639/9639 [==============================] - 4s 381us/step\n",
      "TN:108,FP:63,FN:6,TP:158,Macc:0.797496738134,F1:0.820773794469\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2525 - acc: 0.8876 - val_loss: 0.4392 - val_acc: 0.8058\n",
      "9639/9639 [==============================] - 4s 371us/step\n",
      "TN:104,FP:67,FN:5,TP:159,Macc:0.788849612737,F1:0.815379208424\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2500 - acc: 0.8884 - val_loss: 0.4080 - val_acc: 0.8308\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:111,FP:60,FN:3,TP:161,Macc:0.815415008244,F1:0.836358209163\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2488 - acc: 0.8894 - val_loss: 0.4307 - val_acc: 0.8163\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:103,FP:68,FN:1,TP:163,Macc:0.79812075745,F1:0.825311068004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2476 - acc: 0.8914 - val_loss: 0.4911 - val_acc: 0.7580\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:82,FP:89,FN:1,TP:163,Macc:0.736717252628,F1:0.783648550863\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2462 - acc: 0.8915 - val_loss: 0.4225 - val_acc: 0.8136\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:108,FP:63,FN:5,TP:159,Macc:0.800545518417,F1:0.82382877415\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2457 - acc: 0.8914 - val_loss: 0.4185 - val_acc: 0.8181\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:107,FP:64,FN:5,TP:159,Macc:0.797621541997,F1:0.821700007513\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2457 - acc: 0.8898 - val_loss: 0.4531 - val_acc: 0.7940\n",
      "9639/9639 [==============================] - 4s 380us/step\n",
      "TN:96,FP:75,FN:1,TP:163,Macc:0.777652922509,F1:0.810939915656\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2450 - acc: 0.8906 - val_loss: 0.4132 - val_acc: 0.8228\n",
      "9639/9639 [==============================] - 4s 367us/step\n",
      "TN:110,FP:61,FN:5,TP:159,Macc:0.806393471257,F1:0.828119569561\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2421 - acc: 0.8927 - val_loss: 0.4140 - val_acc: 0.8164\n",
      "9639/9639 [==============================] - 4s 372us/step\n",
      "TN:104,FP:67,FN:3,TP:161,Macc:0.794947173303,F1:0.821423171968\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2432 - acc: 0.8908 - val_loss: 0.3947 - val_acc: 0.8376\n",
      "9639/9639 [==============================] - 4s 369us/step\n",
      "TN:115,FP:56,FN:5,TP:159,Macc:0.821013353358,F1:0.839044682848\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2421 - acc: 0.8928 - val_loss: 0.4333 - val_acc: 0.8018\n",
      "9639/9639 [==============================] - 4s 375us/step\n",
      "TN:102,FP:69,FN:3,TP:161,Macc:0.789099220463,F1:0.817253491983\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2414 - acc: 0.8926 - val_loss: 0.3874 - val_acc: 0.8460\n",
      "9639/9639 [==============================] - 4s 369us/step\n",
      "TN:124,FP:47,FN:8,TP:156,Macc:0.838182800289,F1:0.850130750965\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2406 - acc: 0.8931 - val_loss: 0.4040 - val_acc: 0.8231\n",
      "9639/9639 [==============================] - 4s 377us/step\n",
      "TN:109,FP:62,FN:4,TP:160,Macc:0.806518275121,F1:0.829010121005\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2403 - acc: 0.8915 - val_loss: 0.4257 - val_acc: 0.8083\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:102,FP:69,FN:5,TP:159,Macc:0.783001659897,F1:0.811219090909\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2406 - acc: 0.8933 - val_loss: 0.4554 - val_acc: 0.7855\n",
      "9639/9639 [==============================] - 4s 368us/step\n",
      "TN:98,FP:73,FN:1,TP:163,Macc:0.783500875349,F1:0.81499463341\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2397 - acc: 0.8935 - val_loss: 0.4672 - val_acc: 0.7731\n",
      "9639/9639 [==============================] - 4s 373us/step\n",
      "TN:89,FP:82,FN:3,TP:161,Macc:0.751087527002,F1:0.791149455643\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2415 - acc: 0.8924 - val_loss: 0.4500 - val_acc: 0.7872\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:96,FP:75,FN:1,TP:163,Macc:0.777652922509,F1:0.810939915656\n",
      "Loss: 0.444370\n",
      "Iteration No: 72 ended. Search finished for the next optimal point.\n",
      "Time taken: 2959.4298\n",
      "Function value obtained: 0.4444\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 73 started. Searching for the next optimal point.\n",
      "args [4, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 79s 66ms/step - loss: 0.7899 - acc: 0.8318 - val_loss: 0.6996 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 27s 3ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3209 - acc: 0.8324 - val_loss: 0.6474 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 392us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3081 - acc: 0.8326 - val_loss: 0.6507 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 379us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3019 - acc: 0.8327 - val_loss: 0.6184 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 382us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3008 - acc: 0.8324 - val_loss: 0.5997 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 380us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2961 - acc: 0.8327 - val_loss: 0.6098 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2934 - acc: 0.8327 - val_loss: 0.5923 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 380us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2936 - acc: 0.8362 - val_loss: 0.5794 - val_acc: 0.7552\n",
      "9639/9639 [==============================] - 4s 379us/step\n",
      "TN:90,FP:81,FN:1,TP:163,Macc:0.760109063989,F1:0.799014276251\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2909 - acc: 0.8355 - val_loss: 0.5929 - val_acc: 0.7330\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:81,FP:90,FN:0,TP:164,Macc:0.736842056491,F1:0.784683708924\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2882 - acc: 0.8390 - val_loss: 0.5590 - val_acc: 0.7810\n",
      "9639/9639 [==============================] - 4s 385us/step\n",
      "TN:98,FP:73,FN:2,TP:162,Macc:0.780452095066,F1:0.812024704603\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2894 - acc: 0.8372 - val_loss: 0.5668 - val_acc: 0.7675\n",
      "9639/9639 [==============================] - 4s 380us/step\n",
      "TN:92,FP:79,FN:0,TP:164,Macc:0.769005797112,F1:0.805891469588\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2864 - acc: 0.8392 - val_loss: 0.5595 - val_acc: 0.7820\n",
      "9639/9639 [==============================] - 4s 380us/step\n",
      "TN:104,FP:67,FN:5,TP:159,Macc:0.788849612737,F1:0.815379208424\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2857 - acc: 0.8409 - val_loss: 0.6023 - val_acc: 0.7106\n",
      "9639/9639 [==============================] - 4s 384us/step\n",
      "TN:69,FP:102,FN:0,TP:164,Macc:0.70175433945,F1:0.762785468159\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2846 - acc: 0.8407 - val_loss: 0.5626 - val_acc: 0.7728\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:96,FP:75,FN:2,TP:162,Macc:0.774604142226,F1:0.807974687854\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2867 - acc: 0.8409 - val_loss: 0.5646 - val_acc: 0.7640\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:92,FP:79,FN:0,TP:164,Macc:0.769005797112,F1:0.805891469588\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2838 - acc: 0.8396 - val_loss: 0.5491 - val_acc: 0.7937\n",
      "9639/9639 [==============================] - 4s 376us/step\n",
      "TN:109,FP:62,FN:6,TP:158,Macc:0.800420714554,F1:0.822911236526\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2825 - acc: 0.8425 - val_loss: 0.5316 - val_acc: 0.8110\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:110,FP:61,FN:4,TP:160,Macc:0.809442251541,F1:0.831163404265\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2823 - acc: 0.8414 - val_loss: 0.5505 - val_acc: 0.7847\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:95,FP:76,FN:1,TP:163,Macc:0.774728946089,F1:0.808927648847\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2831 - acc: 0.8407 - val_loss: 0.5574 - val_acc: 0.7890\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:102,FP:69,FN:8,TP:156,Macc:0.773855319047,F1:0.802051145191\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2814 - acc: 0.8434 - val_loss: 0.5419 - val_acc: 0.8061\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:110,FP:61,FN:7,TP:157,Macc:0.800295910691,F1:0.821984091398\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2819 - acc: 0.8428 - val_loss: 0.5559 - val_acc: 0.7900\n",
      "9639/9639 [==============================] - 4s 376us/step\n",
      "TN:102,FP:69,FN:6,TP:158,Macc:0.779952879613,F1:0.808178740572\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2795 - acc: 0.8466 - val_loss: 0.5408 - val_acc: 0.7933\n",
      "9639/9639 [==============================] - 4s 384us/step\n",
      "TN:104,FP:67,FN:3,TP:161,Macc:0.794947173303,F1:0.821423171968\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2804 - acc: 0.8440 - val_loss: 0.5624 - val_acc: 0.7854\n",
      "9639/9639 [==============================] - 4s 377us/step\n",
      "TN:103,FP:68,FN:9,TP:155,Macc:0.773730515183,F1:0.801028174064\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2814 - acc: 0.8436 - val_loss: 0.5587 - val_acc: 0.7956\n",
      "9639/9639 [==============================] - 4s 382us/step\n",
      "TN:107,FP:64,FN:9,TP:155,Macc:0.785426420864,F1:0.80939404477\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2795 - acc: 0.8461 - val_loss: 0.5513 - val_acc: 0.7762\n",
      "9639/9639 [==============================] - 4s 375us/step\n",
      "TN:94,FP:77,FN:1,TP:163,Macc:0.771804969669,F1:0.8069253438\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2805 - acc: 0.8441 - val_loss: 0.5416 - val_acc: 0.7957\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:105,FP:66,FN:5,TP:159,Macc:0.791773589157,F1:0.817475308842\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2803 - acc: 0.8461 - val_loss: 0.5432 - val_acc: 0.7852\n",
      "9639/9639 [==============================] - 4s 390us/step\n",
      "TN:98,FP:73,FN:0,TP:164,Macc:0.786549655633,F1:0.817949749651\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2777 - acc: 0.8567 - val_loss: 0.5578 - val_acc: 0.7863\n",
      "9639/9639 [==============================] - 4s 385us/step\n",
      "TN:111,FP:60,FN:22,TP:142,Macc:0.757488182861,F1:0.775950796837\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2732 - acc: 0.8664 - val_loss: 0.5288 - val_acc: 0.7873\n",
      "9639/9639 [==============================] - 4s 379us/step\n",
      "TN:106,FP:65,FN:9,TP:155,Macc:0.782502444444,F1:0.807286237422\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2730 - acc: 0.8674 - val_loss: 0.5259 - val_acc: 0.7859\n",
      "9639/9639 [==============================] - 4s 378us/step\n",
      "TN:107,FP:64,FN:11,TP:153,Macc:0.779328860297,F1:0.803144166389\n",
      "Loss: 0.520271\n",
      "Iteration No: 73 ended. Search finished for the next optimal point.\n",
      "Time taken: 3038.4937\n",
      "Function value obtained: 0.5203\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 74 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 67s 56ms/step - loss: 0.7547 - acc: 0.8491 - val_loss: 0.5931 - val_acc: 0.7475\n",
      "9639/9639 [==============================] - 27s 3ms/step\n",
      "TN:83,FP:88,FN:3,TP:161,Macc:0.733543668481,F1:0.779655708423\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2996 - acc: 0.8658 - val_loss: 0.5975 - val_acc: 0.7046\n",
      "9639/9639 [==============================] - 4s 400us/step\n",
      "TN:80,FP:91,FN:11,TP:153,Macc:0.700381496955,F1:0.749994671053\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2826 - acc: 0.8751 - val_loss: 0.6156 - val_acc: 0.6655\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:49,FP:122,FN:4,TP:160,Macc:0.631079689915,F1:0.717483638879\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2730 - acc: 0.8784 - val_loss: 0.6196 - val_acc: 0.6627\n",
      "9639/9639 [==============================] - 4s 385us/step\n",
      "TN:46,FP:125,FN:0,TP:164,Macc:0.634502881788,F1:0.724056693806\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2659 - acc: 0.8819 - val_loss: 0.5669 - val_acc: 0.7093\n",
      "9639/9639 [==============================] - 4s 385us/step\n",
      "TN:68,FP:103,FN:0,TP:164,Macc:0.69883036303,F1:0.761015656971\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2593 - acc: 0.8853 - val_loss: 0.6187 - val_acc: 0.6613\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:48,FP:123,FN:0,TP:164,Macc:0.640350834628,F1:0.727267600923\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2588 - acc: 0.8846 - val_loss: 0.5952 - val_acc: 0.6846\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:53,FP:118,FN:0,TP:164,Macc:0.654970716728,F1:0.735420857724\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2578 - acc: 0.8856 - val_loss: 0.5571 - val_acc: 0.7146\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:67,FP:104,FN:0,TP:164,Macc:0.69590638661,F1:0.759254039387\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2548 - acc: 0.8857 - val_loss: 0.5537 - val_acc: 0.7151\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:68,FP:103,FN:0,TP:164,Macc:0.69883036303,F1:0.761015656971\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2539 - acc: 0.8868 - val_loss: 0.4984 - val_acc: 0.7555\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:86,FP:85,FN:4,TP:160,Macc:0.739266817458,F1:0.782390761688\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2513 - acc: 0.8867 - val_loss: 0.5649 - val_acc: 0.6914\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:55,FP:116,FN:6,TP:158,Macc:0.642525987869,F1:0.72145599791\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2491 - acc: 0.8894 - val_loss: 0.5132 - val_acc: 0.7279\n",
      "9639/9639 [==============================] - 4s 385us/step\n",
      "TN:73,FP:98,FN:2,TP:162,Macc:0.707352684564,F1:0.764145685733\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2468 - acc: 0.8892 - val_loss: 0.5782 - val_acc: 0.6937\n",
      "9639/9639 [==============================] - 4s 382us/step\n",
      "TN:59,FP:112,FN:0,TP:164,Macc:0.672514575249,F1:0.745449364581\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2464 - acc: 0.8899 - val_loss: 0.4940 - val_acc: 0.7459\n",
      "9639/9639 [==============================] - 4s 382us/step\n",
      "TN:81,FP:90,FN:4,TP:160,Macc:0.724646935358,F1:0.772941556198\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2445 - acc: 0.8917 - val_loss: 0.6086 - val_acc: 0.6679\n",
      "9639/9639 [==============================] - 4s 390us/step\n",
      "TN:46,FP:125,FN:1,TP:163,Macc:0.631454101504,F1:0.721233816917\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2444 - acc: 0.8912 - val_loss: 0.4900 - val_acc: 0.7406\n",
      "9639/9639 [==============================] - 4s 384us/step\n",
      "TN:73,FP:98,FN:3,TP:161,Macc:0.70430390428,F1:0.76122405228\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2416 - acc: 0.8927 - val_loss: 0.5854 - val_acc: 0.6839\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:54,FP:117,FN:1,TP:163,Macc:0.654845912865,F1:0.734229073305\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2406 - acc: 0.8937 - val_loss: 0.4997 - val_acc: 0.7312\n",
      "9639/9639 [==============================] - 4s 384us/step\n",
      "TN:72,FP:99,FN:3,TP:161,Macc:0.70137992786,F1:0.759428704846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2391 - acc: 0.8934 - val_loss: 0.5043 - val_acc: 0.7277\n",
      "9639/9639 [==============================] - 4s 385us/step\n",
      "TN:70,FP:101,FN:5,TP:159,Macc:0.689434414454,F1:0.749994743072\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2395 - acc: 0.8928 - val_loss: 0.5079 - val_acc: 0.7262\n",
      "9639/9639 [==============================] - 4s 381us/step\n",
      "TN:66,FP:105,FN:0,TP:164,Macc:0.69298241019,F1:0.757500558639\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2380 - acc: 0.8941 - val_loss: 0.4775 - val_acc: 0.7469\n",
      "9639/9639 [==============================] - 4s 382us/step\n",
      "TN:77,FP:94,FN:3,TP:161,Macc:0.715999809961,F1:0.768491139161\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2380 - acc: 0.8940 - val_loss: 0.4642 - val_acc: 0.7609\n",
      "9639/9639 [==============================] - 4s 382us/step\n",
      "TN:88,FP:83,FN:6,TP:158,Macc:0.739017209732,F1:0.780241570041\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2376 - acc: 0.8945 - val_loss: 0.4692 - val_acc: 0.7578\n",
      "9639/9639 [==============================] - 4s 393us/step\n",
      "TN:85,FP:86,FN:4,TP:160,Macc:0.736342841038,F1:0.780482483034\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2362 - acc: 0.8947 - val_loss: 0.4647 - val_acc: 0.7562\n",
      "9639/9639 [==============================] - 4s 380us/step\n",
      "TN:85,FP:86,FN:4,TP:160,Macc:0.736342841038,F1:0.780482483034\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2373 - acc: 0.8949 - val_loss: 0.4742 - val_acc: 0.7499\n",
      "9639/9639 [==============================] - 4s 384us/step\n",
      "TN:87,FP:84,FN:7,TP:157,Macc:0.733044453028,F1:0.775303298704\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2352 - acc: 0.8960 - val_loss: 0.4683 - val_acc: 0.7590\n",
      "9639/9639 [==============================] - 4s 380us/step\n",
      "TN:90,FP:81,FN:7,TP:157,Macc:0.741816382289,F1:0.781089171022\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2346 - acc: 0.8949 - val_loss: 0.4945 - val_acc: 0.7326\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:78,FP:93,FN:8,TP:156,Macc:0.703679884964,F1:0.755442634654\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2336 - acc: 0.8956 - val_loss: 0.4481 - val_acc: 0.7696\n",
      "9639/9639 [==============================] - 4s 379us/step\n",
      "TN:89,FP:82,FN:6,TP:158,Macc:0.741941186152,F1:0.782172869902\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2346 - acc: 0.8958 - val_loss: 0.4462 - val_acc: 0.7846\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:100,FP:71,FN:10,TP:154,Macc:0.76190980564,F1:0.791768369423\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2351 - acc: 0.8950 - val_loss: 0.4249 - val_acc: 0.8011\n",
      "9639/9639 [==============================] - 4s 381us/step\n",
      "TN:103,FP:68,FN:11,TP:153,Macc:0.767632954617,F1:0.794799769981\n",
      "Loss: 0.418696\n",
      "Iteration No: 74 ended. Search finished for the next optimal point.\n",
      "Time taken: 3016.9278\n",
      "Function value obtained: 0.4187\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 75 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 64s 53ms/step - loss: 0.7590 - acc: 0.8454 - val_loss: 0.5517 - val_acc: 0.7571\n",
      "9639/9639 [==============================] - 28s 3ms/step\n",
      "TN:129,FP:42,FN:44,TP:120,Macc:0.74304659219,F1:0.736190769585\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3045 - acc: 0.8587 - val_loss: 0.4979 - val_acc: 0.8016\n",
      "9639/9639 [==============================] - 4s 398us/step\n",
      "TN:107,FP:64,FN:10,TP:154,Macc:0.78237764058,F1:0.806277286021\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2899 - acc: 0.8663 - val_loss: 0.6168 - val_acc: 0.6735\n",
      "9639/9639 [==============================] - 4s 390us/step\n",
      "TN:58,FP:113,FN:6,TP:158,Macc:0.651297917129,F1:0.726431577682\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2770 - acc: 0.8747 - val_loss: 0.4400 - val_acc: 0.8308\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:113,FP:58,FN:7,TP:157,Macc:0.809067839951,F1:0.828490593751\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2728 - acc: 0.8752 - val_loss: 0.4756 - val_acc: 0.7982\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:114,FP:57,FN:22,TP:142,Macc:0.766260112121,F1:0.782363649758\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2684 - acc: 0.8776 - val_loss: 0.5633 - val_acc: 0.7247\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:94,FP:77,FN:24,TP:140,Macc:0.701683023153,F1:0.734902700517\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2665 - acc: 0.8795 - val_loss: 0.4725 - val_acc: 0.7988\n",
      "9639/9639 [==============================] - 4s 402us/step\n",
      "TN:124,FP:47,FN:29,TP:135,Macc:0.774158414339,F1:0.780341286116\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2648 - acc: 0.8780 - val_loss: 0.4966 - val_acc: 0.7801\n",
      "9639/9639 [==============================] - 4s 389us/step\n",
      "TN:111,FP:60,FN:23,TP:141,Macc:0.754439402578,F1:0.772597249714\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2630 - acc: 0.8798 - val_loss: 0.6089 - val_acc: 0.6782\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:68,FP:103,FN:16,TP:148,Macc:0.650049878497,F1:0.713247715992\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2612 - acc: 0.8816 - val_loss: 0.4601 - val_acc: 0.7977\n",
      "9639/9639 [==============================] - 4s 399us/step\n",
      "TN:113,FP:58,FN:18,TP:146,Macc:0.775531256835,F1:0.793472778452\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2600 - acc: 0.8808 - val_loss: 0.4704 - val_acc: 0.7918\n",
      "9639/9639 [==============================] - 4s 389us/step\n",
      "TN:114,FP:57,FN:22,TP:142,Macc:0.766260112121,F1:0.782363649758\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2581 - acc: 0.8820 - val_loss: 0.4354 - val_acc: 0.8152\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:102,FP:69,FN:4,TP:160,Macc:0.78605044018,F1:0.814243968776\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2581 - acc: 0.8827 - val_loss: 0.4069 - val_acc: 0.8355\n",
      "9639/9639 [==============================] - 4s 394us/step\n",
      "TN:125,FP:46,FN:17,TP:147,Macc:0.813667754159,F1:0.823523897344\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2562 - acc: 0.8837 - val_loss: 0.4016 - val_acc: 0.8387\n",
      "9639/9639 [==============================] - 4s 390us/step\n",
      "TN:120,FP:51,FN:14,TP:150,Macc:0.808194212908,F1:0.821912315232\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2584 - acc: 0.8826 - val_loss: 0.4271 - val_acc: 0.8173\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:125,FP:46,FN:23,TP:141,Macc:0.795375072459,F1:0.803413276715\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2548 - acc: 0.8841 - val_loss: 0.4112 - val_acc: 0.8302\n",
      "9639/9639 [==============================] - 4s 393us/step\n",
      "TN:120,FP:51,FN:14,TP:150,Macc:0.808194212908,F1:0.821912315232\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2556 - acc: 0.8838 - val_loss: 0.4001 - val_acc: 0.8386\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:127,FP:44,FN:18,TP:146,Macc:0.816466926716,F1:0.824853235505\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2535 - acc: 0.8840 - val_loss: 0.3896 - val_acc: 0.8419\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:125,FP:46,FN:14,TP:150,Macc:0.822814095009,F1:0.8333278259\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2545 - acc: 0.8838 - val_loss: 0.3918 - val_acc: 0.8433\n",
      "9639/9639 [==============================] - 4s 398us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:124,FP:47,FN:11,TP:153,Macc:0.829036459439,F1:0.840653843683\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2509 - acc: 0.8864 - val_loss: 0.3953 - val_acc: 0.8362\n",
      "9639/9639 [==============================] - 4s 389us/step\n",
      "TN:116,FP:55,FN:9,TP:155,Macc:0.811742208645,F1:0.828871539828\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2504 - acc: 0.8862 - val_loss: 0.3861 - val_acc: 0.8425\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:123,FP:48,FN:14,TP:150,Macc:0.816966142169,F1:0.828723779957\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2509 - acc: 0.8865 - val_loss: 0.3779 - val_acc: 0.8484\n",
      "9639/9639 [==============================] - 4s 395us/step\n",
      "TN:124,FP:47,FN:11,TP:153,Macc:0.829036459439,F1:0.840653843683\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2494 - acc: 0.8873 - val_loss: 0.3872 - val_acc: 0.8348\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:114,FP:57,FN:9,TP:155,Macc:0.805894255804,F1:0.824462626535\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2483 - acc: 0.8862 - val_loss: 0.4297 - val_acc: 0.8321\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:134,FP:37,FN:25,TP:139,Macc:0.815593299673,F1:0.817641512805\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2494 - acc: 0.8858 - val_loss: 0.3873 - val_acc: 0.8428\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:128,FP:43,FN:16,TP:148,Macc:0.825488463703,F1:0.83379729708\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2464 - acc: 0.8880 - val_loss: 0.3872 - val_acc: 0.8414\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:119,FP:52,FN:9,TP:155,Macc:0.820514137905,F1:0.835574039195\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2480 - acc: 0.8869 - val_loss: 0.3719 - val_acc: 0.8461\n",
      "9639/9639 [==============================] - 4s 389us/step\n",
      "TN:121,FP:50,FN:14,TP:150,Macc:0.811118189329,F1:0.824170328197\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2468 - acc: 0.8880 - val_loss: 0.3875 - val_acc: 0.8381\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:127,FP:44,FN:14,TP:150,Macc:0.828662047849,F1:0.837983313977\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2466 - acc: 0.8874 - val_loss: 0.3761 - val_acc: 0.8462\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:128,FP:43,FN:17,TP:147,Macc:0.822439683419,F1:0.830502952668\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2470 - acc: 0.8875 - val_loss: 0.4028 - val_acc: 0.8374\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:139,FP:32,FN:29,TP:135,Macc:0.818018060641,F1:0.815704416061\n",
      "Loss: 0.397094\n",
      "Iteration No: 75 ended. Search finished for the next optimal point.\n",
      "Time taken: 3055.4174\n",
      "Function value obtained: 0.3971\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 76 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 85s 70ms/step - loss: 0.7542 - acc: 0.8495 - val_loss: 0.5736 - val_acc: 0.7630\n",
      "9639/9639 [==============================] - 29s 3ms/step\n",
      "TN:96,FP:75,FN:12,TP:152,Macc:0.744116339393,F1:0.777488205216\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.3021 - acc: 0.8649 - val_loss: 0.5796 - val_acc: 0.7116\n",
      "9639/9639 [==============================] - 4s 394us/step\n",
      "TN:75,FP:96,FN:9,TP:155,Macc:0.691859175421,F1:0.746982653961\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2873 - acc: 0.8720 - val_loss: 0.5677 - val_acc: 0.7215\n",
      "9639/9639 [==============================] - 4s 392us/step\n",
      "TN:89,FP:82,FN:17,TP:147,Macc:0.708404603036,F1:0.748086211668\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2798 - acc: 0.8747 - val_loss: 0.5110 - val_acc: 0.7719\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:89,FP:82,FN:3,TP:161,Macc:0.751087527002,F1:0.791149455643\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2759 - acc: 0.8768 - val_loss: 0.5287 - val_acc: 0.7484\n",
      "9639/9639 [==============================] - 4s 393us/step\n",
      "TN:83,FP:88,FN:2,TP:162,Macc:0.736592448765,F1:0.782603391433\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2684 - acc: 0.8806 - val_loss: 0.4780 - val_acc: 0.7867\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:98,FP:73,FN:5,TP:159,Macc:0.771305754216,F1:0.80302492063\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2671 - acc: 0.8813 - val_loss: 0.5134 - val_acc: 0.7526\n",
      "9639/9639 [==============================] - 4s 394us/step\n",
      "TN:82,FP:89,FN:1,TP:163,Macc:0.736717252628,F1:0.783648550863\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2643 - acc: 0.8814 - val_loss: 0.5022 - val_acc: 0.7633\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:84,FP:87,FN:0,TP:164,Macc:0.745613985751,F1:0.790356145635\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2598 - acc: 0.8832 - val_loss: 0.4969 - val_acc: 0.7619\n",
      "9639/9639 [==============================] - 4s 393us/step\n",
      "TN:86,FP:85,FN:4,TP:160,Macc:0.739266817458,F1:0.782390761688\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2589 - acc: 0.8829 - val_loss: 0.4344 - val_acc: 0.8149\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:106,FP:65,FN:4,TP:160,Macc:0.79774634586,F1:0.822616696726\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2587 - acc: 0.8830 - val_loss: 0.4733 - val_acc: 0.7824\n",
      "9639/9639 [==============================] - 4s 397us/step\n",
      "TN:93,FP:78,FN:0,TP:164,Macc:0.771929773532,F1:0.807876432659\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2578 - acc: 0.8846 - val_loss: 0.5082 - val_acc: 0.7526\n",
      "9639/9639 [==============================] - 4s 389us/step\n",
      "TN:89,FP:82,FN:5,TP:159,Macc:0.744989966435,F1:0.785179841377\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2558 - acc: 0.8848 - val_loss: 0.4441 - val_acc: 0.8067\n",
      "9639/9639 [==============================] - 4s 392us/step\n",
      "TN:110,FP:61,FN:8,TP:156,Macc:0.797247130407,F1:0.818892196975\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2546 - acc: 0.8861 - val_loss: 0.4786 - val_acc: 0.7690\n",
      "9639/9639 [==============================] - 4s 393us/step\n",
      "TN:86,FP:85,FN:2,TP:162,Macc:0.745364378025,F1:0.788315850024\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2554 - acc: 0.8850 - val_loss: 0.3938 - val_acc: 0.8441\n",
      "9639/9639 [==============================] - 4s 392us/step\n",
      "TN:127,FP:44,FN:11,TP:153,Macc:0.837808388699,F1:0.847639923701\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2540 - acc: 0.8853 - val_loss: 0.5093 - val_acc: 0.7440\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:79,FP:92,FN:2,TP:162,Macc:0.724896543084,F1:0.775114331438\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2506 - acc: 0.8867 - val_loss: 0.4162 - val_acc: 0.8273\n",
      "9639/9639 [==============================] - 4s 398us/step\n",
      "TN:116,FP:55,FN:7,TP:157,Macc:0.817839769211,F1:0.835100923784\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2521 - acc: 0.8867 - val_loss: 0.4113 - val_acc: 0.8309\n",
      "9639/9639 [==============================] - 4s 389us/step\n",
      "TN:114,FP:57,FN:10,TP:154,Macc:0.802845475521,F1:0.821327871581\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2514 - acc: 0.8864 - val_loss: 0.4215 - val_acc: 0.8175\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:109,FP:62,FN:7,TP:157,Macc:0.797371934271,F1:0.819837908399\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2505 - acc: 0.8857 - val_loss: 0.4236 - val_acc: 0.8315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9639/9639 [==============================] - 4s 397us/step\n",
      "TN:119,FP:52,FN:11,TP:153,Macc:0.814416577338,F1:0.829262811178\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2505 - acc: 0.8869 - val_loss: 0.4334 - val_acc: 0.8093\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:109,FP:62,FN:8,TP:156,Macc:0.794323153987,F1:0.816748489605\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2468 - acc: 0.8890 - val_loss: 0.4227 - val_acc: 0.8212\n",
      "9639/9639 [==============================] - 4s 389us/step\n",
      "TN:112,FP:59,FN:10,TP:154,Macc:0.796997522681,F1:0.816970672594\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2472 - acc: 0.8883 - val_loss: 0.4566 - val_acc: 0.7868\n",
      "9639/9639 [==============================] - 4s 399us/step\n",
      "TN:103,FP:68,FN:5,TP:159,Macc:0.785925636317,F1:0.813293829798\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2466 - acc: 0.8881 - val_loss: 0.4188 - val_acc: 0.8251\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:111,FP:60,FN:8,TP:156,Macc:0.800171106827,F1:0.821047187071\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2453 - acc: 0.8882 - val_loss: 0.4534 - val_acc: 0.7900\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:102,FP:69,FN:6,TP:158,Macc:0.779952879613,F1:0.808178740572\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2477 - acc: 0.8876 - val_loss: 0.5265 - val_acc: 0.7274\n",
      "9639/9639 [==============================] - 4s 393us/step\n",
      "TN:77,FP:94,FN:5,TP:159,Macc:0.709902249394,F1:0.762584638397\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2456 - acc: 0.8896 - val_loss: 0.4072 - val_acc: 0.8439\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:127,FP:44,FN:15,TP:149,Macc:0.825613267566,F1:0.834728378446\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2453 - acc: 0.8897 - val_loss: 0.4266 - val_acc: 0.8206\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:114,FP:57,FN:10,TP:154,Macc:0.802845475521,F1:0.821327871581\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2458 - acc: 0.8886 - val_loss: 0.4901 - val_acc: 0.7560\n",
      "9639/9639 [==============================] - 4s 393us/step\n",
      "TN:89,FP:82,FN:3,TP:161,Macc:0.751087527002,F1:0.791149455643\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2442 - acc: 0.8903 - val_loss: 0.4305 - val_acc: 0.8122\n",
      "9639/9639 [==============================] - 4s 381us/step\n",
      "TN:110,FP:61,FN:9,TP:155,Macc:0.794198350124,F1:0.815784029482\n",
      "Loss: 0.425190\n",
      "Iteration No: 76 ended. Search finished for the next optimal point.\n",
      "Time taken: 3172.3938\n",
      "Function value obtained: 0.4252\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 77 started. Searching for the next optimal point.\n",
      "args [1, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.7547 - acc: 0.8440 - val_loss: 0.5532 - val_acc: 0.6964\n",
      "9639/9639 [==============================] - 29s 3ms/step\n",
      "TN:144,FP:27,FN:72,TP:92,Macc:0.701540390559,F1:0.65017126701\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.3024 - acc: 0.8609 - val_loss: 0.5131 - val_acc: 0.7977\n",
      "9639/9639 [==============================] - 4s 385us/step\n",
      "TN:134,FP:37,FN:36,TP:128,Macc:0.782056716557,F1:0.778109949577\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2853 - acc: 0.8701 - val_loss: 0.4926 - val_acc: 0.7995\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:130,FP:41,FN:35,TP:129,Macc:0.77340959116,F1:0.772449540754\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2758 - acc: 0.8765 - val_loss: 0.4685 - val_acc: 0.8292\n",
      "9639/9639 [==============================] - 4s 379us/step\n",
      "TN:124,FP:47,FN:18,TP:146,Macc:0.807694997456,F1:0.817921656793\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2713 - acc: 0.8788 - val_loss: 0.4464 - val_acc: 0.8301\n",
      "9639/9639 [==============================] - 4s 382us/step\n",
      "TN:135,FP:36,FN:23,TP:141,Macc:0.82461483666,F1:0.826973926817\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2651 - acc: 0.8807 - val_loss: 0.4985 - val_acc: 0.8048\n",
      "9639/9639 [==============================] - 4s 384us/step\n",
      "TN:111,FP:60,FN:8,TP:156,Macc:0.800171106827,F1:0.821047187071\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2627 - acc: 0.8825 - val_loss: 0.4602 - val_acc: 0.8314\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:133,FP:38,FN:22,TP:142,Macc:0.821815664103,F1:0.825575854485\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2614 - acc: 0.8831 - val_loss: 0.4531 - val_acc: 0.8374\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:136,FP:35,FN:21,TP:143,Macc:0.833636373647,F1:0.8362517654\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2585 - acc: 0.8842 - val_loss: 0.4675 - val_acc: 0.8341\n",
      "9639/9639 [==============================] - 4s 379us/step\n",
      "TN:135,FP:36,FN:26,TP:138,Macc:0.81546849581,F1:0.816562499039\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2567 - acc: 0.8842 - val_loss: 0.4675 - val_acc: 0.8243\n",
      "9639/9639 [==============================] - 4s 385us/step\n",
      "TN:144,FP:27,FN:28,TP:136,Macc:0.835686723024,F1:0.831798725471\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2551 - acc: 0.8864 - val_loss: 0.5061 - val_acc: 0.7447\n",
      "9639/9639 [==============================] - 4s 387us/step\n",
      "TN:161,FP:10,FN:68,TP:96,Macc:0.763443110833,F1:0.711105807007\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2535 - acc: 0.8865 - val_loss: 0.4943 - val_acc: 0.7662\n",
      "9639/9639 [==============================] - 4s 379us/step\n",
      "TN:151,FP:20,FN:53,TP:111,Macc:0.779935050882,F1:0.752536885624\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2524 - acc: 0.8875 - val_loss: 0.4935 - val_acc: 0.7722\n",
      "9639/9639 [==============================] - 4s 388us/step\n",
      "TN:144,FP:27,FN:47,TP:117,Macc:0.777759897641,F1:0.759734728704\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2519 - acc: 0.8880 - val_loss: 0.5010 - val_acc: 0.7398\n",
      "9639/9639 [==============================] - 4s 375us/step\n",
      "TN:156,FP:15,FN:65,TP:99,Macc:0.757969569583,F1:0.712224837419\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2512 - acc: 0.8882 - val_loss: 0.4863 - val_acc: 0.7926\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:141,FP:30,FN:38,TP:126,Macc:0.796426990931,F1:0.787494449335\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2500 - acc: 0.8875 - val_loss: 0.5125 - val_acc: 0.7143\n",
      "9639/9639 [==============================] - 4s 386us/step\n",
      "TN:146,FP:25,FN:56,TP:108,Macc:0.756168827932,F1:0.727267233362\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2504 - acc: 0.8887 - val_loss: 0.5116 - val_acc: 0.6966\n",
      "9639/9639 [==============================] - 4s 376us/step\n",
      "TN:155,FP:16,FN:74,TP:90,Macc:0.727606570613,F1:0.666661366187\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2496 - acc: 0.8884 - val_loss: 0.4934 - val_acc: 0.7415\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:150,FP:21,FN:55,TP:109,Macc:0.770913513895,F1:0.74149111675\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2479 - acc: 0.8893 - val_loss: 0.5104 - val_acc: 0.6987\n",
      "9639/9639 [==============================] - 4s 377us/step\n",
      "TN:151,FP:20,FN:67,TP:97,Macc:0.737252126916,F1:0.690386058931\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2453 - acc: 0.8906 - val_loss: 0.5307 - val_acc: 0.6818\n",
      "9639/9639 [==============================] - 4s 383us/step\n",
      "TN:150,FP:21,FN:71,TP:93,Macc:0.722133029362,F1:0.669059373211\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2450 - acc: 0.8910 - val_loss: 0.5344 - val_acc: 0.6810\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:154,FP:17,FN:77,TP:87,Macc:0.715536253343,F1:0.649248453762\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 51s 43ms/step - loss: 0.2449 - acc: 0.8909 - val_loss: 0.5491 - val_acc: 0.6317\n",
      "9639/9639 [==============================] - 4s 370us/step\n",
      "TN:153,FP:18,FN:86,TP:78,Macc:0.685173254373,F1:0.599994825487\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2444 - acc: 0.8912 - val_loss: 0.5341 - val_acc: 0.6684\n",
      "9639/9639 [==============================] - 4s 377us/step\n",
      "TN:153,FP:18,FN:77,TP:87,Macc:0.712612276923,F1:0.646834860423\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2436 - acc: 0.8915 - val_loss: 0.5661 - val_acc: 0.6143\n",
      "9639/9639 [==============================] - 4s 384us/step\n",
      "TN:158,FP:13,FN:102,TP:62,Macc:0.65101265194,F1:0.518823666856\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2435 - acc: 0.8914 - val_loss: 0.5417 - val_acc: 0.6583\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:149,FP:22,FN:77,TP:87,Macc:0.700916371242,F1:0.637357309279\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2419 - acc: 0.8926 - val_loss: 0.5328 - val_acc: 0.6718\n",
      "9639/9639 [==============================] - 4s 379us/step\n",
      "TN:150,FP:21,FN:72,TP:92,Macc:0.719084249079,F1:0.664254561525\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2439 - acc: 0.8907 - val_loss: 0.5752 - val_acc: 0.6138\n",
      "9639/9639 [==============================] - 4s 377us/step\n",
      "TN:156,FP:15,FN:98,TP:66,Macc:0.657359820234,F1:0.538770593097\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2414 - acc: 0.8941 - val_loss: 0.6060 - val_acc: 0.5900\n",
      "9639/9639 [==============================] - 4s 377us/step\n",
      "TN:159,FP:12,FN:111,TP:53,Macc:0.626497605811,F1:0.46287757957\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2413 - acc: 0.8925 - val_loss: 0.5426 - val_acc: 0.6564\n",
      "9639/9639 [==============================] - 4s 392us/step\n",
      "TN:150,FP:21,FN:77,TP:87,Macc:0.703840347663,F1:0.639700563787\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2419 - acc: 0.8918 - val_loss: 0.5583 - val_acc: 0.6358\n",
      "9639/9639 [==============================] - 4s 374us/step\n",
      "TN:154,FP:17,FN:88,TP:76,Macc:0.681999670226,F1:0.591434557902\n",
      "Loss: 0.553153\n",
      "Iteration No: 77 ended. Search finished for the next optimal point.\n",
      "Time taken: 3095.4352\n",
      "Function value obtained: 0.5532\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 78 started. Searching for the next optimal point.\n",
      "args [4, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 68s 56ms/step - loss: 0.7760 - acc: 0.8323 - val_loss: 0.6151 - val_acc: 0.6180\n",
      "9639/9639 [==============================] - 29s 3ms/step\n",
      "TN:145,FP:26,FN:95,TP:69,Macc:0.634342420462,F1:0.532813377965\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.3118 - acc: 0.8478 - val_loss: 0.7497 - val_acc: 0.5787\n",
      "9639/9639 [==============================] - 4s 407us/step\n",
      "TN:150,FP:21,FN:112,TP:52,Macc:0.597133037747,F1:0.438813835582\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2941 - acc: 0.8595 - val_loss: 0.6178 - val_acc: 0.6414\n",
      "9639/9639 [==============================] - 4s 409us/step\n",
      "TN:148,FP:23,FN:90,TP:74,Macc:0.658358251139,F1:0.567044623114\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2864 - acc: 0.8652 - val_loss: 0.5315 - val_acc: 0.7614\n",
      "9639/9639 [==============================] - 4s 401us/step\n",
      "TN:143,FP:28,FN:50,TP:114,Macc:0.765689580371,F1:0.745092514116\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2799 - acc: 0.8691 - val_loss: 0.5480 - val_acc: 0.7260\n",
      "9639/9639 [==============================] - 4s 405us/step\n",
      "TN:147,FP:24,FN:62,TP:102,Macc:0.740800122652,F1:0.703442816974\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2731 - acc: 0.8731 - val_loss: 0.5328 - val_acc: 0.7310\n",
      "9639/9639 [==============================] - 4s 400us/step\n",
      "TN:147,FP:24,FN:61,TP:103,Macc:0.743848902935,F1:0.707898315508\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2719 - acc: 0.8720 - val_loss: 0.5615 - val_acc: 0.7411\n",
      "9639/9639 [==============================] - 4s 402us/step\n",
      "TN:149,FP:22,FN:62,TP:102,Macc:0.746648075492,F1:0.708327885361\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2695 - acc: 0.8740 - val_loss: 0.4684 - val_acc: 0.7961\n",
      "9639/9639 [==============================] - 4s 398us/step\n",
      "TN:140,FP:31,FN:41,TP:123,Macc:0.784356673661,F1:0.77357935762\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2664 - acc: 0.8757 - val_loss: 0.4751 - val_acc: 0.7863\n",
      "9639/9639 [==============================] - 4s 403us/step\n",
      "TN:141,FP:30,FN:41,TP:123,Macc:0.787280650081,F1:0.776019689398\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2628 - acc: 0.8771 - val_loss: 0.4786 - val_acc: 0.8099\n",
      "9639/9639 [==============================] - 4s 410us/step\n",
      "TN:121,FP:50,FN:16,TP:148,Macc:0.805020628762,F1:0.817674056872\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2628 - acc: 0.8778 - val_loss: 0.4908 - val_acc: 0.7783\n",
      "9639/9639 [==============================] - 4s 402us/step\n",
      "TN:148,FP:23,FN:47,TP:117,Macc:0.789455803322,F1:0.769731320719\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2626 - acc: 0.8779 - val_loss: 0.6631 - val_acc: 0.7099\n",
      "9639/9639 [==============================] - 4s 401us/step\n",
      "TN:152,FP:19,FN:73,TP:91,Macc:0.721883421636,F1:0.664228236975\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2615 - acc: 0.8790 - val_loss: 0.4239 - val_acc: 0.8335\n",
      "9639/9639 [==============================] - 4s 407us/step\n",
      "TN:138,FP:33,FN:23,TP:141,Macc:0.83338676592,F1:0.834313977173\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2590 - acc: 0.8803 - val_loss: 0.4371 - val_acc: 0.8233\n",
      "9639/9639 [==============================] - 4s 401us/step\n",
      "TN:134,FP:37,FN:24,TP:140,Macc:0.818642079957,F1:0.821108824557\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2602 - acc: 0.8797 - val_loss: 0.4621 - val_acc: 0.7949\n",
      "9639/9639 [==============================] - 4s 404us/step\n",
      "TN:144,FP:27,FN:46,TP:118,Macc:0.780808677925,F1:0.763748511764\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2584 - acc: 0.8797 - val_loss: 0.4630 - val_acc: 0.8006\n",
      "9639/9639 [==============================] - 4s 400us/step\n",
      "TN:147,FP:24,FN:45,TP:119,Macc:0.792629387468,F1:0.775238769893\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2562 - acc: 0.8813 - val_loss: 0.4494 - val_acc: 0.8038\n",
      "9639/9639 [==============================] - 4s 404us/step\n",
      "TN:142,FP:29,FN:33,TP:131,Macc:0.814594868768,F1:0.808636421277\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2545 - acc: 0.8818 - val_loss: 0.4541 - val_acc: 0.7929\n",
      "9639/9639 [==============================] - 4s 399us/step\n",
      "TN:141,FP:30,FN:42,TP:122,Macc:0.784231869798,F1:0.772146352947\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2536 - acc: 0.8821 - val_loss: 0.4158 - val_acc: 0.8310\n",
      "9639/9639 [==============================] - 4s 400us/step\n",
      "TN:132,FP:39,FN:19,TP:145,Macc:0.828038028533,F1:0.833327798854\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2534 - acc: 0.8820 - val_loss: 0.4411 - val_acc: 0.8071\n",
      "9639/9639 [==============================] - 4s 403us/step\n",
      "TN:145,FP:26,FN:41,TP:123,Macc:0.798976555761,F1:0.785936949441\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2528 - acc: 0.8826 - val_loss: 0.4279 - val_acc: 0.8244\n",
      "9639/9639 [==============================] - 4s 405us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:142,FP:29,FN:28,TP:136,Macc:0.829838770184,F1:0.826742165168\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2502 - acc: 0.8837 - val_loss: 0.4127 - val_acc: 0.8365\n",
      "9639/9639 [==============================] - 4s 389us/step\n",
      "TN:140,FP:31,FN:27,TP:137,Macc:0.827039597627,F1:0.825295650966\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2513 - acc: 0.8835 - val_loss: 0.4492 - val_acc: 0.8042\n",
      "9639/9639 [==============================] - 4s 401us/step\n",
      "TN:140,FP:31,FN:37,TP:127,Macc:0.796551794794,F1:0.78881432383\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2500 - acc: 0.8837 - val_loss: 0.4874 - val_acc: 0.7806\n",
      "9639/9639 [==============================] - 4s 394us/step\n",
      "TN:146,FP:25,FN:47,TP:117,Macc:0.783607850482,F1:0.764700355842\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2500 - acc: 0.8851 - val_loss: 0.4159 - val_acc: 0.8343\n",
      "9639/9639 [==============================] - 4s 399us/step\n",
      "TN:138,FP:33,FN:22,TP:142,Macc:0.836435546204,F1:0.837752563554\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2495 - acc: 0.8847 - val_loss: 0.4467 - val_acc: 0.8148\n",
      "9639/9639 [==============================] - 4s 399us/step\n",
      "TN:133,FP:38,FN:26,TP:138,Macc:0.80962054297,F1:0.811759160245\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2478 - acc: 0.8858 - val_loss: 0.4502 - val_acc: 0.8064\n",
      "9639/9639 [==============================] - 4s 399us/step\n",
      "TN:144,FP:27,FN:37,TP:127,Macc:0.808247700475,F1:0.798736588583\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2460 - acc: 0.8873 - val_loss: 0.4022 - val_acc: 0.8465\n",
      "9639/9639 [==============================] - 4s 396us/step\n",
      "TN:139,FP:32,FN:16,TP:148,Macc:0.857652204324,F1:0.860459573183\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2467 - acc: 0.8860 - val_loss: 0.4293 - val_acc: 0.8125\n",
      "9639/9639 [==============================] - 4s 399us/step\n",
      "TN:140,FP:31,FN:27,TP:137,Macc:0.827039597627,F1:0.825295650966\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2452 - acc: 0.8878 - val_loss: 0.4073 - val_acc: 0.8320\n",
      "9639/9639 [==============================] - 4s 395us/step\n",
      "TN:138,FP:33,FN:23,TP:141,Macc:0.83338676592,F1:0.834313977173\n",
      "Loss: 0.401257\n",
      "Iteration No: 78 ended. Search finished for the next optimal point.\n",
      "Time taken: 3181.3129\n",
      "Function value obtained: 0.4013\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 79 started. Searching for the next optimal point.\n",
      "args [4, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 77s 64ms/step - loss: 0.7844 - acc: 0.8324 - val_loss: 0.6333 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 30s 3ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.3140 - acc: 0.8321 - val_loss: 0.6213 - val_acc: 0.6912\n",
      "9639/9639 [==============================] - 4s 414us/step\n",
      "TN:61,FP:110,FN:0,TP:164,Macc:0.678362528089,F1:0.748853256807\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.3028 - acc: 0.8370 - val_loss: 0.5801 - val_acc: 0.7579\n",
      "9639/9639 [==============================] - 4s 405us/step\n",
      "TN:86,FP:85,FN:1,TP:163,Macc:0.748413158308,F1:0.791256822332\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2959 - acc: 0.8424 - val_loss: 0.5780 - val_acc: 0.7428\n",
      "9639/9639 [==============================] - 4s 403us/step\n",
      "TN:83,FP:88,FN:0,TP:164,Macc:0.742690009331,F1:0.788456242916\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2909 - acc: 0.8497 - val_loss: 0.5577 - val_acc: 0.7635\n",
      "9639/9639 [==============================] - 4s 400us/step\n",
      "TN:95,FP:76,FN:7,TP:157,Macc:0.756436264389,F1:0.790926612273\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2868 - acc: 0.8511 - val_loss: 0.5491 - val_acc: 0.7719\n",
      "9639/9639 [==============================] - 4s 404us/step\n",
      "TN:97,FP:74,FN:8,TP:156,Macc:0.759235436946,F1:0.791872782741\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2858 - acc: 0.8554 - val_loss: 0.5457 - val_acc: 0.7682\n",
      "9639/9639 [==============================] - 4s 407us/step\n",
      "TN:97,FP:74,FN:6,TP:158,Macc:0.765332997513,F1:0.797974415861\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 53s 45ms/step - loss: 0.2828 - acc: 0.8556 - val_loss: 0.5498 - val_acc: 0.7550\n",
      "9639/9639 [==============================] - 4s 408us/step\n",
      "TN:87,FP:84,FN:2,TP:162,Macc:0.748288354445,F1:0.790238580071\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2807 - acc: 0.8594 - val_loss: 0.5314 - val_acc: 0.7747\n",
      "9639/9639 [==============================] - 4s 405us/step\n",
      "TN:96,FP:75,FN:4,TP:160,Macc:0.768506581659,F1:0.8019996425\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2798 - acc: 0.8608 - val_loss: 0.5307 - val_acc: 0.7808\n",
      "9639/9639 [==============================] - 4s 408us/step\n",
      "TN:108,FP:63,FN:20,TP:144,Macc:0.754813814167,F1:0.776274851339\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2788 - acc: 0.8669 - val_loss: 0.5474 - val_acc: 0.7528\n",
      "9639/9639 [==============================] - 4s 401us/step\n",
      "TN:96,FP:75,FN:20,TP:144,Macc:0.719726097126,F1:0.751952794808\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2726 - acc: 0.8750 - val_loss: 0.5519 - val_acc: 0.7269\n",
      "9639/9639 [==============================] - 4s 403us/step\n",
      "TN:75,FP:96,FN:0,TP:164,Macc:0.719298197971,F1:0.773579647508\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2713 - acc: 0.8743 - val_loss: 0.5274 - val_acc: 0.7648\n",
      "9639/9639 [==============================] - 4s 406us/step\n",
      "TN:97,FP:74,FN:12,TP:152,Macc:0.747040315813,F1:0.779481774554\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2705 - acc: 0.8763 - val_loss: 0.5365 - val_acc: 0.7487\n",
      "9639/9639 [==============================] - 4s 409us/step\n",
      "TN:89,FP:82,FN:10,TP:154,Macc:0.729746065019,F1:0.769994635887\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2704 - acc: 0.8758 - val_loss: 0.5219 - val_acc: 0.7630\n",
      "9639/9639 [==============================] - 4s 401us/step\n",
      "TN:95,FP:76,FN:11,TP:153,Macc:0.744241143256,F1:0.778620561102\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2692 - acc: 0.8768 - val_loss: 0.5151 - val_acc: 0.7712\n",
      "9639/9639 [==============================] - 4s 405us/step\n",
      "TN:101,FP:70,FN:10,TP:154,Macc:0.76483378206,F1:0.793809019539\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2693 - acc: 0.8764 - val_loss: 0.5176 - val_acc: 0.7787\n",
      "9639/9639 [==============================] - 4s 400us/step\n",
      "TN:111,FP:60,FN:22,TP:142,Macc:0.757488182861,F1:0.775950796837\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2667 - acc: 0.8780 - val_loss: 0.5212 - val_acc: 0.7725\n",
      "9639/9639 [==============================] - 4s 415us/step\n",
      "TN:102,FP:69,FN:13,TP:151,Macc:0.75861141763,F1:0.786452905283\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2649 - acc: 0.8786 - val_loss: 0.5579 - val_acc: 0.7817\n",
      "9639/9639 [==============================] - 4s 402us/step\n",
      "TN:120,FP:51,FN:25,TP:139,Macc:0.774657629792,F1:0.785305215366\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2680 - acc: 0.8779 - val_loss: 0.5198 - val_acc: 0.7965\n",
      "9639/9639 [==============================] - 4s 402us/step\n",
      "TN:124,FP:47,FN:24,TP:140,Macc:0.789402315756,F1:0.797715271375\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2667 - acc: 0.8787 - val_loss: 0.5301 - val_acc: 0.7612\n",
      "9639/9639 [==============================] - 4s 402us/step\n",
      "TN:101,FP:70,FN:14,TP:150,Macc:0.752638660927,F1:0.781244572248\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2638 - acc: 0.8799 - val_loss: 0.5175 - val_acc: 0.7805\n",
      "9639/9639 [==============================] - 4s 411us/step\n",
      "TN:114,FP:57,FN:22,TP:142,Macc:0.766260112121,F1:0.782363649758\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2638 - acc: 0.8799 - val_loss: 0.5123 - val_acc: 0.7747\n",
      "9639/9639 [==============================] - 4s 400us/step\n",
      "TN:107,FP:64,FN:16,TP:148,Macc:0.764084958881,F1:0.787228586162\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2645 - acc: 0.8800 - val_loss: 0.5171 - val_acc: 0.7642\n",
      "9639/9639 [==============================] - 4s 399us/step\n",
      "TN:98,FP:73,FN:14,TP:150,Macc:0.743866731666,F1:0.775188382252\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2629 - acc: 0.8791 - val_loss: 0.5321 - val_acc: 0.7861\n",
      "9639/9639 [==============================] - 4s 401us/step\n",
      "TN:122,FP:49,FN:30,TP:134,Macc:0.765261681216,F1:0.77232876151\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2623 - acc: 0.8788 - val_loss: 0.5320 - val_acc: 0.7761\n",
      "9639/9639 [==============================] - 4s 405us/step\n",
      "TN:109,FP:62,FN:21,TP:143,Macc:0.754689010304,F1:0.775062272407\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2623 - acc: 0.8801 - val_loss: 0.5150 - val_acc: 0.7744\n",
      "9639/9639 [==============================] - 4s 400us/step\n",
      "TN:102,FP:69,FN:11,TP:153,Macc:0.764708978197,F1:0.792740693021\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2602 - acc: 0.8819 - val_loss: 0.5353 - val_acc: 0.7868\n",
      "9639/9639 [==============================] - 4s 403us/step\n",
      "TN:119,FP:52,FN:26,TP:138,Macc:0.768684873089,F1:0.779655498203\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2631 - acc: 0.8800 - val_loss: 0.5364 - val_acc: 0.7821\n",
      "9639/9639 [==============================] - 4s 402us/step\n",
      "TN:122,FP:49,FN:28,TP:136,Macc:0.771359241782,F1:0.77936409833\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2624 - acc: 0.8814 - val_loss: 0.5372 - val_acc: 0.7849\n",
      "9639/9639 [==============================] - 4s 407us/step\n",
      "TN:122,FP:49,FN:27,TP:137,Macc:0.774408022066,F1:0.782851615418\n",
      "Loss: 0.531190\n",
      "Iteration No: 79 ended. Search finished for the next optimal point.\n",
      "Time taken: 3219.4173\n",
      "Function value obtained: 0.5312\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 80 started. Searching for the next optimal point.\n",
      "args [2, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 78s 65ms/step - loss: 0.7722 - acc: 0.8323 - val_loss: 0.5839 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 31s 3ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.3164 - acc: 0.8326 - val_loss: 0.6385 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 394us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2974 - acc: 0.8540 - val_loss: 0.6789 - val_acc: 0.5902\n",
      "9639/9639 [==============================] - 4s 402us/step\n",
      "TN:19,FP:152,FN:5,TP:159,Macc:0.540311617029,F1:0.669468679997\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2872 - acc: 0.8608 - val_loss: 0.5368 - val_acc: 0.7658\n",
      "9639/9639 [==============================] - 4s 395us/step\n",
      "TN:91,FP:80,FN:3,TP:161,Macc:0.756935479842,F1:0.79505638405\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2831 - acc: 0.8646 - val_loss: 0.5429 - val_acc: 0.7552\n",
      "9639/9639 [==============================] - 4s 397us/step\n",
      "TN:86,FP:85,FN:6,TP:158,Macc:0.733169256892,F1:0.776407441699\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2778 - acc: 0.8680 - val_loss: 0.4861 - val_acc: 0.8091\n",
      "9639/9639 [==============================] - 4s 394us/step\n",
      "TN:110,FP:61,FN:8,TP:156,Macc:0.797247130407,F1:0.818892196975\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2763 - acc: 0.8708 - val_loss: 0.4759 - val_acc: 0.8089\n",
      "9639/9639 [==============================] - 4s 403us/step\n",
      "TN:108,FP:63,FN:1,TP:163,Macc:0.81274063955,F1:0.835892027779\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2728 - acc: 0.8728 - val_loss: 0.5814 - val_acc: 0.7063\n",
      "9639/9639 [==============================] - 4s 395us/step\n",
      "TN:97,FP:74,FN:42,TP:122,Macc:0.655576907313,F1:0.677772279859\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2712 - acc: 0.8738 - val_loss: 0.6115 - val_acc: 0.6680\n",
      "9639/9639 [==============================] - 4s 397us/step\n",
      "TN:65,FP:106,FN:19,TP:145,Macc:0.632131608387,F1:0.698789885434\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2683 - acc: 0.8764 - val_loss: 0.5027 - val_acc: 0.7795\n",
      "9639/9639 [==============================] - 4s 391us/step\n",
      "TN:108,FP:63,FN:15,TP:149,Macc:0.770057715584,F1:0.792547734787\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2652 - acc: 0.8786 - val_loss: 0.4513 - val_acc: 0.8234\n",
      "9639/9639 [==============================] - 4s 404us/step\n",
      "TN:118,FP:53,FN:8,TP:156,Macc:0.820638941768,F1:0.836455656757\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2656 - acc: 0.8773 - val_loss: 0.4319 - val_acc: 0.8329\n",
      "9639/9639 [==============================] - 4s 394us/step\n",
      "TN:127,FP:44,FN:16,TP:148,Macc:0.822564487282,F1:0.831455156834\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2665 - acc: 0.8778 - val_loss: 0.4419 - val_acc: 0.8264\n",
      "9639/9639 [==============================] - 4s 396us/step\n",
      "TN:113,FP:58,FN:5,TP:159,Macc:0.815165400518,F1:0.834640227562\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2627 - acc: 0.8790 - val_loss: 0.4343 - val_acc: 0.8356\n",
      "9639/9639 [==============================] - ETA:  - 4s 393us/step\n",
      "TN:122,FP:49,FN:11,TP:153,Macc:0.823188506599,F1:0.836060082839\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2620 - acc: 0.8800 - val_loss: 0.5259 - val_acc: 0.7578\n",
      "9639/9639 [==============================] - 4s 405us/step\n",
      "TN:86,FP:85,FN:4,TP:160,Macc:0.739266817458,F1:0.782390761688\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2602 - acc: 0.8805 - val_loss: 0.4796 - val_acc: 0.7937\n",
      "9639/9639 [==============================] - 4s 394us/step\n",
      "TN:109,FP:62,FN:11,TP:153,Macc:0.785176813137,F1:0.807382415558\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2609 - acc: 0.8806 - val_loss: 0.4594 - val_acc: 0.8042\n",
      "9639/9639 [==============================] - 4s 396us/step\n",
      "TN:104,FP:67,FN:3,TP:161,Macc:0.794947173303,F1:0.821423171968\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2606 - acc: 0.8807 - val_loss: 0.4238 - val_acc: 0.8362\n",
      "9639/9639 [==============================] - 4s 395us/step\n",
      "TN:128,FP:43,FN:14,TP:150,Macc:0.831586024269,F1:0.840330618997\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2602 - acc: 0.8793 - val_loss: 0.4328 - val_acc: 0.8305\n",
      "9639/9639 [==============================] - 4s 404us/step\n",
      "TN:123,FP:48,FN:14,TP:150,Macc:0.816966142169,F1:0.828723779957\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2560 - acc: 0.8826 - val_loss: 0.4256 - val_acc: 0.8346\n",
      "9639/9639 [==============================] - 4s 395us/step\n",
      "TN:127,FP:44,FN:15,TP:149,Macc:0.825613267566,F1:0.834728378446\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2580 - acc: 0.8826 - val_loss: 0.4913 - val_acc: 0.7864\n",
      "9639/9639 [==============================] - 4s 397us/step\n",
      "TN:120,FP:51,FN:19,TP:145,Macc:0.792950311492,F1:0.805550049821\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2551 - acc: 0.8839 - val_loss: 0.4387 - val_acc: 0.8215\n",
      "9639/9639 [==============================] - 4s 396us/step\n",
      "TN:137,FP:34,FN:28,TP:136,Macc:0.815218888084,F1:0.814365705656\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 52s 44ms/step - loss: 0.2563 - acc: 0.8830 - val_loss: 0.4257 - val_acc: 0.8353\n",
      "9639/9639 [==============================] - 4s 395us/step\n",
      "TN:132,FP:39,FN:20,TP:144,Macc:0.82498924825,F1:0.829965645461\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2562 - acc: 0.8836 - val_loss: 0.5004 - val_acc: 0.7849\n",
      "9639/9639 [==============================] - 4s 395us/step\n",
      "TN:120,FP:51,FN:27,TP:137,Macc:0.768560069225,F1:0.778403567865\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2559 - acc: 0.8827 - val_loss: 0.4083 - val_acc: 0.8465\n",
      "9639/9639 [==============================] - 4s 393us/step\n",
      "TN:131,FP:40,FN:14,TP:150,Macc:0.840357953529,F1:0.847452104156\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2547 - acc: 0.8845 - val_loss: 0.4363 - val_acc: 0.8211\n",
      "9639/9639 [==============================] - 4s 403us/step\n",
      "TN:114,FP:57,FN:9,TP:155,Macc:0.805894255804,F1:0.824462626535\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2535 - acc: 0.8829 - val_loss: 0.4574 - val_acc: 0.8223\n",
      "9639/9639 [==============================] - 4s 396us/step\n",
      "TN:142,FP:29,FN:30,TP:134,Macc:0.823741209618,F1:0.819566310392\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2535 - acc: 0.8845 - val_loss: 0.4385 - val_acc: 0.8298\n",
      "9639/9639 [==============================] - 4s 396us/step\n",
      "TN:146,FP:25,FN:29,TP:135,Macc:0.838485895581,F1:0.833327777624\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2525 - acc: 0.8842 - val_loss: 0.4073 - val_acc: 0.8441\n",
      "9639/9639 [==============================] - 4s 397us/step\n",
      "TN:129,FP:42,FN:12,TP:152,Macc:0.840607561256,F1:0.849156497648\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 52s 43ms/step - loss: 0.2521 - acc: 0.8840 - val_loss: 0.4042 - val_acc: 0.8511\n",
      "9639/9639 [==============================] - 4s 407us/step\n",
      "TN:138,FP:33,FN:17,TP:147,Macc:0.85167944762,F1:0.854645620066\n",
      "Loss: 0.399401\n",
      "Iteration No: 80 ended. Search finished for the next optimal point.\n",
      "Time taken: 3194.9034\n",
      "Function value obtained: 0.3994\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 81 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 72s 60ms/step - loss: 0.7581 - acc: 0.8393 - val_loss: 0.6458 - val_acc: 0.6792\n",
      "9639/9639 [==============================] - 31s 3ms/step\n",
      "TN:57,FP:114,FN:1,TP:163,Macc:0.663617842126,F1:0.739223849214\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.3062 - acc: 0.8574 - val_loss: 0.5455 - val_acc: 0.7716\n",
      "9639/9639 [==============================] - 4s 417us/step\n",
      "TN:89,FP:82,FN:1,TP:163,Macc:0.757185087569,F1:0.797060687549\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2888 - acc: 0.8664 - val_loss: 0.5837 - val_acc: 0.7010\n",
      "9639/9639 [==============================] - 4s 419us/step\n",
      "TN:67,FP:104,FN:3,TP:161,Macc:0.68676004576,F1:0.750577516981\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 53s 45ms/step - loss: 0.2797 - acc: 0.8736 - val_loss: 0.6161 - val_acc: 0.6572\n",
      "9639/9639 [==============================] - 4s 416us/step\n",
      "TN:51,FP:120,FN:1,TP:163,Macc:0.646073983605,F1:0.729301341637\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2740 - acc: 0.8774 - val_loss: 0.5847 - val_acc: 0.6978\n",
      "9639/9639 [==============================] - 4s 423us/step\n",
      "TN:69,FP:102,FN:0,TP:164,Macc:0.70175433945,F1:0.762785468159\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2658 - acc: 0.8803 - val_loss: 0.5213 - val_acc: 0.7503\n",
      "9639/9639 [==============================] - 4s 420us/step\n",
      "TN:83,FP:88,FN:0,TP:164,Macc:0.742690009331,F1:0.788456242916\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2622 - acc: 0.8826 - val_loss: 0.4931 - val_acc: 0.7745\n",
      "9639/9639 [==============================] - 4s 418us/step\n",
      "TN:93,FP:78,FN:0,TP:164,Macc:0.771929773532,F1:0.807876432659\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2615 - acc: 0.8825 - val_loss: 0.5040 - val_acc: 0.7610\n",
      "9639/9639 [==============================] - 4s 419us/step\n",
      "TN:86,FP:85,FN:0,TP:164,Macc:0.751461938592,F1:0.794183552685\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2564 - acc: 0.8856 - val_loss: 0.5540 - val_acc: 0.7215\n",
      "9639/9639 [==============================] - 4s 418us/step\n",
      "TN:72,FP:99,FN:1,TP:163,Macc:0.707477488427,F1:0.765252967546\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2539 - acc: 0.8871 - val_loss: 0.5169 - val_acc: 0.7487\n",
      "9639/9639 [==============================] - 4s 423us/step\n",
      "TN:82,FP:89,FN:1,TP:163,Macc:0.736717252628,F1:0.783648550863\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2535 - acc: 0.8870 - val_loss: 0.4417 - val_acc: 0.8069\n",
      "9639/9639 [==============================] - 4s 415us/step\n",
      "TN:106,FP:65,FN:3,TP:161,Macc:0.800795126144,F1:0.825635618102\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2505 - acc: 0.8897 - val_loss: 0.4448 - val_acc: 0.8091\n",
      "9639/9639 [==============================] - 4s 417us/step\n",
      "TN:109,FP:62,FN:3,TP:161,Macc:0.809567055404,F1:0.832035924238\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2495 - acc: 0.8890 - val_loss: 0.4998 - val_acc: 0.7676\n",
      "9639/9639 [==============================] - 4s 416us/step\n",
      "TN:87,FP:84,FN:1,TP:163,Macc:0.751337134728,F1:0.793182029812\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2495 - acc: 0.8888 - val_loss: 0.4612 - val_acc: 0.7899\n",
      "9639/9639 [==============================] - 4s 419us/step\n",
      "TN:95,FP:76,FN:1,TP:163,Macc:0.774728946089,F1:0.808927648847\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2491 - acc: 0.8876 - val_loss: 0.4592 - val_acc: 0.7920\n",
      "9639/9639 [==============================] - 4s 419us/step\n",
      "TN:97,FP:74,FN:1,TP:163,Macc:0.780576898929,F1:0.812962218752\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2474 - acc: 0.8900 - val_loss: 0.4460 - val_acc: 0.8030\n",
      "9639/9639 [==============================] - 4s 418us/step\n",
      "TN:103,FP:68,FN:0,TP:164,Macc:0.801169537733,F1:0.828277444479\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2466 - acc: 0.8901 - val_loss: 0.4079 - val_acc: 0.8299\n",
      "9639/9639 [==============================] - 4s 414us/step\n",
      "TN:115,FP:56,FN:3,TP:161,Macc:0.827110913924,F1:0.845138914619\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2460 - acc: 0.8891 - val_loss: 0.4218 - val_acc: 0.8184\n",
      "9639/9639 [==============================] - 4s 418us/step\n",
      "TN:108,FP:63,FN:3,TP:161,Macc:0.806643078984,F1:0.829891491718\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2455 - acc: 0.8895 - val_loss: 0.4490 - val_acc: 0.7974\n",
      "9639/9639 [==============================] - 4s 424us/step\n",
      "TN:102,FP:69,FN:2,TP:162,Macc:0.792148000746,F1:0.820247777147\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2435 - acc: 0.8900 - val_loss: 0.4145 - val_acc: 0.8227\n",
      "9639/9639 [==============================] - 4s 415us/step\n",
      "TN:109,FP:62,FN:0,TP:164,Macc:0.818713396254,F1:0.841020232618\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2456 - acc: 0.8901 - val_loss: 0.4275 - val_acc: 0.8121\n",
      "9639/9639 [==============================] - 4s 415us/step\n",
      "TN:105,FP:66,FN:0,TP:164,Macc:0.807017490573,F1:0.832481917528\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2427 - acc: 0.8892 - val_loss: 0.4058 - val_acc: 0.8276\n",
      "9639/9639 [==============================] - 4s 415us/step\n",
      "TN:115,FP:56,FN:4,TP:160,Macc:0.824062133641,F1:0.842099817431\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2421 - acc: 0.8909 - val_loss: 0.4125 - val_acc: 0.8226\n",
      "9639/9639 [==============================] - 4s 415us/step\n",
      "TN:110,FP:61,FN:3,TP:161,Macc:0.812491031824,F1:0.834191467859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2409 - acc: 0.8921 - val_loss: 0.4032 - val_acc: 0.8265\n",
      "9639/9639 [==============================] - 4s 418us/step\n",
      "TN:110,FP:61,FN:3,TP:161,Macc:0.812491031824,F1:0.834191467859\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2411 - acc: 0.8918 - val_loss: 0.4450 - val_acc: 0.7968\n",
      "9639/9639 [==============================] - 4s 422us/step\n",
      "TN:101,FP:70,FN:2,TP:162,Macc:0.789224024326,F1:0.818176434939\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2416 - acc: 0.8918 - val_loss: 0.3938 - val_acc: 0.8315\n",
      "9639/9639 [==============================] - 4s 413us/step\n",
      "TN:112,FP:59,FN:1,TP:163,Macc:0.824436545231,F1:0.844554161569\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2416 - acc: 0.8916 - val_loss: 0.3847 - val_acc: 0.8401\n",
      "9639/9639 [==============================] - 4s 415us/step\n",
      "TN:119,FP:52,FN:4,TP:160,Macc:0.835758039321,F1:0.851058369659\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2403 - acc: 0.8920 - val_loss: 0.4083 - val_acc: 0.8228\n",
      "9639/9639 [==============================] - 4s 415us/step\n",
      "TN:111,FP:60,FN:5,TP:159,Macc:0.809317447677,F1:0.830281772029\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2399 - acc: 0.8934 - val_loss: 0.4085 - val_acc: 0.8236\n",
      "9639/9639 [==============================] - 4s 416us/step\n",
      "TN:110,FP:61,FN:1,TP:163,Macc:0.818588592391,F1:0.840200769484\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2412 - acc: 0.8921 - val_loss: 0.3851 - val_acc: 0.8385\n",
      "9639/9639 [==============================] - 4s 425us/step\n",
      "TN:117,FP:54,FN:4,TP:160,Macc:0.829910086481,F1:0.846555393557\n",
      "Loss: 0.379269\n",
      "Iteration No: 81 ended. Search finished for the next optimal point.\n",
      "Time taken: 3281.6704\n",
      "Function value obtained: 0.3793\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 82 started. Searching for the next optimal point.\n",
      "args [2, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 92s 77ms/step - loss: 0.7544 - acc: 0.8432 - val_loss: 0.5810 - val_acc: 0.7880\n",
      "9639/9639 [==============================] - 32s 3ms/step\n",
      "TN:109,FP:62,FN:15,TP:149,Macc:0.772981692004,F1:0.79466120648\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.3048 - acc: 0.8583 - val_loss: 0.5514 - val_acc: 0.7727\n",
      "9639/9639 [==============================] - 4s 419us/step\n",
      "TN:100,FP:71,FN:12,TP:152,Macc:0.755812245073,F1:0.785524298977\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2875 - acc: 0.8675 - val_loss: 0.5599 - val_acc: 0.7400\n",
      "9639/9639 [==============================] - 4s 414us/step\n",
      "TN:88,FP:83,FN:11,TP:153,Macc:0.723773308315,F1:0.764994636162\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2789 - acc: 0.8749 - val_loss: 0.7153 - val_acc: 0.5529\n",
      "9639/9639 [==============================] - 4s 413us/step\n",
      "TN:3,FP:168,FN:0,TP:164,Macc:0.508771895724,F1:0.661285424268\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2733 - acc: 0.8792 - val_loss: 0.6402 - val_acc: 0.6441\n",
      "9639/9639 [==============================] - 4s 415us/step\n",
      "TN:41,FP:130,FN:4,TP:160,Macc:0.607687878554,F1:0.704840704494\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2677 - acc: 0.8812 - val_loss: 0.6583 - val_acc: 0.6270\n",
      "9639/9639 [==============================] - 4s 409us/step\n",
      "TN:35,FP:136,FN:5,TP:159,Macc:0.58709523975,F1:0.692805372348\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2639 - acc: 0.8838 - val_loss: 0.5702 - val_acc: 0.7132\n",
      "9639/9639 [==============================] - 4s 413us/step\n",
      "TN:76,FP:95,FN:4,TP:160,Macc:0.710027053257,F1:0.763717869722\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2604 - acc: 0.8867 - val_loss: 0.6546 - val_acc: 0.6354\n",
      "9639/9639 [==============================] - 4s 414us/step\n",
      "TN:37,FP:134,FN:2,TP:162,Macc:0.60208953344,F1:0.704342745329\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2544 - acc: 0.8882 - val_loss: 0.6521 - val_acc: 0.6362\n",
      "9639/9639 [==============================] - 4s 411us/step\n",
      "TN:42,FP:129,FN:5,TP:159,Macc:0.607563074691,F1:0.703534702735\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2537 - acc: 0.8892 - val_loss: 0.6981 - val_acc: 0.5994\n",
      "9639/9639 [==============================] - 4s 412us/step\n",
      "TN:25,FP:146,FN:0,TP:164,Macc:0.573099376966,F1:0.69197811209\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2512 - acc: 0.8898 - val_loss: 0.6241 - val_acc: 0.6586\n",
      "9639/9639 [==============================] - 4s 411us/step\n",
      "TN:49,FP:122,FN:2,TP:162,Macc:0.637177250481,F1:0.723209144846\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 53s 45ms/step - loss: 0.2501 - acc: 0.8899 - val_loss: 0.6613 - val_acc: 0.6256\n",
      "9639/9639 [==============================] - 4s 412us/step\n",
      "TN:33,FP:138,FN:1,TP:163,Macc:0.593442408043,F1:0.701070213101\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2466 - acc: 0.8909 - val_loss: 0.6245 - val_acc: 0.6571\n",
      "9639/9639 [==============================] - 4s 413us/step\n",
      "TN:43,FP:128,FN:2,TP:162,Macc:0.619633391961,F1:0.713651276754\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2450 - acc: 0.8920 - val_loss: 0.6086 - val_acc: 0.6653\n",
      "9639/9639 [==============================] - 4s 408us/step\n",
      "TN:49,FP:122,FN:2,TP:162,Macc:0.637177250481,F1:0.723209144846\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2457 - acc: 0.8922 - val_loss: 0.6684 - val_acc: 0.6227\n",
      "9639/9639 [==============================] - 4s 413us/step\n",
      "TN:31,FP:140,FN:0,TP:164,Macc:0.590643235486,F1:0.700849660128\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2441 - acc: 0.8926 - val_loss: 0.6129 - val_acc: 0.6580\n",
      "9639/9639 [==============================] - 4s 409us/step\n",
      "TN:42,FP:129,FN:1,TP:163,Macc:0.619758195824,F1:0.71490717961\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2429 - acc: 0.8923 - val_loss: 0.5858 - val_acc: 0.6851\n",
      "9639/9639 [==============================] - 4s 416us/step\n",
      "TN:55,FP:116,FN:4,TP:160,Macc:0.648623548435,F1:0.727267547309\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2410 - acc: 0.8940 - val_loss: 0.7383 - val_acc: 0.5691\n",
      "9639/9639 [==============================] - 4s 410us/step\n",
      "TN:14,FP:157,FN:0,TP:164,Macc:0.540935636345,F1:0.676283705492\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2412 - acc: 0.8939 - val_loss: 0.6173 - val_acc: 0.6614\n",
      "9639/9639 [==============================] - 4s 410us/step\n",
      "TN:47,FP:124,FN:2,TP:162,Macc:0.631329297641,F1:0.719994869093\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2393 - acc: 0.8931 - val_loss: 0.5636 - val_acc: 0.7058\n",
      "9639/9639 [==============================] - 4s 413us/step\n",
      "TN:64,FP:107,FN:2,TP:162,Macc:0.681036896783,F1:0.74826268382\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2393 - acc: 0.8944 - val_loss: 0.4940 - val_acc: 0.7507\n",
      "9639/9639 [==============================] - 4s 412us/step\n",
      "TN:87,FP:84,FN:8,TP:156,Macc:0.729995672745,F1:0.772271880342\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2377 - acc: 0.8946 - val_loss: 0.5224 - val_acc: 0.7318\n",
      "9639/9639 [==============================] - 4s 419us/step\n",
      "TN:73,FP:98,FN:2,TP:162,Macc:0.707352684564,F1:0.764145685733\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2381 - acc: 0.8941 - val_loss: 0.5975 - val_acc: 0.6729\n",
      "9639/9639 [==============================] - 4s 413us/step\n",
      "TN:50,FP:121,FN:0,TP:164,Macc:0.646198787468,F1:0.730507113115\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2362 - acc: 0.8956 - val_loss: 0.5692 - val_acc: 0.6888\n",
      "9639/9639 [==============================] - 4s 408us/step\n",
      "TN:56,FP:115,FN:1,TP:163,Macc:0.660693865705,F1:0.737551390282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2362 - acc: 0.8960 - val_loss: 0.5786 - val_acc: 0.6794\n",
      "9639/9639 [==============================] - 4s 412us/step\n",
      "TN:53,FP:118,FN:1,TP:163,Macc:0.651921936445,F1:0.732579113683\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2351 - acc: 0.8950 - val_loss: 0.5713 - val_acc: 0.6887\n",
      "9639/9639 [==============================] - 4s 409us/step\n",
      "TN:54,FP:117,FN:1,TP:163,Macc:0.654845912865,F1:0.734229073305\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2358 - acc: 0.8957 - val_loss: 0.5239 - val_acc: 0.7258\n",
      "9639/9639 [==============================] - 4s 407us/step\n",
      "TN:73,FP:98,FN:3,TP:161,Macc:0.70430390428,F1:0.76122405228\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2352 - acc: 0.8958 - val_loss: 0.5105 - val_acc: 0.7294\n",
      "9639/9639 [==============================] - 4s 414us/step\n",
      "TN:70,FP:101,FN:1,TP:163,Macc:0.701629535587,F1:0.761677004119\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2347 - acc: 0.8966 - val_loss: 0.6422 - val_acc: 0.6437\n",
      "9639/9639 [==============================] - 4s 414us/step\n",
      "TN:39,FP:132,FN:0,TP:164,Macc:0.614035046847,F1:0.713038397087\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2370 - acc: 0.8964 - val_loss: 0.5363 - val_acc: 0.7063\n",
      "9639/9639 [==============================] - 4s 411us/step\n",
      "TN:58,FP:113,FN:1,TP:163,Macc:0.666541818546,F1:0.740903910263\n",
      "Loss: 0.530587\n",
      "Iteration No: 82 ended. Search finished for the next optimal point.\n",
      "Time taken: 3351.8424\n",
      "Function value obtained: 0.5306\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 83 started. Searching for the next optimal point.\n",
      "args [1, 3, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 127s 106ms/step - loss: 0.7759 - acc: 0.8497 - val_loss: 0.5875 - val_acc: 0.8085\n",
      "9639/9639 [==============================] - 36s 4ms/step\n",
      "TN:124,FP:47,FN:19,TP:145,Macc:0.804646217172,F1:0.814601225292\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.3203 - acc: 0.8621 - val_loss: 0.5390 - val_acc: 0.7787\n",
      "9639/9639 [==============================] - 4s 446us/step\n",
      "TN:116,FP:55,FN:25,TP:139,Macc:0.762961724112,F1:0.77653080379\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2995 - acc: 0.8701 - val_loss: 0.5263 - val_acc: 0.7672\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:91,FP:80,FN:9,TP:155,Macc:0.738642798142,F1:0.776936987241\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2823 - acc: 0.8780 - val_loss: 0.5336 - val_acc: 0.7512\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:101,FP:70,FN:20,TP:144,Macc:0.734345979227,F1:0.761899313831\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2763 - acc: 0.8804 - val_loss: 0.4473 - val_acc: 0.8321\n",
      "9639/9639 [==============================] - 4s 435us/step\n",
      "TN:111,FP:60,FN:1,TP:163,Macc:0.821512568811,F1:0.842371840963\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2652 - acc: 0.8848 - val_loss: 0.3971 - val_acc: 0.8576\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:129,FP:42,FN:9,TP:155,Macc:0.849753902106,F1:0.858720255435\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2612 - acc: 0.8880 - val_loss: 0.4272 - val_acc: 0.8551\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:132,FP:39,FN:14,TP:150,Macc:0.84328192995,F1:0.849852831596\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2562 - acc: 0.8908 - val_loss: 0.5296 - val_acc: 0.8067\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:139,FP:32,FN:28,TP:136,Macc:0.821066840924,F1:0.81927155498\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2509 - acc: 0.8929 - val_loss: 0.6404 - val_acc: 0.7342\n",
      "9639/9639 [==============================] - 4s 442us/step\n",
      "TN:148,FP:23,FN:59,TP:105,Macc:0.752870439922,F1:0.719172611647\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2477 - acc: 0.8943 - val_loss: 0.5767 - val_acc: 0.7891\n",
      "9639/9639 [==============================] - 4s 428us/step\n",
      "TN:157,FP:14,FN:51,TP:113,Macc:0.803576469969,F1:0.776626832645\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2445 - acc: 0.8959 - val_loss: 1.1735 - val_acc: 0.6322\n",
      "9639/9639 [==============================] - 4s 426us/step\n",
      "TN:151,FP:20,FN:90,TP:74,Macc:0.6671301804,F1:0.573638266856\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2419 - acc: 0.8972 - val_loss: 1.7878 - val_acc: 0.5572\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:159,FP:12,FN:122,TP:42,Macc:0.592961022694,F1:0.38531696242\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2394 - acc: 0.8983 - val_loss: 2.7396 - val_acc: 0.5122\n",
      "9639/9639 [==============================] - 4s 434us/step\n",
      "TN:160,FP:11,FN:139,TP:25,Macc:0.544055734298,F1:0.249996725342\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2359 - acc: 0.9002 - val_loss: 1.9478 - val_acc: 0.5597\n",
      "9639/9639 [==============================] - 4s 436us/step\n",
      "TN:154,FP:17,FN:117,TP:47,Macc:0.59358504201,F1:0.412276220038\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2354 - acc: 0.8989 - val_loss: 3.5973 - val_acc: 0.4966\n",
      "9639/9639 [==============================] - 4s 434us/step\n",
      "TN:161,FP:10,FN:143,TP:21,Macc:0.534784589585,F1:0.215381649691\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2325 - acc: 0.9004 - val_loss: 2.1421 - val_acc: 0.5669\n",
      "9639/9639 [==============================] - 4s 434us/step\n",
      "TN:154,FP:17,FN:117,TP:47,Macc:0.59358504201,F1:0.412276220038\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2324 - acc: 0.9009 - val_loss: 1.0403 - val_acc: 0.6291\n",
      "9639/9639 [==============================] - 4s 427us/step\n",
      "TN:150,FP:21,FN:88,TP:76,Macc:0.670303764546,F1:0.582370292319\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2299 - acc: 0.9030 - val_loss: 2.4717 - val_acc: 0.5289\n",
      "9639/9639 [==============================] - 4s 441us/step\n",
      "TN:168,FP:3,FN:135,TP:29,Macc:0.579642666792,F1:0.295915328747\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2290 - acc: 0.9023 - val_loss: 1.1414 - val_acc: 0.6574\n",
      "9639/9639 [==============================] - 4s 427us/step\n",
      "TN:148,FP:23,FN:81,TP:83,Macc:0.685797273689,F1:0.614809518563\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2267 - acc: 0.9037 - val_loss: 1.2603 - val_acc: 0.6212\n",
      "9639/9639 [==============================] - 4s 437us/step\n",
      "TN:151,FP:20,FN:95,TP:69,Macc:0.651886278983,F1:0.5454494814\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2249 - acc: 0.9045 - val_loss: 1.7589 - val_acc: 0.5780\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:154,FP:17,FN:110,TP:54,Macc:0.614926503994,F1:0.459569786484\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2228 - acc: 0.9048 - val_loss: 1.5755 - val_acc: 0.5897\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:157,FP:14,FN:112,TP:52,Macc:0.617600872687,F1:0.452169368362\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2238 - acc: 0.9045 - val_loss: 1.6425 - val_acc: 0.5902\n",
      "9639/9639 [==============================] - 4s 434us/step\n",
      "TN:156,FP:15,FN:111,TP:53,Macc:0.61772567655,F1:0.456891950179\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2227 - acc: 0.9039 - val_loss: 1.6805 - val_acc: 0.5753\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:154,FP:17,FN:110,TP:54,Macc:0.614926503994,F1:0.459569786484\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 53s 45ms/step - loss: 0.2213 - acc: 0.9058 - val_loss: 1.1090 - val_acc: 0.6228\n",
      "9639/9639 [==============================] - 4s 428us/step\n",
      "TN:158,FP:13,FN:102,TP:62,Macc:0.65101265194,F1:0.518823666856\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2210 - acc: 0.9064 - val_loss: 1.6957 - val_acc: 0.5451\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:161,FP:10,FN:124,TP:40,Macc:0.592711414968,F1:0.373827798102\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2185 - acc: 0.9064 - val_loss: 0.6310 - val_acc: 0.7349\n",
      "9639/9639 [==============================] - 4s 436us/step\n",
      "TN:153,FP:18,FN:67,TP:97,Macc:0.743100079756,F1:0.69533511665\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2173 - acc: 0.9084 - val_loss: 0.6309 - val_acc: 0.7220\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:144,FP:27,FN:64,TP:100,Macc:0.725930632825,F1:0.687279760367\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2176 - acc: 0.9075 - val_loss: 1.4305 - val_acc: 0.5874\n",
      "9639/9639 [==============================] - 4s 432us/step\n",
      "TN:161,FP:10,FN:113,TP:51,Macc:0.626247998084,F1:0.453328941632\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2158 - acc: 0.9080 - val_loss: 0.6390 - val_acc: 0.7276\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:150,FP:21,FN:61,TP:103,Macc:0.752620832196,F1:0.715272329275\n",
      "Loss: 0.624048\n",
      "Iteration No: 83 ended. Search finished for the next optimal point.\n",
      "Time taken: 3787.7379\n",
      "Function value obtained: 0.6240\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 84 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 80s 66ms/step - loss: 0.7543 - acc: 0.8482 - val_loss: 0.5512 - val_acc: 0.7708\n",
      "9639/9639 [==============================] - 34s 4ms/step\n",
      "TN:107,FP:64,FN:21,TP:143,Macc:0.748841057464,F1:0.77088401608\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.3089 - acc: 0.8602 - val_loss: 0.8060 - val_acc: 0.5926\n",
      "9639/9639 [==============================] - 4s 436us/step\n",
      "TN:155,FP:16,FN:111,TP:53,Macc:0.61480170013,F1:0.454930993728\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2907 - acc: 0.8697 - val_loss: 0.5860 - val_acc: 0.6836\n",
      "9639/9639 [==============================] - 4s 425us/step\n",
      "TN:145,FP:26,FN:75,TP:89,Macc:0.695318026129,F1:0.637987450925\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2797 - acc: 0.8746 - val_loss: 0.4814 - val_acc: 0.7789\n",
      "9639/9639 [==============================] - 4s 446us/step\n",
      "TN:138,FP:33,FN:40,TP:124,Macc:0.781557501104,F1:0.772580119487\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2763 - acc: 0.8752 - val_loss: 0.4510 - val_acc: 0.8038\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:135,FP:36,FN:32,TP:132,Macc:0.79717581411,F1:0.795175171035\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2687 - acc: 0.8778 - val_loss: 0.5970 - val_acc: 0.7082\n",
      "9639/9639 [==============================] - 4s 435us/step\n",
      "TN:155,FP:16,FN:70,TP:94,Macc:0.739801691746,F1:0.686126045435\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2664 - acc: 0.8795 - val_loss: 0.4981 - val_acc: 0.7740\n",
      "9639/9639 [==============================] - 4s 437us/step\n",
      "TN:138,FP:33,FN:46,TP:118,Macc:0.763264819404,F1:0.749200806289\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2640 - acc: 0.8805 - val_loss: 0.4957 - val_acc: 0.7689\n",
      "9639/9639 [==============================] - 4s 423us/step\n",
      "TN:149,FP:22,FN:49,TP:115,Macc:0.786282219175,F1:0.764114089773\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2618 - acc: 0.8813 - val_loss: 0.4361 - val_acc: 0.8193\n",
      "9639/9639 [==============================] - 4s 436us/step\n",
      "TN:130,FP:41,FN:27,TP:137,Macc:0.797799833426,F1:0.80116404836\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2607 - acc: 0.8822 - val_loss: 0.4227 - val_acc: 0.8293\n",
      "9639/9639 [==============================] - 4s 435us/step\n",
      "TN:138,FP:33,FN:28,TP:136,Macc:0.818142864504,F1:0.81681126413\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2600 - acc: 0.8825 - val_loss: 0.5500 - val_acc: 0.7376\n",
      "9639/9639 [==============================] - 4s 434us/step\n",
      "TN:153,FP:18,FN:65,TP:99,Macc:0.749197640323,F1:0.704620933261\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2553 - acc: 0.8852 - val_loss: 0.4537 - val_acc: 0.8040\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:140,FP:31,FN:35,TP:129,Macc:0.802649355361,F1:0.796290743103\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2559 - acc: 0.8846 - val_loss: 0.4569 - val_acc: 0.7925\n",
      "9639/9639 [==============================] - 4s 432us/step\n",
      "TN:139,FP:32,FN:39,TP:125,Macc:0.787530257807,F1:0.778810648654\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2542 - acc: 0.8840 - val_loss: 0.4318 - val_acc: 0.8120\n",
      "9639/9639 [==============================] - 4s 421us/step\n",
      "TN:136,FP:35,FN:31,TP:133,Macc:0.803148570814,F1:0.801199267021\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2551 - acc: 0.8842 - val_loss: 0.4406 - val_acc: 0.8152\n",
      "9639/9639 [==============================] - 4s 432us/step\n",
      "TN:138,FP:33,FN:29,TP:135,Macc:0.81509408422,F1:0.813247458993\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2509 - acc: 0.8856 - val_loss: 0.4216 - val_acc: 0.8282\n",
      "9639/9639 [==============================] - 4s 434us/step\n",
      "TN:128,FP:43,FN:22,TP:142,Macc:0.807195782003,F1:0.813748050316\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2509 - acc: 0.8861 - val_loss: 0.4254 - val_acc: 0.8195\n",
      "9639/9639 [==============================] - 4s 422us/step\n",
      "TN:122,FP:49,FN:22,TP:142,Macc:0.789651923482,F1:0.799994482275\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2506 - acc: 0.8863 - val_loss: 0.4205 - val_acc: 0.8332\n",
      "9639/9639 [==============================] - 4s 435us/step\n",
      "TN:131,FP:40,FN:25,TP:139,Macc:0.806821370413,F1:0.810490085393\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2500 - acc: 0.8873 - val_loss: 0.4181 - val_acc: 0.8291\n",
      "9639/9639 [==============================] - 4s 427us/step\n",
      "TN:140,FP:31,FN:31,TP:133,Macc:0.814844476494,F1:0.810970055399\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2476 - acc: 0.8876 - val_loss: 0.4207 - val_acc: 0.8292\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:128,FP:43,FN:24,TP:140,Macc:0.801098221436,F1:0.806910891881\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2456 - acc: 0.8879 - val_loss: 0.4071 - val_acc: 0.8287\n",
      "9639/9639 [==============================] - 4s 432us/step\n",
      "TN:133,FP:38,FN:26,TP:138,Macc:0.80962054297,F1:0.811759160245\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 53s 45ms/step - loss: 0.2464 - acc: 0.8894 - val_loss: 0.4112 - val_acc: 0.8243\n",
      "9639/9639 [==============================] - 4s 420us/step\n",
      "TN:142,FP:29,FN:32,TP:132,Macc:0.817643649051,F1:0.812302137827\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2462 - acc: 0.8877 - val_loss: 0.4375 - val_acc: 0.8143\n",
      "9639/9639 [==============================] - 4s 432us/step\n",
      "TN:147,FP:24,FN:40,TP:124,Macc:0.807873288885,F1:0.794866253325\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2468 - acc: 0.8883 - val_loss: 0.4029 - val_acc: 0.8328\n",
      "9639/9639 [==============================] - 4s 421us/step\n",
      "TN:133,FP:38,FN:25,TP:139,Macc:0.812669323253,F1:0.815243722296\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2441 - acc: 0.8894 - val_loss: 0.4470 - val_acc: 0.7955\n",
      "9639/9639 [==============================] - 4s 435us/step\n",
      "TN:143,FP:28,FN:40,TP:124,Macc:0.796177383204,F1:0.784804579913\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2459 - acc: 0.8876 - val_loss: 0.4335 - val_acc: 0.8152\n",
      "9639/9639 [==============================] - 4s 419us/step\n",
      "TN:138,FP:33,FN:34,TP:130,Macc:0.799850182804,F1:0.795101480235\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2430 - acc: 0.8896 - val_loss: 0.4015 - val_acc: 0.8292\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:133,FP:38,FN:24,TP:140,Macc:0.815718103537,F1:0.81870790688\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2424 - acc: 0.8905 - val_loss: 0.4081 - val_acc: 0.8306\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:131,FP:40,FN:25,TP:139,Macc:0.806821370413,F1:0.810490085393\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2439 - acc: 0.8892 - val_loss: 0.4532 - val_acc: 0.7985\n",
      "9639/9639 [==============================] - 4s 418us/step\n",
      "TN:148,FP:23,FN:43,TP:121,Macc:0.801650924455,F1:0.785708752821\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2439 - acc: 0.8889 - val_loss: 0.4176 - val_acc: 0.8228\n",
      "9639/9639 [==============================] - 4s 432us/step\n",
      "TN:146,FP:25,FN:35,TP:129,Macc:0.820193213881,F1:0.811315204064\n",
      "Loss: 0.411939\n",
      "Iteration No: 84 ended. Search finished for the next optimal point.\n",
      "Time taken: 3432.1278\n",
      "Function value obtained: 0.4119\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 85 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 124s 103ms/step - loss: 0.7563 - acc: 0.8453 - val_loss: 0.6178 - val_acc: 0.7211\n",
      "9639/9639 [==============================] - 35s 4ms/step\n",
      "TN:99,FP:72,FN:32,TP:132,Macc:0.691912662987,F1:0.717385826483\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 53s 45ms/step - loss: 0.3017 - acc: 0.8627 - val_loss: 0.5844 - val_acc: 0.7057\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:69,FP:102,FN:3,TP:161,Macc:0.6926079986,F1:0.754093117488\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2855 - acc: 0.8697 - val_loss: 0.5321 - val_acc: 0.7565\n",
      "9639/9639 [==============================] - 4s 433us/step\n",
      "TN:89,FP:82,FN:3,TP:161,Macc:0.751087527002,F1:0.791149455643\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2775 - acc: 0.8745 - val_loss: 0.5276 - val_acc: 0.7540\n",
      "9639/9639 [==============================] - 4s 437us/step\n",
      "TN:83,FP:88,FN:2,TP:162,Macc:0.736592448765,F1:0.782603391433\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2718 - acc: 0.8758 - val_loss: 0.4822 - val_acc: 0.7879\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:99,FP:72,FN:5,TP:159,Macc:0.774229730636,F1:0.805057904576\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2666 - acc: 0.8794 - val_loss: 0.4789 - val_acc: 0.7893\n",
      "9639/9639 [==============================] - 4s 426us/step\n",
      "TN:99,FP:72,FN:4,TP:160,Macc:0.77727851092,F1:0.8080754254\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2655 - acc: 0.8802 - val_loss: 0.5007 - val_acc: 0.7723\n",
      "9639/9639 [==============================] - 4s 436us/step\n",
      "TN:91,FP:80,FN:0,TP:164,Macc:0.766081820692,F1:0.803916236771\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2628 - acc: 0.8817 - val_loss: 0.4813 - val_acc: 0.7875\n",
      "9639/9639 [==============================] - 4s 428us/step\n",
      "TN:98,FP:73,FN:2,TP:162,Macc:0.780452095066,F1:0.812024704603\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2614 - acc: 0.8809 - val_loss: 0.4534 - val_acc: 0.8057\n",
      "9639/9639 [==============================] - 4s 432us/step\n",
      "TN:105,FP:66,FN:5,TP:159,Macc:0.791773589157,F1:0.817475308842\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2602 - acc: 0.8816 - val_loss: 0.4595 - val_acc: 0.8014\n",
      "9639/9639 [==============================] - 4s 438us/step\n",
      "TN:105,FP:66,FN:3,TP:161,Macc:0.797871149723,F1:0.82352400825\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2588 - acc: 0.8826 - val_loss: 0.4424 - val_acc: 0.8104\n",
      "9639/9639 [==============================] - 4s 435us/step\n",
      "TN:104,FP:67,FN:2,TP:162,Macc:0.797995953587,F1:0.824422085254\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2574 - acc: 0.8824 - val_loss: 0.4344 - val_acc: 0.8144\n",
      "9639/9639 [==============================] - 4s 425us/step\n",
      "TN:111,FP:60,FN:8,TP:156,Macc:0.800171106827,F1:0.821047187071\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2570 - acc: 0.8836 - val_loss: 0.4121 - val_acc: 0.8285\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:113,FP:58,FN:7,TP:157,Macc:0.809067839951,F1:0.828490593751\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2551 - acc: 0.8845 - val_loss: 0.4111 - val_acc: 0.8255\n",
      "9639/9639 [==============================] - 4s 425us/step\n",
      "TN:122,FP:49,FN:13,TP:151,Macc:0.817090946032,F1:0.829664833359\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2553 - acc: 0.8839 - val_loss: 0.4271 - val_acc: 0.8174\n",
      "9639/9639 [==============================] - 4s 428us/step\n",
      "TN:109,FP:62,FN:6,TP:158,Macc:0.800420714554,F1:0.822911236526\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2529 - acc: 0.8856 - val_loss: 0.4043 - val_acc: 0.8293\n",
      "9639/9639 [==============================] - 4s 433us/step\n",
      "TN:118,FP:53,FN:11,TP:153,Macc:0.811492600918,F1:0.827021548757\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2534 - acc: 0.8841 - val_loss: 0.4262 - val_acc: 0.8207\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:110,FP:61,FN:2,TP:162,Macc:0.815539812107,F1:0.8372038826\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2523 - acc: 0.8863 - val_loss: 0.3991 - val_acc: 0.8336\n",
      "9639/9639 [==============================] - 4s 427us/step\n",
      "TN:120,FP:51,FN:10,TP:154,Macc:0.820389334042,F1:0.834682865055\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2506 - acc: 0.8860 - val_loss: 0.4103 - val_acc: 0.8252\n",
      "9639/9639 [==============================] - 4s 425us/step\n",
      "TN:112,FP:59,FN:7,TP:157,Macc:0.806143863531,F1:0.826310344661\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 53s 45ms/step - loss: 0.2506 - acc: 0.8863 - val_loss: 0.4153 - val_acc: 0.8196\n",
      "9639/9639 [==============================] - 4s 423us/step\n",
      "TN:117,FP:54,FN:10,TP:154,Macc:0.811617404781,F1:0.827951517263\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2509 - acc: 0.8852 - val_loss: 0.4086 - val_acc: 0.8262\n",
      "9639/9639 [==============================] - 4s 432us/step\n",
      "TN:113,FP:58,FN:9,TP:155,Macc:0.802970279384,F1:0.822275712072\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2489 - acc: 0.8874 - val_loss: 0.4077 - val_acc: 0.8253\n",
      "9639/9639 [==============================] - 4s 444us/step\n",
      "TN:114,FP:57,FN:6,TP:158,Macc:0.815040596654,F1:0.8337676383\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2482 - acc: 0.8864 - val_loss: 0.4046 - val_acc: 0.8292\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:114,FP:57,FN:6,TP:158,Macc:0.815040596654,F1:0.8337676383\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2472 - acc: 0.8885 - val_loss: 0.4087 - val_acc: 0.8226\n",
      "9639/9639 [==============================] - 4s 423us/step\n",
      "TN:118,FP:53,FN:14,TP:150,Macc:0.802346260068,F1:0.817433205242\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2469 - acc: 0.8886 - val_loss: 0.4298 - val_acc: 0.8191\n",
      "9639/9639 [==============================] - 4s 428us/step\n",
      "TN:128,FP:43,FN:18,TP:146,Macc:0.819390903136,F1:0.827189943491\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2480 - acc: 0.8878 - val_loss: 0.4126 - val_acc: 0.8227\n",
      "9639/9639 [==============================] - 4s 425us/step\n",
      "TN:123,FP:48,FN:17,TP:147,Macc:0.807819801319,F1:0.81893599504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2473 - acc: 0.8878 - val_loss: 0.4027 - val_acc: 0.8303\n",
      "9639/9639 [==============================] - 4s 427us/step\n",
      "TN:125,FP:46,FN:15,TP:149,Macc:0.819765314726,F1:0.830078055638\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2472 - acc: 0.8876 - val_loss: 0.4194 - val_acc: 0.8151\n",
      "9639/9639 [==============================] - 4s 433us/step\n",
      "TN:121,FP:50,FN:16,TP:148,Macc:0.805020628762,F1:0.817674056872\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2455 - acc: 0.8882 - val_loss: 0.3972 - val_acc: 0.8312\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:116,FP:55,FN:9,TP:155,Macc:0.811742208645,F1:0.828871539828\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2449 - acc: 0.8887 - val_loss: 0.4030 - val_acc: 0.8299\n",
      "9639/9639 [==============================] - 4s 425us/step\n",
      "TN:124,FP:47,FN:12,TP:152,Macc:0.825987679155,F1:0.837460065149\n",
      "Loss: 0.397107\n",
      "Iteration No: 85 ended. Search finished for the next optimal point.\n",
      "Time taken: 3627.2505\n",
      "Function value obtained: 0.3971\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 86 started. Searching for the next optimal point.\n",
      "args [4, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 90s 75ms/step - loss: 0.7901 - acc: 0.8324 - val_loss: 0.6503 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 35s 4ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.3223 - acc: 0.8337 - val_loss: 0.5486 - val_acc: 0.8068\n",
      "9639/9639 [==============================] - 4s 441us/step\n",
      "TN:118,FP:53,FN:14,TP:150,Macc:0.802346260068,F1:0.817433205242\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2986 - acc: 0.8514 - val_loss: 0.5580 - val_acc: 0.7705\n",
      "9639/9639 [==============================] - 4s 441us/step\n",
      "TN:97,FP:74,FN:7,TP:157,Macc:0.762284217229,F1:0.794931322862\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2914 - acc: 0.8548 - val_loss: 0.5395 - val_acc: 0.7938\n",
      "9639/9639 [==============================] - 4s 450us/step\n",
      "TN:110,FP:61,FN:13,TP:151,Macc:0.782003228991,F1:0.803186032036\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2870 - acc: 0.8597 - val_loss: 0.5811 - val_acc: 0.7232\n",
      "9639/9639 [==============================] - 4s 438us/step\n",
      "TN:73,FP:98,FN:5,TP:159,Macc:0.698206343714,F1:0.755339147006\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2801 - acc: 0.8657 - val_loss: 0.5396 - val_acc: 0.7601\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:96,FP:75,FN:10,TP:154,Macc:0.750213899959,F1:0.783709619341\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2770 - acc: 0.8729 - val_loss: 0.6009 - val_acc: 0.6855\n",
      "9639/9639 [==============================] - 4s 444us/step\n",
      "TN:62,FP:109,FN:7,TP:157,Macc:0.659945042526,F1:0.730227330291\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2732 - acc: 0.8757 - val_loss: 0.5919 - val_acc: 0.6892\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:63,FP:108,FN:4,TP:160,Macc:0.672015359796,F1:0.740735521813\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2705 - acc: 0.8755 - val_loss: 0.6289 - val_acc: 0.6487\n",
      "9639/9639 [==============================] - 4s 437us/step\n",
      "TN:49,FP:122,FN:3,TP:161,Macc:0.634128470198,F1:0.720352796217\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2675 - acc: 0.8779 - val_loss: 0.5828 - val_acc: 0.6986\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:72,FP:99,FN:8,TP:156,Macc:0.686136026444,F1:0.744624791967\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2677 - acc: 0.8785 - val_loss: 0.5789 - val_acc: 0.7005\n",
      "9639/9639 [==============================] - 4s 434us/step\n",
      "TN:67,FP:104,FN:6,TP:158,Macc:0.67761370491,F1:0.741778790356\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2638 - acc: 0.8823 - val_loss: 0.5490 - val_acc: 0.7310\n",
      "9639/9639 [==============================] - 4s 442us/step\n",
      "TN:82,FP:89,FN:7,TP:157,Macc:0.718424570928,F1:0.765848337478\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2638 - acc: 0.8810 - val_loss: 0.6092 - val_acc: 0.6672\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:54,FP:117,FN:2,TP:162,Macc:0.651797132582,F1:0.731371809523\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2609 - acc: 0.8832 - val_loss: 0.5933 - val_acc: 0.6861\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:62,FP:109,FN:6,TP:158,Macc:0.662993822809,F1:0.733173431015\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2614 - acc: 0.8823 - val_loss: 0.4976 - val_acc: 0.7751\n",
      "9639/9639 [==============================] - 4s 443us/step\n",
      "TN:96,FP:75,FN:5,TP:159,Macc:0.765457801376,F1:0.798989600879\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2592 - acc: 0.8839 - val_loss: 0.5314 - val_acc: 0.7431\n",
      "9639/9639 [==============================] - 4s 444us/step\n",
      "TN:82,FP:89,FN:3,TP:161,Macc:0.730619692061,F1:0.777772473816\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2585 - acc: 0.8838 - val_loss: 0.5302 - val_acc: 0.7440\n",
      "9639/9639 [==============================] - 4s 442us/step\n",
      "TN:83,FP:88,FN:6,TP:158,Macc:0.724397327631,F1:0.770726385997\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2580 - acc: 0.8850 - val_loss: 0.5035 - val_acc: 0.7759\n",
      "9639/9639 [==============================] - 4s 435us/step\n",
      "TN:100,FP:71,FN:8,TP:156,Macc:0.768007366206,F1:0.79794856212\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2556 - acc: 0.8853 - val_loss: 0.5071 - val_acc: 0.7721\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:100,FP:71,FN:9,TP:155,Macc:0.764958585923,F1:0.79486638907\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2552 - acc: 0.8857 - val_loss: 0.5464 - val_acc: 0.7339\n",
      "9639/9639 [==============================] - 4s 435us/step\n",
      "TN:84,FP:87,FN:7,TP:157,Macc:0.724272523768,F1:0.769602513132\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2533 - acc: 0.8863 - val_loss: 0.5196 - val_acc: 0.7589\n",
      "9639/9639 [==============================] - 4s 438us/step\n",
      "TN:94,FP:77,FN:7,TP:157,Macc:0.753512287969,F1:0.788939350179\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2528 - acc: 0.8859 - val_loss: 0.5447 - val_acc: 0.7224\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:77,FP:94,FN:6,TP:158,Macc:0.706853469111,F1:0.759610090597\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2517 - acc: 0.8870 - val_loss: 0.4973 - val_acc: 0.7797\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:99,FP:72,FN:9,TP:155,Macc:0.762034609503,F1:0.792833472894\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2504 - acc: 0.8876 - val_loss: 0.4904 - val_acc: 0.7753\n",
      "9639/9639 [==============================] - 4s 437us/step\n",
      "TN:97,FP:74,FN:6,TP:158,Macc:0.765332997513,F1:0.797974415861\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2495 - acc: 0.8868 - val_loss: 0.4835 - val_acc: 0.7860\n",
      "9639/9639 [==============================] - 4s 441us/step\n",
      "TN:101,FP:70,FN:10,TP:154,Macc:0.76483378206,F1:0.793809019539\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2501 - acc: 0.8883 - val_loss: 0.5121 - val_acc: 0.7635\n",
      "9639/9639 [==============================] - 4s 440us/step\n",
      "TN:99,FP:72,FN:12,TP:152,Macc:0.752888268653,F1:0.783499741773\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2476 - acc: 0.8889 - val_loss: 0.4857 - val_acc: 0.7891\n",
      "9639/9639 [==============================] - 4s 437us/step\n",
      "TN:108,FP:63,FN:13,TP:151,Macc:0.776155276151,F1:0.798936348711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2470 - acc: 0.8888 - val_loss: 0.4925 - val_acc: 0.7783\n",
      "9639/9639 [==============================] - 4s 442us/step\n",
      "TN:103,FP:68,FN:12,TP:152,Macc:0.764584174333,F1:0.791661238318\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2467 - acc: 0.8902 - val_loss: 0.4987 - val_acc: 0.7699\n",
      "9639/9639 [==============================] - 4s 444us/step\n",
      "TN:97,FP:74,FN:8,TP:156,Macc:0.759235436946,F1:0.791872782741\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2474 - acc: 0.8894 - val_loss: 0.4950 - val_acc: 0.7767\n",
      "9639/9639 [==============================] - 4s 436us/step\n",
      "TN:101,FP:70,FN:9,TP:155,Macc:0.767882562343,F1:0.796909757307\n",
      "Loss: 0.488442\n",
      "Iteration No: 86 ended. Search finished for the next optimal point.\n",
      "Time taken: 3527.3689\n",
      "Function value obtained: 0.4884\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 87 started. Searching for the next optimal point.\n",
      "args [2, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 80s 67ms/step - loss: 0.7719 - acc: 0.8324 - val_loss: 0.6283 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 35s 4ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.3221 - acc: 0.8325 - val_loss: 0.6249 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.3084 - acc: 0.8327 - val_loss: 0.6497 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 427us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.3032 - acc: 0.8325 - val_loss: 0.6361 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 431us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.3006 - acc: 0.8324 - val_loss: 0.6282 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2946 - acc: 0.8325 - val_loss: 0.6418 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2931 - acc: 0.8313 - val_loss: 0.6127 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2909 - acc: 0.8337 - val_loss: 0.5865 - val_acc: 0.7416\n",
      "9639/9639 [==============================] - 4s 436us/step\n",
      "TN:80,FP:91,FN:0,TP:164,Macc:0.733918080071,F1:0.782810947478\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2886 - acc: 0.8393 - val_loss: 0.6046 - val_acc: 0.7141\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:71,FP:100,FN:0,TP:164,Macc:0.70760229229,F1:0.766349901075\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2878 - acc: 0.8355 - val_loss: 0.5703 - val_acc: 0.7683\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:94,FP:77,FN:0,TP:164,Macc:0.774853749952,F1:0.809871198059\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2878 - acc: 0.8382 - val_loss: 0.5709 - val_acc: 0.7611\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:90,FP:81,FN:0,TP:164,Macc:0.763157844272,F1:0.801950662836\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2879 - acc: 0.8384 - val_loss: 0.5901 - val_acc: 0.7338\n",
      "9639/9639 [==============================] - 4s 428us/step\n",
      "TN:78,FP:93,FN:0,TP:164,Macc:0.728070127231,F1:0.779092114884\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2867 - acc: 0.8369 - val_loss: 0.6427 - val_acc: 0.6529\n",
      "9639/9639 [==============================] - 4s 427us/step\n",
      "TN:48,FP:123,FN:0,TP:164,Macc:0.640350834628,F1:0.727267600923\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2848 - acc: 0.8406 - val_loss: 0.6075 - val_acc: 0.7077\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:70,FP:101,FN:0,TP:164,Macc:0.70467831587,F1:0.764563530249\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2862 - acc: 0.8390 - val_loss: 0.6071 - val_acc: 0.7043\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:63,FP:108,FN:0,TP:164,Macc:0.684210480929,F1:0.752288377529\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2840 - acc: 0.8397 - val_loss: 0.5936 - val_acc: 0.7282\n",
      "9639/9639 [==============================] - 4s 428us/step\n",
      "TN:76,FP:95,FN:0,TP:164,Macc:0.722222174391,F1:0.775408448705\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 53s 44ms/step - loss: 0.2813 - acc: 0.8422 - val_loss: 0.5724 - val_acc: 0.7537\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:90,FP:81,FN:0,TP:164,Macc:0.763157844272,F1:0.801950662836\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2823 - acc: 0.8419 - val_loss: 0.5635 - val_acc: 0.7684\n",
      "9639/9639 [==============================] - 4s 439us/step\n",
      "TN:89,FP:82,FN:0,TP:164,Macc:0.760233867852,F1:0.799994677108\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2818 - acc: 0.8432 - val_loss: 0.5934 - val_acc: 0.7260\n",
      "9639/9639 [==============================] - 4s 433us/step\n",
      "TN:70,FP:101,FN:0,TP:164,Macc:0.70467831587,F1:0.764563530249\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2803 - acc: 0.8424 - val_loss: 0.5417 - val_acc: 0.7935\n",
      "9639/9639 [==============================] - 4s 431us/step\n",
      "TN:102,FP:69,FN:0,TP:164,Macc:0.798245561313,F1:0.826191093944\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2801 - acc: 0.8430 - val_loss: 0.5617 - val_acc: 0.7694\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:90,FP:81,FN:0,TP:164,Macc:0.763157844272,F1:0.801950662836\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2799 - acc: 0.8416 - val_loss: 0.5898 - val_acc: 0.7262\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:69,FP:102,FN:0,TP:164,Macc:0.70175433945,F1:0.762785468159\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2801 - acc: 0.8418 - val_loss: 0.5782 - val_acc: 0.7463\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:83,FP:88,FN:0,TP:164,Macc:0.742690009331,F1:0.788456242916\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2789 - acc: 0.8414 - val_loss: 0.5874 - val_acc: 0.7288\n",
      "9639/9639 [==============================] - 4s 428us/step\n",
      "TN:73,FP:98,FN:0,TP:164,Macc:0.71345024513,F1:0.769947802984\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2798 - acc: 0.8407 - val_loss: 0.5948 - val_acc: 0.7192\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:70,FP:101,FN:0,TP:164,Macc:0.70467831587,F1:0.764563530249\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2784 - acc: 0.8432 - val_loss: 0.5324 - val_acc: 0.8035\n",
      "9639/9639 [==============================] - 4s 431us/step\n",
      "TN:105,FP:66,FN:0,TP:164,Macc:0.807017490573,F1:0.832481917528\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2768 - acc: 0.8452 - val_loss: 0.5454 - val_acc: 0.7855\n",
      "9639/9639 [==============================] - 4s 429us/step\n",
      "TN:95,FP:76,FN:0,TP:164,Macc:0.777777726372,F1:0.81187583858\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 53s 45ms/step - loss: 0.2782 - acc: 0.8438 - val_loss: 0.5919 - val_acc: 0.7159\n",
      "9639/9639 [==============================] - 4s 430us/step\n",
      "TN:66,FP:105,FN:0,TP:164,Macc:0.69298241019,F1:0.757500558639\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2766 - acc: 0.8452 - val_loss: 0.5822 - val_acc: 0.7302\n",
      "9639/9639 [==============================] - 4s 422us/step\n",
      "TN:73,FP:98,FN:0,TP:164,Macc:0.71345024513,F1:0.769947802984\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2786 - acc: 0.8441 - val_loss: 0.5244 - val_acc: 0.8100\n",
      "9639/9639 [==============================] - 4s 438us/step\n",
      "TN:106,FP:65,FN:2,TP:162,Macc:0.803843906427,F1:0.828639097476\n",
      "Loss: 0.519463\n",
      "Iteration No: 87 ended. Search finished for the next optimal point.\n",
      "Time taken: 3462.5189\n",
      "Function value obtained: 0.5195\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 88 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 317s 264ms/step - loss: 0.7542 - acc: 0.8483 - val_loss: 0.6347 - val_acc: 0.7029\n",
      "9639/9639 [==============================] - 35s 4ms/step\n",
      "TN:87,FP:84,FN:21,TP:143,Macc:0.690361529062,F1:0.731452402182\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.3031 - acc: 0.8656 - val_loss: 0.6299 - val_acc: 0.6530\n",
      "9639/9639 [==============================] - 4s 454us/step\n",
      "TN:56,FP:115,FN:7,TP:157,Macc:0.642401184006,F1:0.720178287408\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2870 - acc: 0.8732 - val_loss: 0.6471 - val_acc: 0.6281\n",
      "9639/9639 [==============================] - 4s 451us/step\n",
      "TN:36,FP:135,FN:5,TP:159,Macc:0.59001921617,F1:0.694318053907\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2795 - acc: 0.8756 - val_loss: 0.6744 - val_acc: 0.6046\n",
      "9639/9639 [==============================] - 4s 455us/step\n",
      "TN:23,FP:148,FN:1,TP:163,Macc:0.564202643842,F1:0.686310784479\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2746 - acc: 0.8793 - val_loss: 0.6565 - val_acc: 0.6266\n",
      "9639/9639 [==============================] - 4s 453us/step\n",
      "TN:35,FP:136,FN:1,TP:163,Macc:0.599290360883,F1:0.704098605879\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2689 - acc: 0.8802 - val_loss: 0.6820 - val_acc: 0.6037\n",
      "9639/9639 [==============================] - 4s 450us/step\n",
      "TN:30,FP:141,FN:4,TP:160,Macc:0.575524137933,F1:0.688166987906\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2648 - acc: 0.8820 - val_loss: 0.6502 - val_acc: 0.6355\n",
      "9639/9639 [==============================] - 4s 454us/step\n",
      "TN:38,FP:133,FN:2,TP:162,Macc:0.60501350986,F1:0.705877267146\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2629 - acc: 0.8826 - val_loss: 0.6911 - val_acc: 0.6081\n",
      "9639/9639 [==============================] - 4s 453us/step\n",
      "TN:24,FP:147,FN:0,TP:164,Macc:0.570175400546,F1:0.6905213106\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2609 - acc: 0.8857 - val_loss: 0.6445 - val_acc: 0.6442\n",
      "9639/9639 [==============================] - 4s 462us/step\n",
      "TN:42,FP:129,FN:1,TP:163,Macc:0.619758195824,F1:0.71490717961\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2577 - acc: 0.8858 - val_loss: 0.6710 - val_acc: 0.6273\n",
      "9639/9639 [==============================] - 4s 460us/step\n",
      "TN:33,FP:138,FN:2,TP:162,Macc:0.59039362776,F1:0.6982708015\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2567 - acc: 0.8873 - val_loss: 0.6549 - val_acc: 0.6429\n",
      "9639/9639 [==============================] - 4s 460us/step\n",
      "TN:40,FP:131,FN:2,TP:162,Macc:0.610861462701,F1:0.708966457754\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2566 - acc: 0.8861 - val_loss: 0.6202 - val_acc: 0.6641\n",
      "9639/9639 [==============================] - 4s 455us/step\n",
      "TN:50,FP:121,FN:2,TP:162,Macc:0.640101226902,F1:0.724827068927\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2561 - acc: 0.8864 - val_loss: 0.6679 - val_acc: 0.6350\n",
      "9639/9639 [==============================] - 4s 453us/step\n",
      "TN:34,FP:137,FN:0,TP:164,Macc:0.599415164747,F1:0.705371288166\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2534 - acc: 0.8879 - val_loss: 0.6345 - val_acc: 0.6597\n",
      "9639/9639 [==============================] - 4s 450us/step\n",
      "TN:48,FP:123,FN:1,TP:163,Macc:0.637302054345,F1:0.72443931332\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2545 - acc: 0.8886 - val_loss: 0.6144 - val_acc: 0.6713\n",
      "9639/9639 [==============================] - 4s 452us/step\n",
      "TN:52,FP:119,FN:1,TP:163,Macc:0.648997960025,F1:0.730936553013\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2508 - acc: 0.8886 - val_loss: 0.6039 - val_acc: 0.6820\n",
      "9639/9639 [==============================] - 4s 451us/step\n",
      "TN:52,FP:119,FN:1,TP:163,Macc:0.648997960025,F1:0.730936553013\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2515 - acc: 0.8877 - val_loss: 0.5923 - val_acc: 0.6921\n",
      "9639/9639 [==============================] - 4s 449us/step\n",
      "TN:56,FP:115,FN:2,TP:162,Macc:0.657645085422,F1:0.734688702048\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2508 - acc: 0.8884 - val_loss: 0.5775 - val_acc: 0.7052\n",
      "9639/9639 [==============================] - 4s 448us/step\n",
      "TN:61,FP:110,FN:1,TP:163,Macc:0.675313747806,F1:0.745990227999\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2495 - acc: 0.8899 - val_loss: 0.5889 - val_acc: 0.6980\n",
      "9639/9639 [==============================] - 4s 452us/step\n",
      "TN:58,FP:113,FN:1,TP:163,Macc:0.666541818546,F1:0.740903910263\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2485 - acc: 0.8889 - val_loss: 0.5833 - val_acc: 0.7017\n",
      "9639/9639 [==============================] - 4s 453us/step\n",
      "TN:60,FP:111,FN:1,TP:163,Macc:0.672389771386,F1:0.74428704699\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2476 - acc: 0.8900 - val_loss: 0.5623 - val_acc: 0.7189\n",
      "9639/9639 [==============================] - 4s 452us/step\n",
      "TN:70,FP:101,FN:1,TP:163,Macc:0.701629535587,F1:0.761677004119\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2455 - acc: 0.8912 - val_loss: 0.5202 - val_acc: 0.7467\n",
      "9639/9639 [==============================] - 4s 451us/step\n",
      "TN:79,FP:92,FN:1,TP:163,Macc:0.727945323368,F1:0.778037678039\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2480 - acc: 0.8893 - val_loss: 0.6145 - val_acc: 0.6836\n",
      "9639/9639 [==============================] - 4s 451us/step\n",
      "TN:52,FP:119,FN:0,TP:164,Macc:0.652046740308,F1:0.733775614347\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2456 - acc: 0.8899 - val_loss: 0.5237 - val_acc: 0.7421\n",
      "9639/9639 [==============================] - 4s 450us/step\n",
      "TN:76,FP:95,FN:2,TP:162,Macc:0.716124613824,F1:0.769590927733\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2467 - acc: 0.8899 - val_loss: 0.5297 - val_acc: 0.7398\n",
      "9639/9639 [==============================] - 4s 452us/step\n",
      "TN:75,FP:96,FN:1,TP:163,Macc:0.716249417687,F1:0.770680316563\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2468 - acc: 0.8899 - val_loss: 0.5716 - val_acc: 0.7100\n",
      "9639/9639 [==============================] - 4s 451us/step\n",
      "TN:63,FP:108,FN:1,TP:163,Macc:0.681161700646,F1:0.749420082266\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 55s 45ms/step - loss: 0.2446 - acc: 0.8909 - val_loss: 0.5208 - val_acc: 0.7455\n",
      "9639/9639 [==============================] - 4s 453us/step\n",
      "TN:75,FP:96,FN:2,TP:162,Macc:0.713200637404,F1:0.767767244752\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2452 - acc: 0.8911 - val_loss: 0.4852 - val_acc: 0.7704\n",
      "9639/9639 [==============================] - 4s 446us/step\n",
      "TN:91,FP:80,FN:3,TP:161,Macc:0.756935479842,F1:0.79505638405\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2449 - acc: 0.8902 - val_loss: 0.5616 - val_acc: 0.7184\n",
      "9639/9639 [==============================] - 4s 453us/step\n",
      "TN:68,FP:103,FN:1,TP:163,Macc:0.695781582747,F1:0.758134305606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 54s 45ms/step - loss: 0.2448 - acc: 0.8914 - val_loss: 0.4663 - val_acc: 0.7867\n",
      "9639/9639 [==============================] - 4s 450us/step\n",
      "TN:100,FP:71,FN:4,TP:160,Macc:0.78020248734,F1:0.810121195433\n",
      "Loss: 0.460093\n",
      "Iteration No: 88 ended. Search finished for the next optimal point.\n",
      "Time taken: 3734.0368\n",
      "Function value obtained: 0.4601\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 89 started. Searching for the next optimal point.\n",
      "args [5, 3, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 146s 122ms/step - loss: 0.7696 - acc: 0.8483 - val_loss: 0.6056 - val_acc: 0.7708\n",
      "9639/9639 [==============================] - 39s 4ms/step\n",
      "TN:96,FP:75,FN:6,TP:158,Macc:0.762409021093,F1:0.795964395368\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.3075 - acc: 0.8729 - val_loss: 0.5511 - val_acc: 0.7617\n",
      "9639/9639 [==============================] - 5s 561us/step\n",
      "TN:97,FP:74,FN:8,TP:156,Macc:0.759235436946,F1:0.791872782741\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2874 - acc: 0.8790 - val_loss: 0.5650 - val_acc: 0.7321\n",
      "9639/9639 [==============================] - 5s 527us/step\n",
      "TN:81,FP:90,FN:5,TP:159,Macc:0.721598155075,F1:0.769970478916\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2762 - acc: 0.8828 - val_loss: 0.5467 - val_acc: 0.7413\n",
      "9639/9639 [==============================] - 5s 537us/step\n",
      "TN:88,FP:83,FN:6,TP:158,Macc:0.739017209732,F1:0.780241570041\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2684 - acc: 0.8849 - val_loss: 0.5948 - val_acc: 0.6928\n",
      "9639/9639 [==============================] - 5s 530us/step\n",
      "TN:70,FP:101,FN:4,TP:160,Macc:0.692483194737,F1:0.752935924033\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2623 - acc: 0.8873 - val_loss: 0.5047 - val_acc: 0.7690\n",
      "9639/9639 [==============================] - 5s 540us/step\n",
      "TN:96,FP:75,FN:2,TP:162,Macc:0.774604142226,F1:0.807974687854\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2584 - acc: 0.8890 - val_loss: 0.4948 - val_acc: 0.7756\n",
      "9639/9639 [==============================] - 5s 536us/step\n",
      "TN:98,FP:73,FN:4,TP:160,Macc:0.774354534499,F1:0.80603996156\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2532 - acc: 0.8926 - val_loss: 0.4651 - val_acc: 0.7975\n",
      "9639/9639 [==============================] - 5s 537us/step\n",
      "TN:105,FP:66,FN:3,TP:161,Macc:0.797871149723,F1:0.82352400825\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2507 - acc: 0.8917 - val_loss: 0.4683 - val_acc: 0.7982\n",
      "9639/9639 [==============================] - 5s 540us/step\n",
      "TN:108,FP:63,FN:7,TP:157,Macc:0.79444795785,F1:0.817702903491\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2475 - acc: 0.8929 - val_loss: 0.4154 - val_acc: 0.8334\n",
      "9639/9639 [==============================] - 5s 539us/step\n",
      "TN:118,FP:53,FN:7,TP:157,Macc:0.823687722051,F1:0.839566726364\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2480 - acc: 0.8933 - val_loss: 0.4545 - val_acc: 0.8129\n",
      "9639/9639 [==============================] - 5s 536us/step\n",
      "TN:113,FP:58,FN:11,TP:153,Macc:0.796872718818,F1:0.81599453856\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2434 - acc: 0.8957 - val_loss: 0.4482 - val_acc: 0.8057\n",
      "9639/9639 [==============================] - 5s 536us/step\n",
      "TN:110,FP:61,FN:8,TP:156,Macc:0.797247130407,F1:0.818892196975\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2423 - acc: 0.8956 - val_loss: 0.4469 - val_acc: 0.8048\n",
      "9639/9639 [==============================] - 5s 535us/step\n",
      "TN:108,FP:63,FN:6,TP:158,Macc:0.797496738134,F1:0.820773794469\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2402 - acc: 0.8963 - val_loss: 0.4139 - val_acc: 0.8316\n",
      "9639/9639 [==============================] - 5s 539us/step\n",
      "TN:117,FP:54,FN:7,TP:157,Macc:0.820763745631,F1:0.837327870641\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2390 - acc: 0.8979 - val_loss: 0.4117 - val_acc: 0.8378\n",
      "9639/9639 [==============================] - 5s 523us/step\n",
      "TN:121,FP:50,FN:10,TP:154,Macc:0.823313310462,F1:0.83695103672\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2364 - acc: 0.8980 - val_loss: 0.4211 - val_acc: 0.8280\n",
      "9639/9639 [==============================] - 5s 538us/step\n",
      "TN:123,FP:48,FN:11,TP:153,Macc:0.826112483019,F1:0.838350670405\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2367 - acc: 0.8986 - val_loss: 0.4801 - val_acc: 0.8095: 3s\n",
      "9639/9639 [==============================] - 5s 533us/step\n",
      "TN:123,FP:48,FN:19,TP:145,Macc:0.801722240752,F1:0.812319416242\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2343 - acc: 0.8990 - val_loss: 0.4256 - val_acc: 0.8268\n",
      "9639/9639 [==============================] - 5s 539us/step\n",
      "TN:126,FP:45,FN:14,TP:150,Macc:0.825738071429,F1:0.835649085937\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2350 - acc: 0.8982 - val_loss: 0.4156 - val_acc: 0.8330\n",
      "9639/9639 [==============================] - 5s 527us/step\n",
      "TN:120,FP:51,FN:9,TP:155,Macc:0.823438114325,F1:0.837832358925\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2308 - acc: 0.8994 - val_loss: 0.4149 - val_acc: 0.8325\n",
      "9639/9639 [==============================] - 5s 532us/step\n",
      "TN:120,FP:51,FN:9,TP:155,Macc:0.823438114325,F1:0.837832358925\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2320 - acc: 0.8997 - val_loss: 0.4098 - val_acc: 0.8359\n",
      "9639/9639 [==============================] - 5s 536us/step\n",
      "TN:130,FP:41,FN:18,TP:146,Macc:0.825238855976,F1:0.831903303418\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2301 - acc: 0.9021 - val_loss: 0.4375 - val_acc: 0.8295\n",
      "9639/9639 [==============================] - 5s 538us/step\n",
      "TN:131,FP:40,FN:26,TP:138,Macc:0.80377259013,F1:0.8070120012\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2292 - acc: 0.9018 - val_loss: 0.4242 - val_acc: 0.8264\n",
      "9639/9639 [==============================] - 5s 530us/step\n",
      "TN:122,FP:49,FN:9,TP:155,Macc:0.829286067165,F1:0.842385819004\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2278 - acc: 0.9011 - val_loss: 0.4066 - val_acc: 0.8398\n",
      "9639/9639 [==============================] - 5s 530us/step\n",
      "TN:129,FP:42,FN:13,TP:151,Macc:0.837558780973,F1:0.845932859548\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2274 - acc: 0.9024 - val_loss: 0.4564 - val_acc: 0.8174\n",
      "9639/9639 [==============================] - 5s 536us/step\n",
      "TN:134,FP:37,FN:27,TP:137,Macc:0.809495739107,F1:0.810645339661\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2273 - acc: 0.9021 - val_loss: 0.4251 - val_acc: 0.8273\n",
      "9639/9639 [==============================] - 5s 537us/step\n",
      "TN:127,FP:44,FN:17,TP:147,Macc:0.819515706999,F1:0.828163494613\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2268 - acc: 0.9013 - val_loss: 0.3909 - val_acc: 0.8468\n",
      "9639/9639 [==============================] - 5s 538us/step\n",
      "TN:141,FP:30,FN:23,TP:141,Macc:0.842158695181,F1:0.841785491932\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2256 - acc: 0.9027 - val_loss: 0.4486 - val_acc: 0.8224\n",
      "9639/9639 [==============================] - 5s 533us/step\n",
      "TN:136,FP:35,FN:23,TP:141,Macc:0.82753881308,F1:0.829406217926\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2238 - acc: 0.9042 - val_loss: 0.3885 - val_acc: 0.8406\n",
      "9639/9639 [==============================] - 5s 526us/step\n",
      "TN:130,FP:41,FN:13,TP:151,Macc:0.840482757393,F1:0.848309088376\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2241 - acc: 0.9035 - val_loss: 0.4429 - val_acc: 0.8229\n",
      "9639/9639 [==============================] - 5s 535us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:133,FP:38,FN:27,TP:137,Macc:0.806571762687,F1:0.808254040396\n",
      "Loss: 0.430931\n",
      "Iteration No: 89 ended. Search finished for the next optimal point.\n",
      "Time taken: 4175.8892\n",
      "Function value obtained: 0.4309\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 90 started. Searching for the next optimal point.\n",
      "args [5, 3, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 371s 309ms/step - loss: 0.7718 - acc: 0.8464 - val_loss: 0.6570 - val_acc: 0.6784\n",
      "9639/9639 [==============================] - 43s 4ms/step\n",
      "TN:60,FP:111,FN:1,TP:163,Macc:0.672389771386,F1:0.74428704699\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.3146 - acc: 0.8649 - val_loss: 0.5829 - val_acc: 0.7246\n",
      "9639/9639 [==============================] - 5s 564us/step\n",
      "TN:78,FP:93,FN:5,TP:159,Macc:0.712826225814,F1:0.76441778265\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2938 - acc: 0.8726 - val_loss: 0.5767 - val_acc: 0.7788\n",
      "9639/9639 [==============================] - 5s 555us/step\n",
      "TN:138,FP:33,FN:43,TP:121,Macc:0.772411160254,F1:0.761000742138\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2828 - acc: 0.8762 - val_loss: 0.4809 - val_acc: 0.8020\n",
      "9639/9639 [==============================] - 5s 545us/step\n",
      "TN:114,FP:57,FN:8,TP:156,Macc:0.808943036088,F1:0.82758075155\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2731 - acc: 0.8790 - val_loss: 0.4457 - val_acc: 0.8221\n",
      "9639/9639 [==============================] - 5s 554us/step\n",
      "TN:119,FP:52,FN:11,TP:153,Macc:0.814416577338,F1:0.829262811178\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2686 - acc: 0.8809 - val_loss: 0.4367 - val_acc: 0.8221\n",
      "9639/9639 [==============================] - 5s 546us/step\n",
      "TN:121,FP:50,FN:14,TP:150,Macc:0.811118189329,F1:0.824170328197\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2639 - acc: 0.8833 - val_loss: 0.4143 - val_acc: 0.8350\n",
      "9639/9639 [==============================] - 5s 539us/step\n",
      "TN:127,FP:44,FN:16,TP:148,Macc:0.822564487282,F1:0.831455156834\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2628 - acc: 0.8833 - val_loss: 0.4122 - val_acc: 0.8449\n",
      "9639/9639 [==============================] - 5s 547us/step\n",
      "TN:140,FP:31,FN:18,TP:146,Macc:0.854478620177,F1:0.856299438121\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2594 - acc: 0.8835 - val_loss: 0.4248 - val_acc: 0.8292\n",
      "9639/9639 [==============================] - 5s 533us/step\n",
      "TN:145,FP:26,FN:31,TP:133,Macc:0.829464358594,F1:0.823523857028\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2556 - acc: 0.8852 - val_loss: 0.4441 - val_acc: 0.8304\n",
      "9639/9639 [==============================] - 5s 542us/step\n",
      "TN:146,FP:25,FN:27,TP:137,Macc:0.844583456148,F1:0.840485241069\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2546 - acc: 0.8863 - val_loss: 0.4512 - val_acc: 0.8165\n",
      "9639/9639 [==============================] - 5s 540us/step\n",
      "TN:137,FP:34,FN:30,TP:134,Macc:0.809121327517,F1:0.807223363007\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2525 - acc: 0.8876 - val_loss: 0.3933 - val_acc: 0.8470\n",
      "9639/9639 [==============================] - 5s 546us/step\n",
      "TN:144,FP:27,FN:22,TP:142,Macc:0.853979404724,F1:0.852847297784\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2505 - acc: 0.8872 - val_loss: 0.4231 - val_acc: 0.8428\n",
      "9639/9639 [==============================] - 5s 538us/step\n",
      "TN:147,FP:24,FN:27,TP:137,Macc:0.847507432568,F1:0.843071366512\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2478 - acc: 0.8889 - val_loss: 0.4280 - val_acc: 0.8384\n",
      "9639/9639 [==============================] - 5s 535us/step\n",
      "TN:149,FP:22,FN:36,TP:128,Macc:0.825916362858,F1:0.815281078052\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2482 - acc: 0.8879 - val_loss: 0.4643 - val_acc: 0.8205\n",
      "9639/9639 [==============================] - 5s 539us/step\n",
      "TN:138,FP:33,FN:30,TP:134,Macc:0.812045303937,F1:0.80966212039\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2444 - acc: 0.8909 - val_loss: 0.4516 - val_acc: 0.8316\n",
      "9639/9639 [==============================] - 5s 537us/step\n",
      "TN:146,FP:25,FN:32,TP:132,Macc:0.829339554731,F1:0.822424352828\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2443 - acc: 0.8911 - val_loss: 0.4449 - val_acc: 0.8300\n",
      "9639/9639 [==============================] - 5s 540us/step\n",
      "TN:146,FP:25,FN:31,TP:133,Macc:0.832388335015,F1:0.826081402027\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2426 - acc: 0.8910 - val_loss: 0.5341 - val_acc: 0.7989\n",
      "9639/9639 [==============================] - 5s 542us/step\n",
      "TN:140,FP:31,FN:40,TP:124,Macc:0.787405453944,F1:0.777423917886\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2414 - acc: 0.8918 - val_loss: 0.4758 - val_acc: 0.8248\n",
      "9639/9639 [==============================] - 5s 543us/step\n",
      "TN:143,FP:28,FN:32,TP:132,Macc:0.820567625471,F1:0.814809260363\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2401 - acc: 0.8940 - val_loss: 0.6000 - val_acc: 0.7700\n",
      "9639/9639 [==============================] - 5s 545us/step\n",
      "TN:145,FP:26,FN:45,TP:119,Macc:0.786781434628,F1:0.770221003212\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2395 - acc: 0.8934 - val_loss: 0.4712 - val_acc: 0.8242\n",
      "9639/9639 [==============================] - 5s 543us/step\n",
      "TN:141,FP:30,FN:30,TP:134,Macc:0.820817233197,F1:0.817067615965\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2365 - acc: 0.8971 - val_loss: 0.5780 - val_acc: 0.7968\n",
      "9639/9639 [==============================] - 5s 530us/step\n",
      "TN:142,FP:29,FN:37,TP:127,Macc:0.802399747634,F1:0.793744448905\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2357 - acc: 0.8970 - val_loss: 0.4580 - val_acc: 0.8284\n",
      "9639/9639 [==============================] - 5s 534us/step\n",
      "TN:138,FP:33,FN:26,TP:138,Macc:0.82424042507,F1:0.823875045347\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2355 - acc: 0.8960 - val_loss: 0.5505 - val_acc: 0.8116\n",
      "9639/9639 [==============================] - 5s 542us/step\n",
      "TN:138,FP:33,FN:33,TP:131,Macc:0.802898963087,F1:0.798774934266\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2325 - acc: 0.8978 - val_loss: 0.5276 - val_acc: 0.7968\n",
      "9639/9639 [==============================] - 5s 539us/step\n",
      "TN:143,FP:28,FN:39,TP:125,Macc:0.799226163488,F1:0.788637985052\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2308 - acc: 0.8989 - val_loss: 0.9224 - val_acc: 0.6583\n",
      "9639/9639 [==============================] - 5s 544us/step\n",
      "TN:147,FP:24,FN:92,TP:72,Macc:0.649336714153,F1:0.553840983243\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2309 - acc: 0.8986 - val_loss: 0.8419 - val_acc: 0.7202\n",
      "9639/9639 [==============================] - 5s 539us/step\n",
      "TN:146,FP:25,FN:69,TP:95,Macc:0.716534684249,F1:0.669008664743\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2285 - acc: 0.9007 - val_loss: 1.0855 - val_acc: 0.6045\n",
      "9639/9639 [==============================] - 5s 542us/step\n",
      "TN:151,FP:20,FN:106,TP:58,Macc:0.618349695867,F1:0.479333994038\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2287 - acc: 0.9006 - val_loss: 0.7864 - val_acc: 0.7059\n",
      "9639/9639 [==============================] - 5s 537us/step\n",
      "TN:144,FP:27,FN:73,TP:91,Macc:0.698491610275,F1:0.645384666963\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2266 - acc: 0.9006 - val_loss: 0.6045 - val_acc: 0.7830\n",
      "9639/9639 [==============================] - 5s 542us/step\n",
      "TN:141,FP:30,FN:43,TP:121,Macc:0.781183089514,F1:0.768248424005\n",
      "Loss: 0.593481\n",
      "Iteration No: 90 ended. Search finished for the next optimal point.\n",
      "Time taken: 4650.4862\n",
      "Function value obtained: 0.5935\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 91 started. Searching for the next optimal point.\n",
      "args [5, 3, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 249s 207ms/step - loss: 0.7690 - acc: 0.8554 - val_loss: 0.5538 - val_acc: 0.8099\n",
      "9639/9639 [==============================] - 46s 5ms/step\n",
      "TN:118,FP:53,FN:21,TP:143,Macc:0.781004798085,F1:0.794438939389\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.3072 - acc: 0.8703 - val_loss: 0.4877 - val_acc: 0.8175\n",
      "9639/9639 [==============================] - 6s 574us/step\n",
      "TN:110,FP:61,FN:8,TP:156,Macc:0.797247130407,F1:0.818892196975\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2879 - acc: 0.8758 - val_loss: 0.4710 - val_acc: 0.8234\n",
      "9639/9639 [==============================] - 5s 555us/step\n",
      "TN:106,FP:65,FN:3,TP:161,Macc:0.800795126144,F1:0.825635618102\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 65s 55ms/step - loss: 0.2783 - acc: 0.8795 - val_loss: 0.5876 - val_acc: 0.7750\n",
      "9639/9639 [==============================] - 5s 554us/step\n",
      "TN:139,FP:32,FN:43,TP:121,Macc:0.775335136674,F1:0.763401393744\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2750 - acc: 0.8808 - val_loss: 0.4114 - val_acc: 0.8581\n",
      "9639/9639 [==============================] - 5s 548us/step\n",
      "TN:122,FP:49,FN:7,TP:157,Macc:0.835383627732,F1:0.848643169092\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 65s 55ms/step - loss: 0.2653 - acc: 0.8852 - val_loss: 0.6200 - val_acc: 0.6993\n",
      "9639/9639 [==============================] - 5s 556us/step\n",
      "TN:144,FP:27,FN:66,TP:98,Macc:0.719833072258,F1:0.678195240617\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2597 - acc: 0.8864 - val_loss: 0.4581 - val_acc: 0.8190\n",
      "9639/9639 [==============================] - 5s 555us/step\n",
      "TN:143,FP:28,FN:32,TP:132,Macc:0.820567625471,F1:0.814809260363\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 65s 54ms/step - loss: 0.2563 - acc: 0.8878 - val_loss: 0.4186 - val_acc: 0.8372\n",
      "9639/9639 [==============================] - 5s 554us/step\n",
      "TN:141,FP:30,FN:25,TP:139,Macc:0.836061134614,F1:0.834829280957\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 65s 55ms/step - loss: 0.2522 - acc: 0.8891 - val_loss: 0.6258 - val_acc: 0.7204\n",
      "9639/9639 [==============================] - 5s 554us/step\n",
      "TN:156,FP:15,FN:68,TP:96,Macc:0.748823228733,F1:0.698176466658\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2491 - acc: 0.8914 - val_loss: 1.0977 - val_acc: 0.5967\n",
      "9639/9639 [==============================] - 5s 553us/step\n",
      "TN:162,FP:9,FN:114,TP:50,Macc:0.626123194221,F1:0.448426168433\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2469 - acc: 0.8920 - val_loss: 0.5749 - val_acc: 0.7609\n",
      "9639/9639 [==============================] - 5s 548us/step\n",
      "TN:156,FP:15,FN:57,TP:107,Macc:0.782359811849,F1:0.748246309345\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2446 - acc: 0.8944 - val_loss: 0.5407 - val_acc: 0.7847\n",
      "9639/9639 [==============================] - 5s 558us/step\n",
      "TN:146,FP:25,FN:43,TP:121,Macc:0.795802971615,F1:0.780639624471\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2426 - acc: 0.8943 - val_loss: 0.4592 - val_acc: 0.8042\n",
      "9639/9639 [==============================] - 5s 550us/step\n",
      "TN:152,FP:19,FN:38,TP:126,Macc:0.828590731552,F1:0.81552844335\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2412 - acc: 0.8945 - val_loss: 0.3994 - val_acc: 0.8515\n",
      "9639/9639 [==============================] - 5s 552us/step\n",
      "TN:143,FP:28,FN:21,TP:143,Macc:0.854104208587,F1:0.853725789654\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2396 - acc: 0.8958 - val_loss: 0.9197 - val_acc: 0.6117\n",
      "9639/9639 [==============================] - 5s 557us/step\n",
      "TN:166,FP:5,FN:109,TP:55,Macc:0.653063001318,F1:0.491067065966\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2370 - acc: 0.8976 - val_loss: 0.7085 - val_acc: 0.6696\n",
      "9639/9639 [==============================] - 5s 557us/step\n",
      "TN:165,FP:6,FN:88,TP:76,Macc:0.714163410848,F1:0.617881234753\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2367 - acc: 0.8972 - val_loss: 0.4613 - val_acc: 0.8033\n",
      "9639/9639 [==============================] - 5s 554us/step\n",
      "TN:146,FP:25,FN:34,TP:130,Macc:0.823241994165,F1:0.815041470148\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2360 - acc: 0.8962 - val_loss: 0.5618 - val_acc: 0.7492\n",
      "9639/9639 [==============================] - 5s 548us/step\n",
      "TN:157,FP:14,FN:63,TP:101,Macc:0.76699110657,F1:0.724008949513\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2335 - acc: 0.8982 - val_loss: 0.6872 - val_acc: 0.6339\n",
      "9639/9639 [==============================] - 5s 551us/step\n",
      "TN:165,FP:6,FN:101,TP:63,Macc:0.674529267164,F1:0.540767895487\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2320 - acc: 0.8978 - val_loss: 0.5341 - val_acc: 0.7448\n",
      "9639/9639 [==============================] - 5s 554us/step\n",
      "TN:155,FP:16,FN:67,TP:97,Macc:0.748948032596,F1:0.700355641688\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2323 - acc: 0.8976 - val_loss: 0.5753 - val_acc: 0.7472\n",
      "9639/9639 [==============================] - 5s 551us/step\n",
      "TN:155,FP:16,FN:68,TP:96,Macc:0.745899252313,F1:0.695646813735\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2310 - acc: 0.8993 - val_loss: 0.9733 - val_acc: 0.6174\n",
      "9639/9639 [==============================] - 5s 552us/step\n",
      "TN:159,FP:12,FN:103,TP:61,Macc:0.650887848077,F1:0.514763195613\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2305 - acc: 0.8992 - val_loss: 0.6376 - val_acc: 0.7414\n",
      "9639/9639 [==============================] - 5s 552us/step\n",
      "TN:153,FP:18,FN:59,TP:105,Macc:0.767490322022,F1:0.731701873269\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2268 - acc: 0.9009 - val_loss: 0.9583 - val_acc: 0.6356\n",
      "9639/9639 [==============================] - 5s 555us/step\n",
      "TN:160,FP:11,FN:97,TP:67,Macc:0.672104506197,F1:0.553714152558\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2278 - acc: 0.9006 - val_loss: 0.8242 - val_acc: 0.7099\n",
      "9639/9639 [==============================] - 5s 556us/step\n",
      "TN:154,FP:17,FN:72,TP:92,Macc:0.730780154759,F1:0.673987342955\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2287 - acc: 0.8999 - val_loss: 0.9694 - val_acc: 0.6285\n",
      "9639/9639 [==============================] - 5s 552us/step\n",
      "TN:163,FP:8,FN:102,TP:62,Macc:0.665632534041,F1:0.529909867671\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2262 - acc: 0.9000 - val_loss: 0.9805 - val_acc: 0.6076\n",
      "9639/9639 [==============================] - 5s 553us/step\n",
      "TN:167,FP:4,FN:108,TP:56,Macc:0.659035758021,F1:0.499995636517\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2262 - acc: 0.9011 - val_loss: 0.8566 - val_acc: 0.6280\n",
      "9639/9639 [==============================] - 5s 549us/step\n",
      "TN:160,FP:11,FN:100,TP:64,Macc:0.662958165347,F1:0.535560066987\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2261 - acc: 0.9010 - val_loss: 0.7353 - val_acc: 0.7108\n",
      "9639/9639 [==============================] - 5s 554us/step\n",
      "TN:155,FP:16,FN:75,TP:89,Macc:0.72455779033,F1:0.661704747682\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 66s 55ms/step - loss: 0.2244 - acc: 0.9012 - val_loss: 1.2345 - val_acc: 0.5823\n",
      "9639/9639 [==============================] - 5s 552us/step\n",
      "TN:165,FP:6,FN:117,TP:47,Macc:0.625748782631,F1:0.433175618717\n",
      "Loss: 1.223356\n",
      "Iteration No: 91 ended. Search finished for the next optimal point.\n",
      "Time taken: 4649.9770\n",
      "Function value obtained: 1.2234\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 92 started. Searching for the next optimal point.\n",
      "args [1, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 172s 143ms/step - loss: 0.7569 - acc: 0.8450 - val_loss: 0.5507 - val_acc: 0.7226\n",
      "9639/9639 [==============================] - 44s 5ms/step\n",
      "TN:143,FP:28,FN:63,TP:101,Macc:0.726055436688,F1:0.68941432198\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.3032 - acc: 0.8578 - val_loss: 0.4909 - val_acc: 0.8157\n",
      "9639/9639 [==============================] - 5s 475us/step\n",
      "TN:132,FP:39,FN:26,TP:138,Macc:0.80669656655,F1:0.809378620035\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2879 - acc: 0.8659 - val_loss: 0.4787 - val_acc: 0.8309\n",
      "9639/9639 [==============================] - 5s 470us/step\n",
      "TN:126,FP:45,FN:16,TP:148,Macc:0.819640510862,F1:0.829126137895\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2804 - acc: 0.8729 - val_loss: 0.4650 - val_acc: 0.8140\n",
      "9639/9639 [==============================] - 5s 476us/step\n",
      "TN:128,FP:43,FN:25,TP:139,Macc:0.798049441153,F1:0.803462671928\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2739 - acc: 0.8778 - val_loss: 0.4588 - val_acc: 0.8240\n",
      "9639/9639 [==============================] - 4s 463us/step\n",
      "TN:120,FP:51,FN:16,TP:148,Macc:0.802096652342,F1:0.815421498993\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2693 - acc: 0.8809 - val_loss: 0.4512 - val_acc: 0.8196\n",
      "9639/9639 [==============================] - 5s 482us/step\n",
      "TN:111,FP:60,FN:14,TP:150,Macc:0.781878425128,F1:0.802133573487\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2646 - acc: 0.8825 - val_loss: 0.4269 - val_acc: 0.8328\n",
      "9639/9639 [==============================] - 5s 474us/step\n",
      "TN:128,FP:43,FN:18,TP:146,Macc:0.819390903136,F1:0.827189943491\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2620 - acc: 0.8840 - val_loss: 0.4921 - val_acc: 0.7941\n",
      "9639/9639 [==============================] - 5s 469us/step\n",
      "TN:105,FP:66,FN:10,TP:154,Macc:0.77652968774,F1:0.802077904387\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2601 - acc: 0.8854 - val_loss: 0.4300 - val_acc: 0.8286\n",
      "9639/9639 [==============================] - 5s 473us/step\n",
      "TN:116,FP:55,FN:14,TP:150,Macc:0.796498307228,F1:0.813002649547\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2578 - acc: 0.8863 - val_loss: 0.4869 - val_acc: 0.7913\n",
      "9639/9639 [==============================] - 5s 469us/step\n",
      "TN:98,FP:73,FN:6,TP:158,Macc:0.768256973933,F1:0.799994613719\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2570 - acc: 0.8857 - val_loss: 0.5364 - val_acc: 0.7330\n",
      "9639/9639 [==============================] - 5s 470us/step\n",
      "TN:76,FP:95,FN:3,TP:161,Macc:0.713075833541,F1:0.766661390444\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2526 - acc: 0.8886 - val_loss: 0.4247 - val_acc: 0.8252\n",
      "9639/9639 [==============================] - 4s 463us/step\n",
      "TN:132,FP:39,FN:22,TP:142,Macc:0.818891687683,F1:0.823182866695\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2520 - acc: 0.8878 - val_loss: 0.4494 - val_acc: 0.8081\n",
      "9639/9639 [==============================] - 5s 482us/step\n",
      "TN:102,FP:69,FN:6,TP:158,Macc:0.779952879613,F1:0.808178740572\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2511 - acc: 0.8897 - val_loss: 0.4819 - val_acc: 0.7787\n",
      "9639/9639 [==============================] - 4s 465us/step\n",
      "TN:96,FP:75,FN:5,TP:159,Macc:0.765457801376,F1:0.798989600879\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2517 - acc: 0.8891 - val_loss: 0.4007 - val_acc: 0.8419\n",
      "9639/9639 [==============================] - 4s 462us/step\n",
      "TN:116,FP:55,FN:6,TP:158,Macc:0.820888549495,F1:0.838190830506\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2488 - acc: 0.8897 - val_loss: 0.4011 - val_acc: 0.8471\n",
      "9639/9639 [==============================] - 5s 471us/step\n",
      "TN:126,FP:45,FN:14,TP:150,Macc:0.825738071429,F1:0.835649085937\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2480 - acc: 0.8904 - val_loss: 0.3959 - val_acc: 0.8467\n",
      "9639/9639 [==============================] - 4s 464us/step\n",
      "TN:121,FP:50,FN:12,TP:152,Macc:0.817215749895,F1:0.830595602293\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2474 - acc: 0.8914 - val_loss: 0.4104 - val_acc: 0.8545\n",
      "9639/9639 [==============================] - 5s 472us/step\n",
      "TN:132,FP:39,FN:14,TP:150,Macc:0.84328192995,F1:0.849852831596\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2455 - acc: 0.8902 - val_loss: 0.4298 - val_acc: 0.8445\n",
      "9639/9639 [==============================] - 4s 460us/step\n",
      "TN:117,FP:54,FN:5,TP:159,Macc:0.826861306198,F1:0.843495869984\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2437 - acc: 0.8921 - val_loss: 0.4146 - val_acc: 0.8590\n",
      "9639/9639 [==============================] - 5s 476us/step\n",
      "TN:124,FP:47,FN:7,TP:157,Macc:0.841231580572,F1:0.853255383571\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2438 - acc: 0.8919 - val_loss: 0.4099 - val_acc: 0.8534\n",
      "9639/9639 [==============================] - 5s 473us/step\n",
      "TN:133,FP:38,FN:15,TP:149,Macc:0.843157126086,F1:0.84899731944\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2430 - acc: 0.8918 - val_loss: 0.4588 - val_acc: 0.8292\n",
      "9639/9639 [==============================] - 4s 465us/step\n",
      "TN:113,FP:58,FN:2,TP:162,Macc:0.824311741368,F1:0.843744568665\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2428 - acc: 0.8929 - val_loss: 0.4383 - val_acc: 0.8358\n",
      "9639/9639 [==============================] - 5s 475us/step\n",
      "TN:149,FP:22,FN:33,TP:131,Macc:0.835062703708,F1:0.826492872013\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2407 - acc: 0.8936 - val_loss: 0.4585 - val_acc: 0.8361\n",
      "9639/9639 [==============================] - 5s 468us/step\n",
      "TN:148,FP:23,FN:32,TP:132,Macc:0.835187507571,F1:0.827580654236\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2405 - acc: 0.8925 - val_loss: 0.4359 - val_acc: 0.8448\n",
      "9639/9639 [==============================] - 5s 474us/step\n",
      "TN:145,FP:26,FN:22,TP:142,Macc:0.856903381144,F1:0.855416130896\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2409 - acc: 0.8927 - val_loss: 0.4268 - val_acc: 0.8512\n",
      "9639/9639 [==============================] - 4s 464us/step\n",
      "TN:142,FP:29,FN:20,TP:144,Macc:0.854229012451,F1:0.854593854696\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2393 - acc: 0.8945 - val_loss: 0.4847 - val_acc: 0.7692\n",
      "9639/9639 [==============================] - 5s 474us/step\n",
      "TN:156,FP:15,FN:58,TP:106,Macc:0.779311031566,F1:0.743854216942\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2400 - acc: 0.8934 - val_loss: 0.4555 - val_acc: 0.8345\n",
      "9639/9639 [==============================] - 5s 475us/step\n",
      "TN:146,FP:25,FN:30,TP:134,Macc:0.835437115298,F1:0.82971580707\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2398 - acc: 0.8942 - val_loss: 0.4557 - val_acc: 0.8491\n",
      "9639/9639 [==============================] - 5s 469us/step\n",
      "TN:144,FP:27,FN:21,TP:143,Macc:0.857028185008,F1:0.856281870558\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2381 - acc: 0.8952 - val_loss: 0.5034 - val_acc: 0.7019\n",
      "9639/9639 [==============================] - 5s 472us/step\n",
      "TN:166,FP:5,FN:75,TP:89,Macc:0.756721530951,F1:0.689917326701\n",
      "Loss: 0.498386\n",
      "Iteration No: 92 ended. Search finished for the next optimal point.\n",
      "Time taken: 4344.4723\n",
      "Function value obtained: 0.4984\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 93 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 114s 95ms/step - loss: 0.7590 - acc: 0.8462 - val_loss: 0.5943 - val_acc: 0.7272\n",
      "9639/9639 [==============================] - 43s 4ms/step\n",
      "TN:102,FP:69,FN:29,TP:135,Macc:0.709830933097,F1:0.733690173333\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.3030 - acc: 0.8590 - val_loss: 0.5145 - val_acc: 0.7737\n",
      "9639/9639 [==============================] - 5s 486us/step\n",
      "TN:115,FP:56,FN:21,TP:143,Macc:0.772232868825,F1:0.787873291297\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2878 - acc: 0.8691 - val_loss: 0.4903 - val_acc: 0.7913\n",
      "9639/9639 [==============================] - 5s 485us/step\n",
      "TN:112,FP:59,FN:12,TP:152,Macc:0.790899962114,F1:0.81066120554\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2771 - acc: 0.8743 - val_loss: 0.4885 - val_acc: 0.8128\n",
      "9639/9639 [==============================] - 5s 479us/step\n",
      "TN:127,FP:44,FN:20,TP:144,Macc:0.810369366149,F1:0.81817629265\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2727 - acc: 0.8784 - val_loss: 0.5441 - val_acc: 0.7289\n",
      "9639/9639 [==============================] - 5s 486us/step\n",
      "TN:78,FP:93,FN:0,TP:164,Macc:0.728070127231,F1:0.779092114884\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2635 - acc: 0.8819 - val_loss: 0.4598 - val_acc: 0.8085\n",
      "9639/9639 [==============================] - 5s 493us/step\n",
      "TN:115,FP:56,FN:10,TP:154,Macc:0.805769451941,F1:0.82352394656\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2605 - acc: 0.8827 - val_loss: 0.4992 - val_acc: 0.7612\n",
      "9639/9639 [==============================] - 5s 487us/step\n",
      "TN:96,FP:75,FN:6,TP:158,Macc:0.762409021093,F1:0.795964395368\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2603 - acc: 0.8821 - val_loss: 0.4350 - val_acc: 0.8116\n",
      "9639/9639 [==============================] - 5s 487us/step\n",
      "TN:116,FP:55,FN:12,TP:152,Macc:0.802595867795,F1:0.819401533416\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2570 - acc: 0.8836 - val_loss: 0.4387 - val_acc: 0.8053\n",
      "9639/9639 [==============================] - 5s 488us/step\n",
      "TN:106,FP:65,FN:6,TP:158,Macc:0.791648785294,F1:0.816532049151\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2573 - acc: 0.8854 - val_loss: 0.4361 - val_acc: 0.8096\n",
      "9639/9639 [==============================] - 5s 485us/step\n",
      "TN:110,FP:61,FN:6,TP:158,Macc:0.803344690974,F1:0.825059840214\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2524 - acc: 0.8853 - val_loss: 0.4527 - val_acc: 0.8189\n",
      "9639/9639 [==============================] - 5s 490us/step\n",
      "TN:125,FP:46,FN:20,TP:144,Macc:0.804521413309,F1:0.813553801179\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2534 - acc: 0.8871 - val_loss: 0.4290 - val_acc: 0.8147\n",
      "9639/9639 [==============================] - 5s 485us/step\n",
      "TN:113,FP:58,FN:8,TP:156,Macc:0.806019059668,F1:0.825391373625\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2515 - acc: 0.8865 - val_loss: 0.4138 - val_acc: 0.8250\n",
      "9639/9639 [==============================] - 5s 480us/step\n",
      "TN:114,FP:57,FN:5,TP:159,Macc:0.818089376938,F1:0.836836659841\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2522 - acc: 0.8866 - val_loss: 0.4248 - val_acc: 0.8163\n",
      "9639/9639 [==============================] - 5s 486us/step\n",
      "TN:107,FP:64,FN:2,TP:162,Macc:0.806767882847,F1:0.830763822941\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2501 - acc: 0.8877 - val_loss: 0.4072 - val_acc: 0.8275\n",
      "9639/9639 [==============================] - 5s 482us/step\n",
      "TN:118,FP:53,FN:8,TP:156,Macc:0.820638941768,F1:0.836455656757\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2491 - acc: 0.8882 - val_loss: 0.3832 - val_acc: 0.8408\n",
      "9639/9639 [==============================] - 5s 489us/step\n",
      "TN:119,FP:52,FN:8,TP:156,Macc:0.823562918188,F1:0.838704204799\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2477 - acc: 0.8881 - val_loss: 0.4200 - val_acc: 0.8156\n",
      "9639/9639 [==============================] - 5s 485us/step\n",
      "TN:107,FP:64,FN:1,TP:163,Macc:0.80981666313,F1:0.833754186702\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2466 - acc: 0.8896 - val_loss: 0.3925 - val_acc: 0.8406\n",
      "9639/9639 [==============================] - 5s 482us/step\n",
      "TN:131,FP:40,FN:17,TP:147,Macc:0.83121161268,F1:0.837601308759\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2478 - acc: 0.8881 - val_loss: 0.4214 - val_acc: 0.8121\n",
      "9639/9639 [==============================] - 5s 480us/step\n",
      "TN:106,FP:65,FN:2,TP:162,Macc:0.803843906427,F1:0.828639097476\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2451 - acc: 0.8893 - val_loss: 0.3796 - val_acc: 0.8399\n",
      "9639/9639 [==============================] - 5s 494us/step\n",
      "TN:123,FP:48,FN:10,TP:154,Macc:0.829161263302,F1:0.841524563384\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2465 - acc: 0.8882 - val_loss: 0.3829 - val_acc: 0.8355\n",
      "9639/9639 [==============================] - 5s 482us/step\n",
      "TN:114,FP:57,FN:5,TP:159,Macc:0.818089376938,F1:0.836836659841\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2434 - acc: 0.8909 - val_loss: 0.3800 - val_acc: 0.8452\n",
      "9639/9639 [==============================] - 5s 484us/step\n",
      "TN:130,FP:41,FN:14,TP:150,Macc:0.837433977109,F1:0.845064902015\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2428 - acc: 0.8900 - val_loss: 0.3862 - val_acc: 0.8417\n",
      "9639/9639 [==============================] - 5s 486us/step\n",
      "TN:126,FP:45,FN:11,TP:153,Macc:0.834884412279,F1:0.845298364584\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2436 - acc: 0.8907 - val_loss: 0.3836 - val_acc: 0.8437\n",
      "9639/9639 [==============================] - 5s 484us/step\n",
      "TN:129,FP:42,FN:15,TP:149,Macc:0.831461220406,F1:0.839431099548\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2432 - acc: 0.8906 - val_loss: 0.3738 - val_acc: 0.8466\n",
      "9639/9639 [==============================] - 5s 487us/step\n",
      "TN:125,FP:46,FN:9,TP:155,Macc:0.838057996425,F1:0.849309573854\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2415 - acc: 0.8916 - val_loss: 0.3669 - val_acc: 0.8481\n",
      "9639/9639 [==============================] - 5s 488us/step\n",
      "TN:125,FP:46,FN:8,TP:156,Macc:0.841106776709,F1:0.852453524476\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2421 - acc: 0.8916 - val_loss: 0.3643 - val_acc: 0.8505\n",
      "9639/9639 [==============================] - 5s 481us/step\n",
      "TN:124,FP:47,FN:8,TP:156,Macc:0.838182800289,F1:0.850130750965\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2390 - acc: 0.8927 - val_loss: 0.3775 - val_acc: 0.8415\n",
      "9639/9639 [==============================] - 5s 482us/step\n",
      "TN:120,FP:51,FN:5,TP:159,Macc:0.835633235458,F1:0.8502619129\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2391 - acc: 0.8922 - val_loss: 0.3826 - val_acc: 0.8464\n",
      "9639/9639 [==============================] - 5s 484us/step\n",
      "TN:133,FP:38,FN:17,TP:147,Macc:0.83705956552,F1:0.842401343637\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2407 - acc: 0.8918 - val_loss: 0.3705 - val_acc: 0.8482\n",
      "9639/9639 [==============================] - 5s 493us/step\n",
      "TN:127,FP:44,FN:11,TP:153,Macc:0.837808388699,F1:0.847639923701\n",
      "Loss: 0.365293\n",
      "Iteration No: 93 ended. Search finished for the next optimal point.\n",
      "Time taken: 4065.8780\n",
      "Function value obtained: 0.3653\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 94 started. Searching for the next optimal point.\n",
      "args [4, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 92s 77ms/step - loss: 0.7886 - acc: 0.8324 - val_loss: 0.6815 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 42s 4ms/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.3202 - acc: 0.8325 - val_loss: 0.6293 - val_acc: 0.4693\n",
      "9639/9639 [==============================] - 5s 506us/step\n",
      "TN:171,FP:0,FN:164,TP:0,Macc:0.499999967836,F1:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.3036 - acc: 0.8429 - val_loss: 0.6401 - val_acc: 0.6447\n",
      "9639/9639 [==============================] - 5s 498us/step\n",
      "TN:49,FP:122,FN:0,TP:164,Macc:0.643274811048,F1:0.728883757547\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2917 - acc: 0.8603 - val_loss: 0.5781 - val_acc: 0.7099\n",
      "9639/9639 [==============================] - 5s 495us/step\n",
      "TN:70,FP:101,FN:2,TP:162,Macc:0.698580755303,F1:0.758776957996\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2879 - acc: 0.8630 - val_loss: 0.5326 - val_acc: 0.7542\n",
      "9639/9639 [==============================] - 5s 497us/step\n",
      "TN:88,FP:83,FN:1,TP:163,Macc:0.754261111148,F1:0.795116628589\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2813 - acc: 0.8653 - val_loss: 0.5298 - val_acc: 0.7555\n",
      "9639/9639 [==============================] - 5s 493us/step\n",
      "TN:91,FP:80,FN:2,TP:162,Macc:0.759984260125,F1:0.798024216444\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2790 - acc: 0.8690 - val_loss: 0.5103 - val_acc: 0.7711\n",
      "9639/9639 [==============================] - 5s 495us/step\n",
      "TN:99,FP:72,FN:5,TP:159,Macc:0.774229730636,F1:0.805057904576\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2762 - acc: 0.8717 - val_loss: 0.5174 - val_acc: 0.7625\n",
      "9639/9639 [==============================] - 5s 500us/step\n",
      "TN:94,FP:77,FN:1,TP:163,Macc:0.771804969669,F1:0.8069253438\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2743 - acc: 0.8721 - val_loss: 0.4768 - val_acc: 0.8018\n",
      "9639/9639 [==============================] - 5s 495us/step\n",
      "TN:107,FP:64,FN:4,TP:160,Macc:0.80067032228,F1:0.824736852835\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2727 - acc: 0.8734 - val_loss: 0.4903 - val_acc: 0.7907\n",
      "9639/9639 [==============================] - 5s 508us/step\n",
      "TN:107,FP:64,FN:3,TP:161,Macc:0.803719102564,F1:0.82775808461\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2709 - acc: 0.8736 - val_loss: 0.4968 - val_acc: 0.7784\n",
      "9639/9639 [==============================] - 5s 491us/step\n",
      "TN:107,FP:64,FN:9,TP:155,Macc:0.785426420864,F1:0.80939404477\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2698 - acc: 0.8759 - val_loss: 0.4708 - val_acc: 0.8063\n",
      "9639/9639 [==============================] - 5s 500us/step\n",
      "TN:108,FP:63,FN:7,TP:157,Macc:0.79444795785,F1:0.817702903491\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2674 - acc: 0.8763 - val_loss: 0.4958 - val_acc: 0.7786\n",
      "9639/9639 [==============================] - 5s 489us/step\n",
      "TN:125,FP:46,FN:27,TP:137,Macc:0.783179951326,F1:0.789619826696\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2660 - acc: 0.8782 - val_loss: 0.4830 - val_acc: 0.8083\n",
      "9639/9639 [==============================] - 5s 491us/step\n",
      "TN:132,FP:39,FN:34,TP:130,Macc:0.782306324283,F1:0.780775230476\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2671 - acc: 0.8768 - val_loss: 0.4532 - val_acc: 0.8178\n",
      "9639/9639 [==============================] - 5s 492us/step\n",
      "TN:108,FP:63,FN:6,TP:158,Macc:0.797496738134,F1:0.820773794469\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2639 - acc: 0.8788 - val_loss: 0.4600 - val_acc: 0.8071\n",
      "9639/9639 [==============================] - 5s 491us/step\n",
      "TN:114,FP:57,FN:10,TP:154,Macc:0.802845475521,F1:0.821327871581\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2640 - acc: 0.8791 - val_loss: 0.4700 - val_acc: 0.7960\n",
      "9639/9639 [==============================] - 5s 497us/step\n",
      "TN:110,FP:61,FN:6,TP:158,Macc:0.803344690974,F1:0.825059840214\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2639 - acc: 0.8789 - val_loss: 0.4695 - val_acc: 0.8021\n",
      "9639/9639 [==============================] - 5s 492us/step\n",
      "TN:109,FP:62,FN:7,TP:157,Macc:0.797371934271,F1:0.819837908399\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2617 - acc: 0.8801 - val_loss: 0.4492 - val_acc: 0.8126\n",
      "9639/9639 [==============================] - 5s 489us/step\n",
      "TN:110,FP:61,FN:6,TP:158,Macc:0.803344690974,F1:0.825059840214\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2604 - acc: 0.8801 - val_loss: 0.4311 - val_acc: 0.8304\n",
      "9639/9639 [==============================] - 5s 491us/step\n",
      "TN:110,FP:61,FN:7,TP:157,Macc:0.800295910691,F1:0.821984091398\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2599 - acc: 0.8794 - val_loss: 0.4419 - val_acc: 0.8203\n",
      "9639/9639 [==============================] - 5s 497us/step\n",
      "TN:109,FP:62,FN:7,TP:157,Macc:0.797371934271,F1:0.819837908399\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2581 - acc: 0.8808 - val_loss: 0.4526 - val_acc: 0.8063\n",
      "9639/9639 [==============================] - 5s 494us/step\n",
      "TN:120,FP:51,FN:18,TP:146,Macc:0.795999091775,F1:0.808858762631\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2576 - acc: 0.8817 - val_loss: 0.4497 - val_acc: 0.8221\n",
      "9639/9639 [==============================] - 5s 489us/step\n",
      "TN:123,FP:48,FN:18,TP:146,Macc:0.804771021035,F1:0.815636946636\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2562 - acc: 0.8822 - val_loss: 0.4389 - val_acc: 0.8219\n",
      "9639/9639 [==============================] - 5s 496us/step\n",
      "TN:113,FP:58,FN:6,TP:158,Macc:0.812116620234,F1:0.831573502251\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2557 - acc: 0.8816 - val_loss: 0.4410 - val_acc: 0.8170\n",
      "9639/9639 [==============================] - 5s 499us/step\n",
      "TN:121,FP:50,FN:18,TP:146,Macc:0.798923068195,F1:0.811105605037\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2555 - acc: 0.8823 - val_loss: 0.4755 - val_acc: 0.8157\n",
      "9639/9639 [==============================] - 5s 488us/step\n",
      "TN:123,FP:48,FN:22,TP:142,Macc:0.792575899902,F1:0.802254366854\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2567 - acc: 0.8839 - val_loss: 0.4132 - val_acc: 0.8385\n",
      "9639/9639 [==============================] - 5s 492us/step\n",
      "TN:114,FP:57,FN:4,TP:160,Macc:0.821138157221,F1:0.83988957109\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2547 - acc: 0.8835 - val_loss: 0.4338 - val_acc: 0.8203\n",
      "9639/9639 [==============================] - 5s 493us/step\n",
      "TN:124,FP:47,FN:18,TP:146,Macc:0.807694997456,F1:0.817921656793\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2531 - acc: 0.8842 - val_loss: 0.4356 - val_acc: 0.8295\n",
      "9639/9639 [==============================] - 5s 494us/step\n",
      "TN:123,FP:48,FN:16,TP:148,Macc:0.810868581602,F1:0.822216715468\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2544 - acc: 0.8832 - val_loss: 0.4310 - val_acc: 0.8375\n",
      "9639/9639 [==============================] - 5s 493us/step\n",
      "TN:133,FP:38,FN:18,TP:146,Macc:0.834010785236,F1:0.839074924927\n",
      "Loss: 0.424295\n",
      "Iteration No: 94 ended. Search finished for the next optimal point.\n",
      "Time taken: 3918.4864\n",
      "Function value obtained: 0.4243\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 95 started. Searching for the next optimal point.\n",
      "args [3, 1, 1]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 97s 81ms/step - loss: 0.7492 - acc: 0.8503 - val_loss: 0.5649 - val_acc: 0.7689\n",
      "9639/9639 [==============================] - 44s 5ms/step\n",
      "TN:113,FP:58,FN:24,TP:140,Macc:0.757238575135,F1:0.773475164533\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2974 - acc: 0.8666 - val_loss: 0.5958 - val_acc: 0.6934\n",
      "9639/9639 [==============================] - 5s 500us/step\n",
      "TN:72,FP:99,FN:4,TP:160,Macc:0.698331147577,F1:0.756495920138\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2841 - acc: 0.8707 - val_loss: 0.5378 - val_acc: 0.7585\n",
      "9639/9639 [==============================] - 5s 490us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:85,FP:86,FN:1,TP:163,Macc:0.745489181888,F1:0.789340937931\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2785 - acc: 0.8737 - val_loss: 0.4597 - val_acc: 0.8181\n",
      "9639/9639 [==============================] - 5s 489us/step\n",
      "TN:120,FP:51,FN:15,TP:149,Macc:0.805145432625,F1:0.818675823035\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2728 - acc: 0.8763 - val_loss: 0.5196 - val_acc: 0.7649\n",
      "9639/9639 [==============================] - 5s 487us/step\n",
      "TN:87,FP:84,FN:0,TP:164,Macc:0.754385915012,F1:0.796111191005\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2669 - acc: 0.8785 - val_loss: 0.4610 - val_acc: 0.8300\n",
      "9639/9639 [==============================] - 5s 498us/step\n",
      "TN:117,FP:54,FN:5,TP:159,Macc:0.826861306198,F1:0.843495869984\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2644 - acc: 0.8811 - val_loss: 0.4954 - val_acc: 0.7901\n",
      "9639/9639 [==============================] - 5s 490us/step\n",
      "TN:99,FP:72,FN:4,TP:160,Macc:0.77727851092,F1:0.8080754254\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2643 - acc: 0.8812 - val_loss: 0.4532 - val_acc: 0.8298\n",
      "9639/9639 [==============================] - 5s 488us/step\n",
      "TN:123,FP:48,FN:11,TP:153,Macc:0.826112483019,F1:0.838350670405\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2631 - acc: 0.8806 - val_loss: 0.4869 - val_acc: 0.8082\n",
      "9639/9639 [==============================] - 5s 492us/step\n",
      "TN:110,FP:61,FN:3,TP:161,Macc:0.812491031824,F1:0.834191467859\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2613 - acc: 0.8817 - val_loss: 0.4774 - val_acc: 0.8036\n",
      "9639/9639 [==============================] - 5s 490us/step\n",
      "TN:103,FP:68,FN:1,TP:163,Macc:0.79812075745,F1:0.825311068004\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2600 - acc: 0.8824 - val_loss: 0.4594 - val_acc: 0.8287\n",
      "9639/9639 [==============================] - 5s 487us/step\n",
      "TN:118,FP:53,FN:4,TP:160,Macc:0.832834062901,F1:0.848800909462\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2580 - acc: 0.8831 - val_loss: 0.4863 - val_acc: 0.8021\n",
      "9639/9639 [==============================] - 5s 486us/step\n",
      "TN:109,FP:62,FN:5,TP:159,Macc:0.803469494837,F1:0.825968599367\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2584 - acc: 0.8835 - val_loss: 0.4409 - val_acc: 0.8379\n",
      "9639/9639 [==============================] - 5s 488us/step\n",
      "TN:123,FP:48,FN:4,TP:160,Macc:0.847453945002,F1:0.86020957987\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2561 - acc: 0.8853 - val_loss: 0.4442 - val_acc: 0.8247\n",
      "9639/9639 [==============================] - 5s 484us/step\n",
      "TN:144,FP:27,FN:35,TP:129,Macc:0.814345261041,F1:0.806244448045\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2582 - acc: 0.8831 - val_loss: 0.4738 - val_acc: 0.8155\n",
      "9639/9639 [==============================] - 5s 483us/step\n",
      "TN:145,FP:26,FN:32,TP:132,Macc:0.826415578311,F1:0.819870222328\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2555 - acc: 0.8839 - val_loss: 0.4710 - val_acc: 0.8236\n",
      "9639/9639 [==============================] - 5s 485us/step\n",
      "TN:127,FP:44,FN:11,TP:153,Macc:0.837808388699,F1:0.847639923701\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2544 - acc: 0.8856 - val_loss: 0.4421 - val_acc: 0.8457\n",
      "9639/9639 [==============================] - 5s 496us/step\n",
      "TN:122,FP:49,FN:2,TP:162,Macc:0.850627529148,F1:0.863994535742\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2539 - acc: 0.8854 - val_loss: 0.4385 - val_acc: 0.8375\n",
      "9639/9639 [==============================] - 5s 491us/step\n",
      "TN:130,FP:41,FN:15,TP:149,Macc:0.834385196826,F1:0.841802386993\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2544 - acc: 0.8862 - val_loss: 0.4427 - val_acc: 0.8455\n",
      "9639/9639 [==============================] - 5s 488us/step\n",
      "TN:143,FP:28,FN:19,TP:145,Macc:0.860201769154,F1:0.86052857241\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2524 - acc: 0.8861 - val_loss: 0.4375 - val_acc: 0.8520\n",
      "9639/9639 [==============================] - 5s 483us/step\n",
      "TN:132,FP:39,FN:9,TP:155,Macc:0.858525831366,F1:0.865916273153\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2525 - acc: 0.8850 - val_loss: 0.4665 - val_acc: 0.8258\n",
      "9639/9639 [==============================] - 5s 492us/step\n",
      "TN:118,FP:53,FN:8,TP:156,Macc:0.820638941768,F1:0.836455656757\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2516 - acc: 0.8860 - val_loss: 0.4539 - val_acc: 0.8489\n",
      "9639/9639 [==============================] - 5s 488us/step\n",
      "TN:150,FP:21,FN:29,TP:135,Macc:0.850181801262,F1:0.843744445465\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2503 - acc: 0.8874 - val_loss: 0.4479 - val_acc: 0.8470\n",
      "9639/9639 [==============================] - 5s 483us/step\n",
      "TN:131,FP:40,FN:12,TP:152,Macc:0.846455514096,F1:0.853927065557\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2504 - acc: 0.8869 - val_loss: 0.4650 - val_acc: 0.8408\n",
      "9639/9639 [==============================] - 5s 484us/step\n",
      "TN:136,FP:35,FN:15,TP:149,Macc:0.851929055347,F1:0.856316303146\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2494 - acc: 0.8873 - val_loss: 0.4394 - val_acc: 0.8424\n",
      "9639/9639 [==============================] - 5s 496us/step\n",
      "TN:136,FP:35,FN:18,TP:146,Macc:0.842782714497,F1:0.846371271012\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2490 - acc: 0.8879 - val_loss: 0.4461 - val_acc: 0.8524\n",
      "9639/9639 [==============================] - 5s 485us/step\n",
      "TN:144,FP:27,FN:20,TP:144,Macc:0.860076965291,F1:0.859695938516\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2485 - acc: 0.8880 - val_loss: 0.4504 - val_acc: 0.8458\n",
      "9639/9639 [==============================] - 5s 482us/step\n",
      "TN:150,FP:21,FN:25,TP:139,Macc:0.862376922395,F1:0.858019133971\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2478 - acc: 0.8889 - val_loss: 0.4717 - val_acc: 0.8193\n",
      "9639/9639 [==============================] - 5s 486us/step\n",
      "TN:126,FP:45,FN:13,TP:151,Macc:0.828786851712,F1:0.838883381116\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2481 - acc: 0.8888 - val_loss: 0.4424 - val_acc: 0.8411\n",
      "9639/9639 [==============================] - 5s 487us/step\n",
      "TN:143,FP:28,FN:29,TP:135,Macc:0.829713966321,F1:0.825682517932\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2481 - acc: 0.8881 - val_loss: 0.4635 - val_acc: 0.8397\n",
      "9639/9639 [==============================] - 5s 488us/step\n",
      "TN:126,FP:45,FN:9,TP:155,Macc:0.840981972846,F1:0.851642854008\n",
      "Loss: 0.458217\n",
      "Iteration No: 95 ended. Search finished for the next optimal point.\n",
      "Time taken: 3998.6531\n",
      "Function value obtained: 0.4582\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 96 started. Searching for the next optimal point.\n",
      "args [1, 1, 11]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 119s 99ms/step - loss: 0.7413 - acc: 0.8556 - val_loss: 0.6160 - val_acc: 0.5796\n",
      "9639/9639 [==============================] - 46s 5ms/step\n",
      "TN:167,FP:4,FN:129,TP:35,Macc:0.595011372072,F1:0.344824134276\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2928 - acc: 0.8688 - val_loss: 0.5403 - val_acc: 0.7204\n",
      "9639/9639 [==============================] - 5s 483us/step\n",
      "TN:151,FP:20,FN:77,TP:87,Macc:0.706764324083,F1:0.642061111902\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2850 - acc: 0.8679 - val_loss: 0.5051 - val_acc: 0.6957\n",
      "9639/9639 [==============================] - 5s 483us/step\n",
      "TN:145,FP:26,FN:87,TP:77,Macc:0.658732662729,F1:0.576773765817\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2783 - acc: 0.8703 - val_loss: 0.4756 - val_acc: 0.7454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9639/9639 [==============================] - 5s 482us/step\n",
      "TN:139,FP:32,FN:61,TP:103,Macc:0.720457091575,F1:0.688957711791\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2729 - acc: 0.8731 - val_loss: 0.6646 - val_acc: 0.6390\n",
      "9639/9639 [==============================] - 5s 480us/step\n",
      "TN:165,FP:6,FN:115,TP:49,Macc:0.631846343198,F1:0.447484402031\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2661 - acc: 0.8748 - val_loss: 0.4681 - val_acc: 0.7536\n",
      "9639/9639 [==============================] - 5s 481us/step\n",
      "TN:158,FP:13,FN:75,TP:89,Macc:0.73332971959,F1:0.669167675828\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2658 - acc: 0.8751 - val_loss: 0.5385 - val_acc: 0.6910\n",
      "9639/9639 [==============================] - 5s 481us/step\n",
      "TN:162,FP:9,FN:97,TP:67,Macc:0.677952459037,F1:0.558328521638\n",
      "Epoch 8/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2641 - acc: 0.8765 - val_loss: 0.5406 - val_acc: 0.6984\n",
      "9639/9639 [==============================] - 5s 481us/step\n",
      "TN:156,FP:15,FN:94,TP:70,Macc:0.669554941367,F1:0.562243999979\n",
      "Epoch 9/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2622 - acc: 0.8768 - val_loss: 0.6418 - val_acc: 0.6497\n",
      "9639/9639 [==============================] - 5s 481us/step\n",
      "TN:162,FP:9,FN:109,TP:55,Macc:0.641367095638,F1:0.482451651857\n",
      "Epoch 10/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2608 - acc: 0.8781 - val_loss: 0.5432 - val_acc: 0.6882\n",
      "9639/9639 [==============================] - 5s 485us/step\n",
      "TN:164,FP:7,FN:99,TP:65,Macc:0.677702851311,F1:0.550842742139\n",
      "Epoch 11/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2582 - acc: 0.8781 - val_loss: 0.6968 - val_acc: 0.6422\n",
      "9639/9639 [==============================] - 5s 480us/step\n",
      "TN:167,FP:4,FN:117,TP:47,Macc:0.631596735472,F1:0.437205276921\n",
      "Epoch 12/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2590 - acc: 0.8793 - val_loss: 0.7820 - val_acc: 0.6169\n",
      "9639/9639 [==============================] - 5s 481us/step\n",
      "TN:165,FP:6,FN:119,TP:45,Macc:0.619651222065,F1:0.418600627663\n",
      "Epoch 13/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2569 - acc: 0.8782 - val_loss: 0.5063 - val_acc: 0.7232\n",
      "9639/9639 [==============================] - 5s 481us/step\n",
      "TN:155,FP:16,FN:83,TP:81,Macc:0.700167548063,F1:0.620684465332\n",
      "Epoch 14/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2566 - acc: 0.8791 - val_loss: 0.6805 - val_acc: 0.6312\n",
      "9639/9639 [==============================] - 5s 484us/step\n",
      "TN:162,FP:9,FN:116,TP:48,Macc:0.620025633654,F1:0.434384886345\n",
      "Epoch 15/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2571 - acc: 0.8799 - val_loss: 0.5962 - val_acc: 0.6668\n",
      "9639/9639 [==============================] - 5s 478us/step\n",
      "TN:165,FP:6,FN:107,TP:57,Macc:0.656236585465,F1:0.502198183353\n",
      "Epoch 16/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2540 - acc: 0.8813 - val_loss: 0.7275 - val_acc: 0.6187\n",
      "9639/9639 [==============================] - 5s 482us/step\n",
      "TN:167,FP:4,FN:121,TP:43,Macc:0.619401614338,F1:0.407579087028\n",
      "Epoch 17/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2539 - acc: 0.8798 - val_loss: 0.7948 - val_acc: 0.6120\n",
      "9639/9639 [==============================] - 5s 483us/step\n",
      "TN:167,FP:4,FN:124,TP:40,Macc:0.610255273488,F1:0.384611674591\n",
      "Epoch 18/30\n",
      "1201/1201 [==============================] - 55s 46ms/step - loss: 0.2517 - acc: 0.8818 - val_loss: 0.6421 - val_acc: 0.6600\n",
      "9639/9639 [==============================] - 5s 479us/step\n",
      "TN:161,FP:10,FN:104,TP:60,Macc:0.653687020634,F1:0.512815852185\n",
      "Epoch 19/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2521 - acc: 0.8817 - val_loss: 0.5955 - val_acc: 0.6744\n",
      "9639/9639 [==============================] - 5s 480us/step\n",
      "TN:164,FP:7,FN:102,TP:62,Macc:0.668556510461,F1:0.532184205311\n",
      "Epoch 20/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2495 - acc: 0.8822 - val_loss: 0.7370 - val_acc: 0.6284\n",
      "9639/9639 [==============================] - 5s 480us/step\n",
      "TN:163,FP:8,FN:115,TP:49,Macc:0.625998390358,F1:0.443434659199\n",
      "Epoch 21/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2477 - acc: 0.8835 - val_loss: 0.6272 - val_acc: 0.6144\n",
      "9639/9639 [==============================] - 5s 487us/step\n",
      "TN:167,FP:4,FN:116,TP:48,Macc:0.634645515755,F1:0.444440377951\n",
      "Epoch 22/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2480 - acc: 0.8847 - val_loss: 0.6802 - val_acc: 0.6400\n",
      "9639/9639 [==============================] - 5s 480us/step\n",
      "TN:167,FP:4,FN:113,TP:51,Macc:0.643791856605,F1:0.465749240378\n",
      "Epoch 23/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2489 - acc: 0.8843 - val_loss: 0.8181 - val_acc: 0.5755\n",
      "9639/9639 [==============================] - 5s 479us/step\n",
      "TN:168,FP:3,FN:134,TP:30,Macc:0.582691447075,F1:0.304565425988\n",
      "Epoch 24/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2467 - acc: 0.8867 - val_loss: 0.6118 - val_acc: 0.6035\n",
      "9639/9639 [==============================] - 5s 479us/step\n",
      "TN:170,FP:1,FN:123,TP:41,Macc:0.622075983032,F1:0.398054639017\n",
      "Epoch 25/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2464 - acc: 0.8860 - val_loss: 0.6632 - val_acc: 0.6240\n",
      "9639/9639 [==============================] - 5s 481us/step\n",
      "TN:167,FP:4,FN:110,TP:54,Macc:0.652938197455,F1:0.486482192229\n",
      "Epoch 26/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2441 - acc: 0.8879 - val_loss: 0.7820 - val_acc: 0.4995\n",
      "9639/9639 [==============================] - 5s 481us/step\n",
      "TN:171,FP:0,FN:156,TP:8,Macc:0.524390210103,F1:0.0930222682622\n",
      "Epoch 27/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2444 - acc: 0.8891 - val_loss: 0.8066 - val_acc: 0.4870\n",
      "9639/9639 [==============================] - 5s 479us/step\n",
      "TN:171,FP:0,FN:160,TP:4,Macc:0.51219508897,F1:0.0476185300495\n",
      "Epoch 28/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2437 - acc: 0.8902 - val_loss: 0.7931 - val_acc: 0.5116\n",
      "9639/9639 [==============================] - 5s 480us/step\n",
      "TN:171,FP:0,FN:154,TP:10,Macc:0.530487770669,F1:0.114941322511\n",
      "Epoch 29/30\n",
      "1201/1201 [==============================] - 56s 46ms/step - loss: 0.2409 - acc: 0.8910 - val_loss: 1.1399 - val_acc: 0.4785\n",
      "9639/9639 [==============================] - 5s 486us/step\n",
      "TN:171,FP:0,FN:162,TP:2,Macc:0.506097528403,F1:0.0240961204834\n",
      "Epoch 30/30\n",
      "1201/1201 [==============================] - 56s 47ms/step - loss: 0.2412 - acc: 0.8919 - val_loss: 0.8600 - val_acc: 0.4899\n",
      "9639/9639 [==============================] - 5s 480us/step\n",
      "TN:171,FP:0,FN:159,TP:5,Macc:0.515243869253,F1:0.0591709583053\n",
      "Loss: 0.853891\n",
      "Iteration No: 96 ended. Search finished for the next optimal point.\n",
      "Time taken: 4072.7037\n",
      "Function value obtained: 0.8539\n",
      "Current minimum: 0.3328\n",
      "Iteration No: 97 started. Searching for the next optimal point.\n",
      "args [1, 3, 16]\n",
      "Epoch 1/30\n",
      "1201/1201 [==============================] - 204s 170ms/step - loss: 0.7738 - acc: 0.8501 - val_loss: 0.5589 - val_acc: 0.8110\n",
      "9639/9639 [==============================] - 52s 5ms/step\n",
      "TN:111,FP:60,FN:13,TP:151,Macc:0.784927205411,F1:0.80532787252\n",
      "Epoch 2/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.3144 - acc: 0.8687 - val_loss: 0.6341 - val_acc: 0.6676\n",
      "9639/9639 [==============================] - 5s 530us/step\n",
      "TN:61,FP:110,FN:4,TP:160,Macc:0.666167406956,F1:0.737321979692\n",
      "Epoch 3/30\n",
      "1201/1201 [==============================] - 57s 47ms/step - loss: 0.2925 - acc: 0.8764 - val_loss: 0.4875 - val_acc: 0.8074\n",
      "9639/9639 [==============================] - 5s 518us/step\n",
      "TN:112,FP:59,FN:11,TP:153,Macc:0.793948742398,F1:0.813824329286\n",
      "Epoch 4/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2796 - acc: 0.8830 - val_loss: 0.4677 - val_acc: 0.8137\n",
      "9639/9639 [==============================] - 5s 532us/step\n",
      "TN:122,FP:49,FN:16,TP:148,Macc:0.807944605182,F1:0.819939094365\n",
      "Epoch 5/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2708 - acc: 0.8859 - val_loss: 0.4972 - val_acc: 0.7849\n",
      "9639/9639 [==============================] - 5s 519us/step\n",
      "TN:106,FP:65,FN:12,TP:152,Macc:0.773356103594,F1:0.797894822861\n",
      "Epoch 6/30\n",
      "1201/1201 [==============================] - 57s 48ms/step - loss: 0.2621 - acc: 0.8888 - val_loss: 0.4580 - val_acc: 0.8123\n",
      "9639/9639 [==============================] - 5s 524us/step\n",
      "TN:111,FP:60,FN:5,TP:159,Macc:0.809317447677,F1:0.830281772029\n",
      "Epoch 7/30\n",
      "1201/1201 [==============================] - 60s 50ms/step - loss: 0.2579 - acc: 0.8906 - val_loss: 0.4746 - val_acc: 0.7931\n",
      "9639/9639 [==============================] - 5s 539us/step\n",
      "TN:98,FP:73,FN:2,TP:162,Macc:0.780452095066,F1:0.812024704603\n",
      "Epoch 8/30\n",
      " 340/1201 [=======>......................] - ETA: 39s - loss: 0.2557 - acc: 0.8931"
     ]
    }
   ],
   "source": [
    "from skopt.space import Integer\n",
    "from skopt import gp_minimize\n",
    "\n",
    "space = [Integer(1,5,name=\"depth\"),\n",
    "         Integer(1,4,name=\"n_blocks\"),\n",
    "         Integer(1,16,name=\"growth\"),\n",
    "        ]\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 4, 3],\n",
       " [4, 3, 5],\n",
       " [4, 3, 5],\n",
       " [4, 3, 5],\n",
       " [4, 3, 5],\n",
       " [4, 3, 5],\n",
       " [4, 3, 5],\n",
       " [4, 3, 5],\n",
       " [4, 3, 5],\n",
       " [4, 3, 5],\n",
       " [1, 1, 16],\n",
       " [1, 1, 16],\n",
       " [1, 1, 16],\n",
       " [5, 4, 16],\n",
       " [1, 4, 1],\n",
       " [5, 1, 1],\n",
       " [5, 1, 1],\n",
       " [5, 1, 1],\n",
       " [1, 1, 16],\n",
       " [1, 1, 16],\n",
       " [1, 1, 16],\n",
       " [1, 1, 16],\n",
       " [1, 2, 16],\n",
       " [5, 1, 1],\n",
       " [5, 1, 1],\n",
       " [5, 1, 1],\n",
       " [1, 1, 16],\n",
       " [5, 1, 1],\n",
       " [1, 1, 16],\n",
       " [5, 1, 1],\n",
       " [1, 1, 16],\n",
       " [5, 1, 1],\n",
       " [1, 1, 16],\n",
       " [1, 1, 16],\n",
       " [5, 1, 1],\n",
       " [5, 1, 1],\n",
       " [5, 1, 1],\n",
       " [1, 1, 16],\n",
       " [1, 1, 16],\n",
       " [5, 1, 1],\n",
       " [1, 1, 16],\n",
       " [1, 1, 16],\n",
       " [5, 1, 1],\n",
       " [1, 1, 16],\n",
       " [5, 1, 1],\n",
       " [1, 1, 16],\n",
       " [5, 1, 1],\n",
       " [5, 1, 16],\n",
       " [4, 1, 1],\n",
       " [3, 1, 1],\n",
       " [3, 1, 1],\n",
       " [2, 1, 1],\n",
       " [3, 1, 1],\n",
       " [3, 1, 1],\n",
       " [1, 1, 7],\n",
       " [5, 2, 1],\n",
       " [1, 3, 16],\n",
       " [1, 3, 1],\n",
       " [5, 3, 16],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1],\n",
       " [1, 1, 10],\n",
       " [3, 1, 1],\n",
       " [3, 1, 1],\n",
       " [1, 1, 10],\n",
       " [4, 1, 1],\n",
       " [1, 1, 9],\n",
       " [4, 1, 1],\n",
       " [3, 1, 1],\n",
       " [4, 1, 1],\n",
       " [3, 1, 1],\n",
       " [4, 1, 1],\n",
       " [3, 1, 1],\n",
       " [3, 1, 1],\n",
       " [3, 1, 1],\n",
       " [1, 1, 1],\n",
       " [4, 1, 1],\n",
       " [4, 1, 1],\n",
       " [2, 1, 1],\n",
       " [3, 1, 1],\n",
       " [2, 1, 1],\n",
       " [1, 3, 16],\n",
       " [3, 1, 1],\n",
       " [3, 1, 1],\n",
       " [4, 1, 1],\n",
       " [2, 1, 1],\n",
       " [3, 1, 1],\n",
       " [5, 3, 1],\n",
       " [5, 3, 1],\n",
       " [5, 3, 1],\n",
       " [1, 1, 1],\n",
       " [3, 1, 1],\n",
       " [4, 1, 1],\n",
       " [3, 1, 1],\n",
       " [1, 1, 11],\n",
       " [1, 3, 16],\n",
       " [4, 1, 1],\n",
       " [4, 1, 1],\n",
       " [1, 1, 1]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.plot(res_gp.x_iters,res_gp.func_vals)\n",
    "# plt.show()\n",
    "res_gp.x_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {'acq_func': 'gp_hedge',\n",
       "  'acq_optimizer': 'auto',\n",
       "  'base_estimator': GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
       "               kernel=1**2 * Matern(length_scale=[1, 1, 1], nu=2.5),\n",
       "               n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "               optimizer='fmin_l_bfgs_b', random_state=1791095845),\n",
       "  'callback': None,\n",
       "  'dimensions': Space([Integer(low=1, high=5),\n",
       "         Integer(low=1, high=4),\n",
       "         Integer(low=1, high=16)]),\n",
       "  'func': <function __main__.objective>,\n",
       "  'kappa': 1.96,\n",
       "  'n_calls': 100,\n",
       "  'n_jobs': 1,\n",
       "  'n_points': 10000,\n",
       "  'n_random_starts': 10,\n",
       "  'n_restarts_optimizer': 5,\n",
       "  'random_state': <mtrand.RandomState at 0x7fad0008b460>,\n",
       "  'verbose': True,\n",
       "  'x0': None,\n",
       "  'xi': 0.01,\n",
       "  'y0': None},\n",
       " 'function': 'base_minimize'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_gp.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/ma/core.py:6442: MaskedArrayFutureWarning: In the future the default for ma.maximum.reduce will be axis=0, not the current None, to match np.maximum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n",
      "/usr/local/lib/python2.7/dist-packages/numpy/ma/core.py:6442: MaskedArrayFutureWarning: In the future the default for ma.minimum.reduce will be axis=0, not the current None, to match np.minimum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa659c93250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAHfCAYAAABjz7DuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9v8BaxHASByVqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa659cba710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skopt.plots import plot_convergence, plot_evaluations, plot_objective\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plot_objective(res_gp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAHfCAYAAAARANJYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa659821350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig = plt.figure(figsize=(15,8))\n",
    "plot_evaluations(res_gp)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
