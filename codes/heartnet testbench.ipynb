{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from __future__ import print_function, absolute_import, division\n",
    "import numpy as np\n",
    "import tables\n",
    "from heartnet_v1 import reshape_folds\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.models import model_from_json\n",
    "from custom_layers import Conv1D_linearphaseType, Conv1D_linearphase, DCT1D, \\\n",
    "            Conv1D_gammatone, Conv1D_linearphaseType_legacy, Conv1D_zerophase\n",
    "    \n",
    "from sklearn.metrics import f1_score,confusion_matrix, roc_auc_score, roc_curve\n",
    "from scipy import signal\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "sns.set()\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, model_inputs, batch_size=64,print_shape_only=True, layer_name=None):\n",
    "    '''\n",
    "    Get activations from a specific layer of a trained model\n",
    "    '''\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "\n",
    "    model_multi_inputs_cond = True\n",
    "    if not isinstance(inp, list):\n",
    "        # only one input! let's wrap it in a list.\n",
    "        inp = [inp]\n",
    "        model_multi_inputs_cond = False\n",
    "\n",
    "    outputs = [layer.output for layer in model.layers if\n",
    "               layer.name == layer_name or layer_name is None]  # all layer outputs\n",
    "\n",
    "    funcs = [K.function(inp + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    \n",
    "    start_idx = 0\n",
    "    for idx in range(batch_size,len(model_inputs),batch_size):\n",
    "        print(batch_size)\n",
    "        if model_multi_inputs_cond:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            list_inputs = [model_inputs[start_idx:idx], 0.]\n",
    "\n",
    "        # Learning phase. 0 = Test mode (no dropout or batch normalization)\n",
    "        # layer_outputs = [func([model_inputs, 0.])[0] for func in funcs]\n",
    "        layer_outputs = [func(list_inputs)[0] for func in funcs]\n",
    "        for layer_activations in layer_outputs:\n",
    "            activations.append(layer_activations)\n",
    "        start_idx = idx\n",
    "    return np.vstack(activations)\n",
    "\n",
    "\n",
    "def display_activations(activation_maps):\n",
    "    '''\n",
    "    Plot activations\n",
    "    '''\n",
    "    batch_size = activation_maps[0].shape[0]\n",
    "    assert batch_size == 1, 'One image at a time to visualize.'\n",
    "    for i, activation_map in enumerate(activation_maps):\n",
    "        print('Displaying activation map {}'.format(i))\n",
    "        shape = activation_map.shape\n",
    "        if len(shape) == 4:\n",
    "            activations = np.hstack(np.transpose(activation_map[0], (2, 0, 1)))\n",
    "        elif len(shape) == 2:\n",
    "            # try to make it square as much as possible. we can skip some activations.\n",
    "            activations = activation_map[0]\n",
    "            num_activations = len(activations)\n",
    "            if num_activations > 1024:  # too hard to display it on the screen.\n",
    "                square_param = int(np.floor(np.sqrt(num_activations)))\n",
    "                activations = activations[0: square_param * square_param]\n",
    "                activations = np.reshape(activations, (square_param, square_param))\n",
    "            else:\n",
    "                activations = np.expand_dims(activations, axis=0)\n",
    "        else:\n",
    "            raise Exception('len(shape) = 3 has not been implemented.')\n",
    "        plt.imshow(activations, interpolation='None', cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "def smooth(scalars, weight):  # Weight between 0 and 1\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "\n",
    "    return np.asarray(smoothed)\n",
    "    \n",
    "def get_weights(log_name,min_metric=.7,min_epoch=50,verbose=1,log_dir='/media/taufiq/Data1/heart_sound/logs'):\n",
    "    '''\n",
    "    Load weights from training.csv file\n",
    "    '''\n",
    "#     log_dir = '/media/taufiq/Data1/heart_sound/logs'\n",
    "    \n",
    "    if not os.path.isdir(os.path.join(log_dir,log_name)):\n",
    "        log_dir = '/media/taufiq/Data1/heart_sound/logArxiv'\n",
    "    training_csv = os.path.join(log_dir,log_name,\"training.csv\")\n",
    "    df = pd.read_csv(training_csv)\n",
    "    sens_idx = df['val_sensitivity'][df.epoch>min_epoch][df.val_specificity>min_metric].idxmax()\n",
    "    spec_idx = df['val_specificity'][df.epoch>min_epoch][df.val_sensitivity>min_metric].idxmax()\n",
    "    macc_idx = df['val_macc'][df.epoch>min_epoch].idxmax()\n",
    "    val_idx = df['val_acc'][df.epoch>min_epoch].idxmax()\n",
    "    weights = dict()\n",
    "    weights['val_sensitivity'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[sens_idx]+1,df.val_acc.iloc[sens_idx])\n",
    "    weights['val_specificity'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[spec_idx]+1,df.val_acc.iloc[spec_idx])\n",
    "    weights['val_macc'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[macc_idx]+1,df.val_acc.iloc[macc_idx])\n",
    "    weights['val_acc'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[val_idx]+1,df.val_acc.iloc[val_idx])\n",
    "    weights['epoch'] = [\"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[idx]+1,df.val_acc.iloc[idx]) \n",
    "                        for idx in range(df.count()[0])]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Best Sensitivity model: {} \\t\\t{}\".format(df.val_sensitivity.iloc[sens_idx],weights['val_sensitivity']))\n",
    "        print(\"Best Specificity model: {} \\t\\t{}\".format(df.val_specificity.iloc[spec_idx],weights['val_specificity']))\n",
    "        print(\"Best Macc model: {} \\t\\t{}\".format(df.val_macc.iloc[macc_idx],weights['val_macc']))\n",
    "        print(\"Best Val model: {} \\t\\t\\t{}\".format(df.val_acc.iloc[val_idx],weights['val_acc']))\n",
    "    return weights\n",
    "\n",
    "      \n",
    "def load_data(foldname,fold_dir=None,_categorical=True,quality=False):\n",
    "    ## import data\n",
    "    if fold_dir is None:\n",
    "        fold_dir = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/folds_dec_2018/'\n",
    "    \n",
    "    feat = tables.open_file(fold_dir + foldname + '.mat')\n",
    "    x_train = feat.root.trainX[:]\n",
    "    y_train = feat.root.trainY[0, :]\n",
    "    q_train = feat.root.trainY[1, :]\n",
    "    x_val = feat.root.valX[:]\n",
    "    y_val = feat.root.valY[0, :]\n",
    "    q_val = feat.root.valY[1, :]\n",
    "    train_parts = feat.root.train_parts[:]\n",
    "    val_parts = feat.root.val_parts[0, :]\n",
    "\n",
    "    ############## Relabeling ################\n",
    "    \n",
    "    for i in range(0, y_train.shape[0]):\n",
    "        if y_train[i] == -1:\n",
    "            y_train[i] = 0  ## Label 0 for normal 1 for abnormal\n",
    "    for i in range(0, y_val.shape[0]):\n",
    "        if y_val[i] == -1:\n",
    "            y_val[i] = 0\n",
    "\n",
    "    ############# Parse Database names ########\n",
    "\n",
    "    train_files = []\n",
    "    for each in feat.root.train_files[:][0]:\n",
    "        train_files.append(chr(each))\n",
    "    print(len(train_files))\n",
    "    val_files = []\n",
    "    for each in feat.root.val_files[:][0]:\n",
    "        val_files.append(chr(each))\n",
    "    print(len(val_files))\n",
    "\n",
    "    ################### Reshaping ############\n",
    "\n",
    "    x_train, y_train, x_val, y_val = reshape_folds(x_train, x_val, y_train, y_val)\n",
    "\n",
    "    if _categorical:\n",
    "        y_train = to_categorical(y_train, num_classes=2)\n",
    "        y_val = to_categorical(y_val, num_classes=2)\n",
    "    \n",
    "    if quality:\n",
    "        return x_train, y_train, train_files, train_parts, q_train, \\\n",
    "                x_val, y_val, val_files, val_parts, q_val\n",
    "    else:\n",
    "        return x_train, y_train, train_files, train_parts, \\\n",
    "                x_val, y_val, val_files, val_parts\n",
    "\n",
    "def load_model(log_name,verbose=0,\n",
    "               model_dir='/media/taufiq/Data1/heart_sound/models/',\n",
    "               log_dir='/media/taufiq/Data1/heart_sound/logs/'):\n",
    "    \n",
    "#     model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "#     log_dir = '/media/taufiq/Data1/heart_sound/logs/'\n",
    "\n",
    "    if os.path.isdir(model_dir+log_name):\n",
    "        print(\"Model directory found\")\n",
    "        if os.path.isfile(os.path.join(model_dir+log_name,\"model.json\")):\n",
    "            print(\"model.json found. Importing\")\n",
    "        else:\n",
    "            raise ImportError(\"model.json not found\")\n",
    "\n",
    "    with open(os.path.join(model_dir+log_name,\"model.json\")) as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    try:\n",
    "        model = model_from_json(loaded_model_json,{'Conv1D_linearphase':Conv1D_linearphase,\n",
    "                                               'DCT1D':DCT1D,\n",
    "                                               'Conv1D_linearphaseType':Conv1D_linearphaseType,\n",
    "                                               'Conv1D_gammatone' : Conv1D_gammatone,\n",
    "                                                'Conv1D_zerophase' : Conv1D_zerophase,\n",
    "                                              })\n",
    "    except:\n",
    "        model = model_from_json(loaded_model_json,{'Conv1D_linearphase':Conv1D_linearphase,\n",
    "                                               'DCT1D':DCT1D,\n",
    "                                               'Conv1D_linearphaseType':Conv1D_linearphaseType_legacy,\n",
    "                                               'Conv1D_gammatone' : Conv1D_gammatone,\n",
    "                                                   'Conv1D_zerophase' : Conv1D_zerophase,\n",
    "                                              })\n",
    "    if verbose:\n",
    "        print(log_name)\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "def cc2parts(cc,parts):\n",
    "    \n",
    "    if not len(cc) == sum(parts):\n",
    "        raise ValueError('Number of CC elements are not equal to total number of parts')\n",
    "    \n",
    "    labels = []\n",
    "    start_idx = 0\n",
    "#     cc = np.round(cc)\n",
    "    \n",
    "    for s in parts:\n",
    "        if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "            continue\n",
    "        temp = cc[start_idx:start_idx + int(s)]\n",
    "        try:\n",
    "            labels.append(sum(temp)/len(temp))\n",
    "        except TypeError: ## TypeError for string input in train_files\n",
    "            labels.append(cc[start_idx])\n",
    "        start_idx = start_idx + int(s)\n",
    "    return np.asarray(labels)\n",
    "\n",
    "def parts2cc(partitioned,parts):\n",
    "    \n",
    "    labels = []\n",
    "    parts = parts[np.nonzero(parts)]\n",
    "    for each,part in zip(partitioned,parts):\n",
    "            labels += list(np.repeat(each,part))\n",
    "    return np.asarray(labels)\n",
    "\n",
    "def predict_parts(model,data,labels,parts,filenames=None,verbose=1,soft=False):\n",
    "    y_pred = model.predict(data, verbose=verbose)\n",
    "    true = []\n",
    "    pred = []\n",
    "    files= []\n",
    "    start_idx = 0\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    y_val = np.transpose(np.argmax(labels, axis=-1))\n",
    "    for s in parts:\n",
    "        if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "            continue\n",
    "        # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "        temp_ = y_val[start_idx:start_idx + int(s)]\n",
    "        temp = y_pred[start_idx:start_idx + int(s)]\n",
    "        if (sum(temp == 0) > sum(temp == 1)):\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "\n",
    "        if (sum(temp_ == 0) > sum(temp_ == 1)):\n",
    "            true.append(0)\n",
    "        else:\n",
    "            true.append(1)\n",
    "\n",
    "        if filenames is not None:\n",
    "            files.append(filenames[start_idx])\n",
    "        start_idx = start_idx + int(s)\n",
    "    \n",
    "    if soft:\n",
    "        pred = cc2parts(y_pred,parts)\n",
    "    return pred,true,files\n",
    "\n",
    "def eerPred(true,pred,verbose=1):\n",
    "    if pred.ndim > 1:\n",
    "            pred = pred[:,-1]\n",
    "    fpr,tpr,thresh = roc_curve(true,pred)\n",
    "    diff = abs(tpr-(1-fpr))\n",
    "    pred = pred > thresh[np.where(diff == min(diff))[0]]\n",
    "    if verbose:\n",
    "        print('Threshold selected as %f'%thresh[np.where(diff == min(diff))[0]])\n",
    "    return pred\n",
    "\n",
    "def calc_metrics(true,pred,files=None,verbose=1,eps=1E-10,thresh=.5):\n",
    "        if thresh=='EER':\n",
    "            TN, FP, FN, TP = confusion_matrix(true, eerPred(true,pred), labels=[0,1]).ravel()\n",
    "        else:\n",
    "            TN, FP, FN, TP = confusion_matrix(true, np.asarray(pred) > thresh, labels=[0,1]).ravel()\n",
    "        sensitivity = TP / (TP + FN + eps)\n",
    "        specificity = TN / (TN + FP + eps)\n",
    "        precision = TP / (TP + FP + eps)\n",
    "        F1 = 2 * (precision * sensitivity) / (precision + sensitivity + eps)\n",
    "        Macc = (sensitivity + specificity) / 2\n",
    "        MCC = (TP*TN-FP*FN)/((TP+FP)*(FN+TN)*(FP+TN)*(TP+FN))**.5\n",
    "        auc = roc_auc_score(true,pred)\n",
    "        logs = dict()\n",
    "        logs['val_sensitivity'] = np.array(sensitivity)\n",
    "        logs['val_specificity'] = np.array(specificity)\n",
    "        logs['val_precision'] = np.array(precision)\n",
    "        logs['val_F1'] = np.array(F1)\n",
    "        logs['val_macc'] = np.array(Macc)\n",
    "        logs['auc'] = np.array(auc)\n",
    "        logs['val_mcc'] = np.array(MCC).astype(np.float64)\n",
    "        if verbose:\n",
    "            print(\"TN:{},FP:{},FN:{},TP:{},Macc:{},F1:{}\".format(TN, FP, FN, TP,Macc,F1))\n",
    "        if files is not None:\n",
    "            true = np.asarray(true)\n",
    "            pred = np.asarray(pred)\n",
    "            files = np.asarray(files)\n",
    "            tpn = true == pred\n",
    "            avg = 0\n",
    "            for dataset in set(files):\n",
    "                mask = files == dataset\n",
    "                avg = avg + np.sum(tpn[mask])/np.sum(mask)/len(set(files))\n",
    "                logs['acc_'+dataset] = np.sum(tpn[mask])/np.sum(mask)\n",
    "            logs['acc_avg'] = avg\n",
    "        df = pd.Series(logs)\n",
    "        return df\n",
    "\n",
    "\n",
    "def log_fusion(logs,data,labels,fusion_weights=None,min_epoch=20,min_metric=.7,\n",
    "               metric='val_macc',model_dir='/media/taufiq/Data1/heart_sound/models/',verbose=0):        \n",
    "    '''\n",
    "    Returns fused predictions\n",
    "    '''\n",
    "    if not type(logs) == list:\n",
    "        logs = [logs]\n",
    "    \n",
    "    if fusion_weights is None:\n",
    "        fusion_weights = np.ones((len(logs)))\n",
    "    else:\n",
    "        if not len(logs)==len(fusion_weights):\n",
    "            raise ValueError('Fusion weights not consistent with number of models')\n",
    "    pred = np.zeros((data.shape[0],2))\n",
    "    \n",
    "    for log_name,weight in zip(logs,fusion_weights):\n",
    "        model = load_model(log_name,verbose=verbose)\n",
    "        weights = get_weights(log_name,min_epoch=min_epoch,\n",
    "                              min_metric=min_metric,verbose=verbose)\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "        pred += model.predict(data,verbose=verbose)*weight\n",
    "    pred /= sum(fusion_weights)\n",
    "    # pred = np.argmax(pred,axis=-1)\n",
    "    return pred\n",
    "\n",
    "def model_confidence(model,data,labels,verbose=0):\n",
    "    '''\n",
    "    Give confidence score for true class\n",
    "    '''\n",
    "    pred = model.predict(data,verbose=verbose)\n",
    "    \n",
    "    if np.asarray(labels).ndim >1:\n",
    "        labels = np.argmax(labels,axis=-1)\n",
    "    \n",
    "    pred = [pred[idx,each] for idx,each in enumerate(labels)]\n",
    "    \n",
    "    return np.asarray(pred)\n",
    "\n",
    "def plot_coeff(logs,branches=[1,2,3,4],min_epoch=20,min_metric=.7,\n",
    "             metric='val_macc',model_dir='/media/taufiq/Data1/heart_sound/models/',\n",
    "             figsize=(10,6),verbose=0):\n",
    "    '''\n",
    "    Plot Learnable FIRs for logs\n",
    "    '''\n",
    "    if not type(logs) == list:\n",
    "        logs = [logs]\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, ax = plt.subplots(len(branches), len(logs), sharex='col', sharey='row', figsize=figsize)\n",
    "    \n",
    "    for _idx,log_name in enumerate(logs):\n",
    "        model = load_model(log_name,verbose=verbose)\n",
    "        weights = get_weights(log_name,min_epoch=min_epoch,\n",
    "                              min_metric=min_metric,verbose=verbose)\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "        \n",
    "        FIR_coeff = []\n",
    "        layer_name = []\n",
    "        layer_type = []\n",
    "        \n",
    "        ## Get filter coefficients\n",
    "        for branch in branches:\n",
    "            if not 'gammatone' in model.layers[branch].name:\n",
    "                FIR_coeff.append(np.asarray(model.layers[branch].get_weights())[0,:,0,0])\n",
    "                layer_name.append(model.layers[branch].name)\n",
    "            else: # for gammatone\n",
    "                FIR_coeff.append(K.get_session().run(model.layers[branch].impulse_gammatone()))\n",
    "                layer_name.append(model.layers[branch].name)\n",
    "            try:\n",
    "                layer_type.append(model.layers[branch].FIR_type)\n",
    "            except: # if not linear phase\n",
    "                layer_type.append(0)\n",
    "        \n",
    "        for idx,coeff in enumerate(FIR_coeff):\n",
    "            \n",
    "            ## Flip-concat coefficients for Linearphase\n",
    "            if 'linearphase' in layer_name[idx]:\n",
    "                if layer_type[idx] == 1:\n",
    "                    FIR_coeff[idx] = np.concatenate([np.flip(FIR_coeff[idx][1:],axis=0),FIR_coeff[idx]])  \n",
    "                elif layer_type[idx] == 2:\n",
    "                    FIR_coeff[idx] = np.concatenate([np.flip(FIR_coeff[idx],axis=0),FIR_coeff[idx]])\n",
    "                elif layer_type[idx] == 3:\n",
    "                    FIR_coeff[idx] = np.concatenate([-1*np.flip(FIR_coeff[idx][1:],axis=0),FIR_coeff[idx]])  \n",
    "                else:\n",
    "                    FIR_coeff[idx] = np.concatenate([-1*np.flip(FIR_coeff[idx],axis=0),FIR_coeff[idx]])\n",
    "                    \n",
    "            \n",
    "            ax[idx,_idx].plot((FIR_coeff[idx]-np.mean(FIR_coeff[idx]))/np.std(FIR_coeff[idx]))\n",
    "    \n",
    "    plt.tight_layout()     \n",
    "    plt.show()\n",
    "    return ax\n",
    "    \n",
    "def plot_freq(logs,branches=[1,2,3,4],phase=False,min_epoch=20,min_metric=.7,\n",
    "             metric='val_macc',model_dir='/media/taufiq/Data1/heart_sound/models/',\n",
    "             figsize=(10,6),verbose=0):\n",
    "    '''\n",
    "    Plot Learnable FIRs for logs\n",
    "    '''\n",
    "    if not type(logs) == list:\n",
    "        logs = [logs]\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, ax = plt.subplots(len(branches), len(logs), sharex='col', sharey='row', figsize=figsize)\n",
    "    \n",
    "    for _idx,log_name in enumerate(logs):\n",
    "        model = load_model(log_name,verbose=verbose)\n",
    "        weights = get_weights(log_name,min_epoch=min_epoch,\n",
    "                              min_metric=min_metric,verbose=verbose)\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "        \n",
    "        FIR_coeff = []\n",
    "        layer_name = []\n",
    "        layer_type = []\n",
    "        \n",
    "        ## Get filter coefficients\n",
    "        for branch in branches:\n",
    "            if not 'gammatone' in model.layers[branch].name:\n",
    "                FIR_coeff.append(np.asarray(model.layers[branch].get_weights())[0,:,0,0])\n",
    "                layer_name.append(model.layers[branch].name)\n",
    "            else: # for gammatone\n",
    "                FIR_coeff.append(K.get_session().run(model.layers[branch].impulse_gammatone()))\n",
    "                layer_name.append(model.layers[branch].name)\n",
    "            try:\n",
    "                layer_type.append(model.layers[branch].FIR_type)\n",
    "            except: # if not linear phase\n",
    "                layer_type.append(0)\n",
    "        \n",
    "        for idx,coeff in enumerate(FIR_coeff):\n",
    "            \n",
    "            ## Flip-concat coefficients for Linearphase\n",
    "            if 'linearphase' in layer_name[idx]:\n",
    "                if layer_type[idx] == 1:\n",
    "                    FIR_coeff[idx] = np.concatenate([np.flip(FIR_coeff[idx][1:],axis=0),FIR_coeff[idx]])  \n",
    "                elif layer_type[idx] == 2:\n",
    "                    FIR_coeff[idx] = np.concatenate([np.flip(FIR_coeff[idx],axis=0),FIR_coeff[idx]])\n",
    "                elif layer_type[idx] == 3:\n",
    "                    FIR_coeff[idx] = np.concatenate([-1*np.flip(FIR_coeff[idx][1:],axis=0),FIR_coeff[idx]])  \n",
    "                else:\n",
    "                    FIR_coeff[idx] = np.concatenate([-1*np.flip(FIR_coeff[idx],axis=0),FIR_coeff[idx]])\n",
    "            \n",
    "            w,freq_res=signal.freqz(FIR_coeff[idx])\n",
    "            ax[idx,_idx].plot(w/np.pi*500,10*np.log10(abs(freq_res)/max(abs(freq_res))))\n",
    "            if phase:\n",
    "                angles = np.unwrap(np.angle(freq_res))\n",
    "                ax2 = ax[idx,_idx].twinx()\n",
    "                ax2.plot(w/np.pi*500, angles, 'g')\n",
    "\n",
    "    plt.tight_layout()     \n",
    "#     plt.show()\n",
    "    return ax\n",
    "\n",
    "def plot_metric(logs,metric='val_loss',smoothing=0.1,lognames=None,xlim=None,ylim=None,\n",
    "                figsize=(10,6),legendLoc=0,colors=None,ax=None):\n",
    "    '''\n",
    "    Plot specified metric for logs\n",
    "    smooth: smoothing factor for each plot \n",
    "    ''' \n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(figsize=figsize)\n",
    "    for idx,log in enumerate(logs):\n",
    "        log_dir='/media/taufiq/Data1/heart_sound/logs'\n",
    "        if not os.path.isdir(os.path.join(log_dir,log)):\n",
    "            log_dir = '/media/taufiq/Data1/heart_sound/logArxiv'          \n",
    "        training_csv = os.path.join(log_dir,log,\"training.csv\")\n",
    "        df = pd.read_csv(training_csv)\n",
    "        data = np.asarray(df[metric].values)\n",
    "            \n",
    "        if colors is not None:\n",
    "            ax.plot(smooth(data,smoothing),color=colors[idx])\n",
    "        else:\n",
    "            ax.plot(smooth(data,smoothing))\n",
    "\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "    if lognames is not None:\n",
    "        ax.legend(lognames,loc=legendLoc)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(metric)\n",
    "    \n",
    "    return ax\n",
    "    \n",
    "def plot_log_metrics(log,metrics=['acc_a','acc_e'],labels=None,smoothing=0.1,\n",
    "                     xlim=None,ylim=None,figsize=(10,6),legendLoc=0,colors=None,ax=None):\n",
    "    '''\n",
    "    Plot multiple metrics of the same log\n",
    "    '''\n",
    "    log_dir='/media/taufiq/Data1/heart_sound/logs'\n",
    "    if not os.path.isdir(os.path.join(log_dir,log)):\n",
    "        log_dir = '/media/taufiq/Data1/heart_sound/logArxiv'          \n",
    "    training_csv = os.path.join(log_dir,log,\"training.csv\")\n",
    "    df = pd.read_csv(training_csv)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(figsize=figsize)\n",
    "    for idx,metric in enumerate(metrics):\n",
    "        data = np.asarray(df[metric].values)\n",
    "        if colors is not None:\n",
    "            ax.plot(smooth(data,smoothing),color=colors[idx])\n",
    "        else:\n",
    "            ax.plot(smooth(data,smoothing))\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "#     if lognames is not None:\n",
    "#         ax.legend(metrics,loc=legendLoc)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    return ax\n",
    "\n",
    "def idx_parts2cc(partidx,parts):\n",
    "    \n",
    "    if type(partidx) == int:\n",
    "        partidx = [partidx]\n",
    "        \n",
    "    idx = []\n",
    "    for each in partidx:\n",
    "        start_idx = int(sum(parts[:each]))\n",
    "        end_idx = int(start_idx + parts[each])\n",
    "        idx = idx+range(start_idx,end_idx)\n",
    "    return idx\n",
    "\n",
    "def grad_cam(model,layer_name,data,label,scale=True,verbose=0):\n",
    "    \n",
    "    if data.ndim < 3:\n",
    "        data = np.expand_dims(data,axis=0)\n",
    "    output = model.output[:,1-int(label)]\n",
    "    last_conv_layer = model.get_layer(layer_name) ##### have to change the name here\n",
    "    grads = K.gradients(output, last_conv_layer.output)[0]\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1)) ### no idea what to do here\n",
    "    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "\n",
    "    pooled_grads_value, conv_layer_output_value = iterate([data])\n",
    "    for i in range(pooled_grads_value.shape[0]):\n",
    "        if verbose:\n",
    "            print(\"Iteration %d\" % i)\n",
    "        conv_layer_output_value[ :, i] *= pooled_grads_value[i]\n",
    "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    if scale:\n",
    "        x = np.linspace(0, data.shape[1], num=len(heatmap))\n",
    "        y = heatmap\n",
    "        f1 = interp1d(x, y, kind='cubic')\n",
    "        xnew = np.linspace(0, data.shape[1], num=data.shape[1])\n",
    "        ynew = f1(xnew)\n",
    "        return ynew\n",
    "    else:\n",
    "        return heatmap\n",
    "\n",
    "def cc2rec(data):\n",
    "    rec = []\n",
    "    for cc in data:\n",
    "        idx = np.where(cc!=0)[0]\n",
    "        cc = cc[:idx[-1],0]\n",
    "        rec.append(cc)\n",
    "    return np.asarray(np.hstack(rec))\n",
    "\n",
    "def cc2rec_labels(data,labels):\n",
    "    gt = []\n",
    "    for i,cc in enumerate(data):\n",
    "        idx = np.where(cc!=0)[0]\n",
    "        cctr = np.ones(idx[-1])*labels[i]\n",
    "        gt.append(cctr)    \n",
    "    return np.asarray(np.hstack(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_coeff([\n",
    "          \"potes_fold0_noFIR 2019-03-02 13:01:33.636778\",\n",
    "          \"fold0_noFIR 2019-02-24 18:02:57.053839\",\n",
    "#           \"fold0_noFIR 2019-02-27 19:52:21.543329\",\n",
    "#           \"fold2_noFIR 2019-01-17 04:16:51.868927\", # random\n",
    "#           \"fold1_noFIR 2019-01-13 15:04:39.094472\", \n",
    "#           \"fold1_noFIR 2019-02-16 12:28:19.127331\", # densenet\n",
    "#             \"fold0_noFIR 2019-03-06 14:21:29.823568\", # bi-conv stage1\n",
    "            \"fold0_noFIR 2019-03-06 21:42:10.719836\", # bi-conv stage2\n",
    "            \"fold0_noFIR 2019-03-09 01:34:03.547265\", #gamma stage 1\n",
    "#             \"fold0_noFIR 2019-03-09 07:12:26.773316\", #gamma stage 2\n",
    "            \"fold0_noFIR 2019-03-08 03:28:46.740442\", #type3\n",
    "            \"potes_fold0_noFIR 2019-03-16 18:44:45.597226\"\n",
    "         ],min_epoch=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283\n",
      "515\n",
      "(93942, 2500, 1)\n",
      "(93942, 1)\n",
      "(15511, 2500, 1)\n",
      "(15511, 1)\n",
      "79810\n",
      "6710\n",
      "(79810, 2500, 1)\n",
      "(79810, 1)\n",
      "(6710, 2500, 1)\n",
      "(6710, 1)\n"
     ]
    }
   ],
   "source": [
    "foldname = 'fold1+compare'\n",
    "fold_dir = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/'\n",
    "\n",
    "x_train, y_train, train_files,train_parts, q_train, \\\n",
    "    x_val, y_val,val_files,val_parts, q_val = load_data(foldname,fold_dir,quality=True)\n",
    "    \n",
    "test_parts = train_parts[0][np.asarray(train_files) =='x']\n",
    "test_parts = np.concatenate([test_parts,val_parts[np.asarray(val_files)=='x']],axis=0)\n",
    "train_files = parts2cc(train_files,train_parts[0])\n",
    "val_files = parts2cc(val_files,val_parts)\n",
    "x_test = x_train[train_files == 'x']\n",
    "x_test = np.concatenate([x_test,x_val[val_files=='x']])\n",
    "y_test = y_train[train_files == 'x']\n",
    "y_test = np.concatenate([y_test,y_val[val_files=='x']])\n",
    "test_files = np.concatenate([train_files[train_files == 'x'],\n",
    "                            val_files[val_files == 'x']])\n",
    "q_test = np.concatenate([q_train[train_files == 'x'],\n",
    "                            q_val[val_files == 'x']])\n",
    "del x_train, y_train, train_files,train_parts, q_train, \\\n",
    "    x_val, y_val,val_files,val_parts, q_val\n",
    "    \n",
    "    \n",
    "foldname = 'fold0_noFIR'\n",
    "x_train, y_train, train_files,train_parts, q_train, \\\n",
    "    x_val, y_val,val_files,val_parts, q_val = load_data(foldname,quality=True) # also return recording quality\n",
    "\n",
    "train_parts = train_parts[np.nonzero(train_parts)] ## Some have zero cardiac cycle\n",
    "val_parts = val_parts[np.nonzero(val_parts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test2Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Test partition\n",
      "(79810, 2500, 1) (22933, 2500, 1) (79810, 2) (22933, 2) (2832,) (682,)\n",
      "After Test partition with fraction 0.3\n",
      "(79810, 2500, 1) (16227, 2500, 1) (79810, 2) (16227, 2) (2832,) (478,)\n"
     ]
    }
   ],
   "source": [
    "frac=.3\n",
    "print('Before Test partition')\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape, train_parts.shape, test_parts.shape)\n",
    "\n",
    "test_files = np.repeat('g',len(test_files),axis=0)\n",
    "random_seed = 1\n",
    "np.random.seed(random_seed)\n",
    "part_idx = np.random.permutation(range(len(test_parts)))\n",
    "part_idx = part_idx[:int(len(test_parts) * frac)]\n",
    "cc_idx = idx_parts2cc(part_idx, test_parts)\n",
    "\n",
    "# train_parts = np.concatenate([train_parts, test_parts[part_idx]], axis=0)\n",
    "test_parts = np.delete(test_parts, part_idx, axis=0)\n",
    "# val_parts = np.concatenate([val_parts, test_parts], axis=0)\n",
    "\n",
    "# train_files = np.concatenate([train_files, test_files[cc_idx]], axis=0)\n",
    "test_files = np.delete(test_files, cc_idx, axis=0)\n",
    "# val_files = np.concatenate([val_files, test_files], axis=0)\n",
    "\n",
    "# x_train = np.concatenate([x_train, x_test[cc_idx]], axis=0)\n",
    "x_test = np.delete(x_test, cc_idx, axis=0)\n",
    "# x_val = np.concatenate([x_val, x_test], axis=0)\n",
    "\n",
    "# y_train = np.concatenate([y_train, y_test[cc_idx]], axis=0)\n",
    "y_test = np.delete(y_test, cc_idx, axis=0)\n",
    "# y_val = np.concatenate([y_val, y_test], axis=0)\n",
    "\n",
    "print('After Test partition with fraction', frac)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape, train_parts.shape, test_parts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory found\n",
      "model.json found. Importing\n",
      "fold0_noFIR 2019-03-09 01:34:03.547265\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_gammatone_1 (Conv1D_gamm (None, 2500, 1)      4           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_gammatone_2 (Conv1D_gamm (None, 2500, 1)      4           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_gammatone_3 (Conv1D_gamm (None, 2500, 1)      4           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_gammatone_4 (Conv1D_gamm (None, 2500, 1)      4           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2496, 8)      40          conv1d_gammatone_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2496, 8)      40          conv1d_gammatone_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 2496, 8)      40          conv1d_gammatone_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 2496, 8)      40          conv1d_gammatone_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 2496, 8)      32          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 2496, 8)      32          conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 2496, 8)      32          conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 2496, 8)      32          conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 2496, 8)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 2496, 8)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 2496, 8)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 2496, 8)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2496, 8)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2496, 8)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2496, 8)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 2496, 8)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1248, 8)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1248, 8)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1248, 8)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1248, 8)      0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1244, 4)      160         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1244, 4)      160         max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1244, 4)      160         max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 1244, 4)      160         max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1244, 4)      16          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1244, 4)      16          conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1244, 4)      16          conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1244, 4)      16          conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1244, 4)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1244, 4)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1244, 4)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1244, 4)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1244, 4)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1244, 4)      0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1244, 4)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1244, 4)      0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 622, 4)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 622, 4)       0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 622, 4)       0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 622, 4)       0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 622, 16)      0           max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9952)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           199040      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            42          dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 200,090\n",
      "Trainable params: 199,994\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n",
      "Best Sensitivity model: 0.920289781716 \t\tweights.0069-0.7216.hdf5\n",
      "Best Specificity model: 0.760273915322 \t\tweights.0145-0.7449.hdf5\n",
      "Best Macc model: 0.794570118883 \t\tweights.0269-0.7440.hdf5\n",
      "Best Val model: 0.752011923428 \t\t\tweights.0206-0.7520.hdf5\n"
     ]
    }
   ],
   "source": [
    "# log_name = \"fold0_noFIR 2019-03-09 01:34:03.547265\"\n",
    "# log_name = \"fold0_noFIR 2019-02-24 18:02:57.053839\" # Type1 macc\n",
    "# log_name = \"potes_fold0_noFIR 2019-03-02 13:01:33.636778\" # potes\n",
    "log_name = \"fold0_noFIR 2019-03-09 01:34:03.547265\" #gamma stage 1\n",
    "# log_name = \"fold0_noFIR 2019-03-07 14:44:47.022240\" # Type2 macc 80 epoch\n",
    "# log_name = \"fold0_noFIR 2019-03-08 03:28:46.740442\" # Type3 sensitivity/spec for balanced\n",
    "# log_name = \"fold0_noFIR 2019-03-08 14:50:52.332924\" # type4 val_acc\n",
    "# log_name = \"fold0_noFIR 2019-03-06 21:42:10.719836\" #zero stage2\n",
    "# log_name = \"potes_fold0_noFIR 2019-03-16 18:44:45.597226\" #potes non balanced\n",
    "\n",
    "### Trained with both train and test\n",
    "\n",
    "# log_name = \"fold0_noFIR 2019-03-24 18:55:14.833080\" #frac=.1\n",
    "# log_name = \"fold0_noFIR 2019-03-24 23:14:47.400720\" #frac=.2\n",
    "# log_name = 'fold0_noFIR 2019-03-25 03:34:29.171850' #frac=.3\n",
    "\n",
    "### Fine tuned with test\n",
    "\n",
    "# log_name = \"fold0_noFIR 2019-04-16 14:48:05.398094\"\n",
    "\n",
    "model = load_model(log_name,verbose=1)\n",
    "weights = get_weights(log_name,min_epoch=0,min_metric=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded:\n",
      " /media/taufiq/Data1/heart_sound/models/fold0_noFIR 2019-03-09 01:34:03.547265/weights.0269-0.7440.hdf5\n"
     ]
    }
   ],
   "source": [
    "metric = 'val_macc'\n",
    "model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "\n",
    "checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "model.load_weights(checkpoint_name)\n",
    "print(\"Checkpoint loaded:\\n %s\" % checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded:\n",
      " /media/taufiq/Data1/heart_sound/models/fold0_noFIR 2019-04-16 14:48:05.398094/weights.0151-0.7191.hdf5\n"
     ]
    }
   ],
   "source": [
    "epoch_num = -1\n",
    "model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "\n",
    "checkpoint_name = os.path.join(model_dir+log_name,weights['epoch'][epoch_num])\n",
    "model.load_weights(checkpoint_name)\n",
    "print(\"Checkpoint loaded:\\n %s\" % checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6710/6710 [==============================] - 12s 2ms/step\n",
      "TN:106,FP:40,FN:19,TP:119,Macc:0.794173118919,F1:0.801346801297\n",
      "acc_a                          0.1375\n",
      "acc_avg                       0.28207\n",
      "acc_b                        0.193878\n",
      "acc_c                        0.285714\n",
      "acc_d                             0.4\n",
      "acc_e                        0.393258\n",
      "auc                0.7980196545562835\n",
      "val_F1             0.8013468012965118\n",
      "val_macc            0.794173118919431\n",
      "val_mcc            0.5923731742131004\n",
      "val_precision      0.7484276729555042\n",
      "val_sensitivity    0.8623188405790854\n",
      "val_specificity    0.7260273972597767\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Calculating metrics for test\n",
      "16227/16227 [==============================] - 28s 2ms/step\n",
      "TN:46,FP:39,FN:167,TP:226,Macc:0.55812004191,F1:0.686930091137\n",
      "acc_avg                     0.0543933\n",
      "acc_g                       0.0543933\n",
      "auc                0.5662775033677594\n",
      "val_F1             0.6869300911370936\n",
      "val_macc           0.5581200419095023\n",
      "val_mcc             0.089422933728786\n",
      "val_precision      0.8528301886789235\n",
      "val_sensitivity    0.5750636132314059\n",
      "val_specificity    0.5411764705875987\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print('Calculating metrics for Training set')\n",
    "# pred,true,files = predict_parts(model,x_train,y_train,train_parts,train_files)\n",
    "# res = calc_metrics(true,pred,files)\n",
    "# print(res)\n",
    "\n",
    "\n",
    "##### print('Calculating metrics for all of Validation')\n",
    "pred,true,files = predict_parts(model,x_val,y_val,val_parts,val_files,soft=True)\n",
    "res = calc_metrics(true,pred,files)\n",
    "print(res)\n",
    "\n",
    "# print('\\n\\nCalculating metrics for good quality only')\n",
    "# pred,true,files = predict_parts(model,\n",
    "#                                 x_val[q_val>0],y_val[q_val>0],\n",
    "#                                 val_parts[cc2parts(q_val,val_parts)>0],\n",
    "#                                 np.asarray(val_files)[q_val>0])\n",
    "# res = calc_metrics(true,pred,files)\n",
    "# print(res)\n",
    "\n",
    "\n",
    "print('\\n\\nCalculating metrics for test')\n",
    "pred,true,files = predict_parts(model,x_test,y_test,test_parts,test_files,soft=True)\n",
    "res = calc_metrics(true,pred,files)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EER\n",
    "# pred = model.predict(x_train)\n",
    "# pred = cc2parts(pred,train_parts)[:,1]\n",
    "# true = cc2parts(y_train,train_parts)[:,1]\n",
    "# files = cc2parts(train_files,train_parts)\n",
    "# res = calc_metrics(true,pred,files,thresh='EER')\n",
    "# print(res)\n",
    "\n",
    "\n",
    "# pred = model.predict(x_val)\n",
    "# pred = cc2parts(pred,val_parts)[:,1]\n",
    "# true = cc2parts(y_val,val_parts)[:,1]\n",
    "# files = cc2parts(val_files,val_parts)\n",
    "# res = calc_metrics(true,pred,files,thresh='EER')\n",
    "# print(res)\n",
    "\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "pred = cc2parts(pred,test_parts)[:,1]\n",
    "true = cc2parts(y_test,test_parts)[:,1]\n",
    "files = cc2parts(test_files,test_parts)\n",
    "res = calc_metrics(true,pred,files,thresh='EER')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calc_metrics(true,np.random.rand(len(true)),thresh='EER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_val)\n",
    "preds = cc2parts(preds[:,1],val_parts)\n",
    "true = cc2parts(y_val[:,1],val_parts)\n",
    "fpr,tpr,thresh = roc_curve(true,preds)\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr)\n",
    "diff = abs(tpr-(1-fpr))\n",
    "preds = preds > thresh[np.where(diff == min(diff))[0]]\n",
    "print(thresh[np.where(diff == min(diff))[0]])\n",
    "\n",
    "calc_metrics(true,preds,cc2parts(val_files,val_parts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)\n",
    "preds = cc2parts(preds[:,1],test_parts)\n",
    "true = cc2parts(y_test[:,1],test_parts)\n",
    "fpr,tpr,thresh = roc_curve(true,preds)\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr)\n",
    "diff = abs(tpr-(1-fpr))\n",
    "preds = preds > thresh[np.where(diff == min(diff))[0]]\n",
    "print(thresh[np.where(diff == min(diff))[0]])\n",
    "\n",
    "calc_metrics(true,preds,cc2parts(test_files,test_parts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=1,return_indices=True)\n",
    "_,y,partidx = rus.fit_resample(np.expand_dims(range(len(test_parts)),axis=-1),cc2parts(y_test[:,1],test_parts))\n",
    "ccidx= idx_parts2cc(partidx,test_parts)\n",
    "_parts = test_parts[partidx]\n",
    "x = x_test[ccidx]\n",
    "y = y_test[ccidx]\n",
    "_files = test_files[ccidx]\n",
    "\n",
    "pred,true,files = predict_parts(model,x,y,_parts,_files,soft=True)\n",
    "res = calc_metrics(true,pred,files,thresh='EER')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Fusion predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fusion Predict Val')\n",
    "model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "# fusion_weights = [.8,1.2,.8,1.2]\n",
    "fusion_weights = [1,1,.4,1]\n",
    "\n",
    "pred = np.zeros((x_val.shape[0],2))\n",
    "for metric,weight in zip(weights.keys(),fusion_weights):\n",
    "    checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "    model.load_weights(checkpoint_name)\n",
    "    pred += model.predict(x_val,verbose=1)*weight\n",
    "pred /= sum(fusion_weights)\n",
    "# pred = np.argmax(pred,axis=-1)\n",
    "pred = pred[:,1]\n",
    "res = calc_metrics(cc2parts(np.argmax(y_val,axis=-1),val_parts),np.round(cc2parts(pred,val_parts)))\n",
    "print(res)\n",
    "\n",
    "print('\\n\\nFusion Predict Test')\n",
    "pred = np.zeros((x_test.shape[0],2))\n",
    "for metric,weight in zip(weights.keys(),fusion_weights):\n",
    "    checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "    model.load_weights(checkpoint_name)\n",
    "    \n",
    "    pred += model.predict(x_test,verbose=1)*weight\n",
    "pred /= sum(fusion_weights)\n",
    "# pred = np.argmax(pred,axis=-1)\n",
    "pred = pred[:,1]\n",
    "res = calc_metrics(cc2parts(np.argmax(y_test,axis=-1),test_parts),np.round(cc2parts(pred,test_parts)))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fold model fusion predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory found\n",
      "model.json found. Importing\n",
      "Best Sensitivity model: 0.876811524312 \t\tweights.0269-0.7440.hdf5\n",
      "Best Specificity model: 0.760273915322 \t\tweights.0145-0.7449.hdf5\n",
      "Best Macc model: 0.794570118883 \t\tweights.0269-0.7440.hdf5\n",
      "Best Val model: 0.752011923428 \t\t\tweights.0206-0.7520.hdf5\n",
      "TN:62,FP:54,FN:241,TP:325,Macc:0.554343852808,F1:0.687830687783\n",
      "auc                0.5519221396368954\n",
      "val_F1             0.6878306877825001\n",
      "val_macc           0.5543438528082969\n",
      "val_mcc            0.0821820010938894\n",
      "val_precision      0.8575197889179795\n",
      "val_sensitivity    0.5742049469963649\n",
      "val_specificity    0.5344827586202289\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "logs=[\n",
    "(0,\"fold0_noFIR 2019-02-24 18:02:57.053839\",'val_macc',100,.7), # Type1 macc\n",
    "(1,\"fold0_noFIR 2019-03-09 01:34:03.547265\",'val_macc',100,.7), #gamma stage 1\n",
    "(0,\"fold0_noFIR 2019-03-07 14:44:47.022240\",'val_macc',80,.7), # Type2 macc 80 epoch\n",
    "(0,\"fold0_noFIR 2019-03-08 03:28:46.740442\",'val_sensitivity',100,.65), # Type3 sensitivity/spec for balanced\n",
    "(0,\"fold0_noFIR 2019-03-08 14:50:52.332924\",'val_acc',100,.7), # type4 val_acc\n",
    "(0,\"fold0_noFIR 2019-03-06 21:42:10.719836\",'val_macc',100,.7), #zero stage2\n",
    "]\n",
    "pred_fusion=0\n",
    "for weight,log,metric,epoch,min_metric in logs:\n",
    "    if not weight:\n",
    "        continue\n",
    "    model=load_model(log_name=log)\n",
    "    model_weights = get_weights(log_name=log,\n",
    "                                min_epoch=epoch,\n",
    "                                min_metric=min_metric)\n",
    "    model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "    checkpoint_name = os.path.join(model_dir+log,model_weights[metric])\n",
    "    model.load_weights(checkpoint_name)\n",
    "    pred = model.predict(x_test)\n",
    "    pred = cc2parts(pred,test_parts)\n",
    "    pred_fusion += weight*pred\n",
    "\n",
    "pred_fusion /= sum([each[0] for each in logs])\n",
    "# pred_fusion = cc2parts(pred_fusion,test_parts)\n",
    "print(calc_metrics(true=cc2parts(y_test,test_parts)[:,1],pred=pred_fusion[:,1],verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = [\n",
    "    \"fold0_noFIR 2019-02-24 18:02:57.053839\", #Type1\n",
    "#     \"fold1_noFIR 2019-02-23 17:59:17.240365\"\n",
    " \n",
    "           ]\n",
    "pred = log_fusion(logs,x_test,y_test,min_metric=.7,\n",
    "                  metric='val_specificity',verbose=0)\n",
    "pred = pred[:,1]\n",
    "res = calc_metrics(cc2parts(np.argmax(y_test,axis=-1),test_parts),\n",
    "                   np.round(cc2parts(pred,test_parts)))\n",
    "print(res.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logs=[\n",
    "\"potes_fold0_noFIR 2019-03-02 13:01:33.636778\", # potes\n",
    "\"fold0_noFIR 2019-02-24 18:02:57.053839\", # Type1 macc\n",
    "\"fold0_noFIR 2019-03-07 14:44:47.022240\", # Type2 macc 80 epoch\n",
    "\"fold0_noFIR 2019-03-08 03:28:46.740442\", # Type3 sensitivity\n",
    "\"fold0_noFIR 2019-03-08 14:50:52.332924\", # type4 val_acc\n",
    "\"fold0_noFIR 2019-03-09 01:34:03.547265\", # gamma stage 1\n",
    "\"fold0_noFIR 2019-03-06 21:42:10.719836\", # zero stage2\n",
    "]\n",
    "lognames=[\n",
    "\"Static FIR\",\n",
    "\"Type I tConv\",\n",
    "\"Type II tConv\",\n",
    "\"Type III tConv\",\n",
    "\"Type IV tConv\",\n",
    "\"Gammatone tConv\",\n",
    "\"Zero Phase tConv\",\n",
    "]\n",
    "branchnames=[\n",
    "'Branch 1',\n",
    "'Branch 2',\n",
    "'Branch 3',\n",
    "'Branch 4',\n",
    "]\n",
    "ax = plot_freq(logs=logs,min_epoch=100,metric='val_macc',min_metric=.6,figsize=(17,7),phase=True)\n",
    "# ax[3,0].set_ylim([-6,6])\n",
    "# ax[3,2].set_xlim([0,59])\n",
    "# ax[3,4].set_xlim([0,59])\n",
    "# for axes,branch in zip(ax[:,0],branchnames):\n",
    "#     axes.set_ylabel('%s Gain' % branch)\n",
    "for axes,log in zip(ax[3,:],lognames):\n",
    "    axes.set_xlabel('%s Weights' % log)\n",
    "# plt.subplots_adjust(left=0.035,bottom=0.065)\n",
    "# # plt.savefig('coeffs.eps')\n",
    "# # plt.savefig('coeffs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for axes,log in zip(ax[3,:],lognames):\n",
    "#     axes.set_xlabel('%s Weights' % log)\n",
    "# plt.subplots_adjust(left=0.035,bottom=0.065)\n",
    "plt.savefig('coeffsFreq.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs=[\n",
    "\"potes_fold0_noFIR 2019-03-16 18:44:45.597226\", # potes non balanced\n",
    "\"potes_fold0_noFIR 2019-03-02 13:01:33.636778\", # potes\n",
    "\"fold0_noFIR 2019-02-27 19:52:21.543329\", # Type1 macc\n",
    "\"fold0_noFIR 2019-03-07 14:44:47.022240\", # Type2 macc 80 epoch\n",
    "\"fold0_noFIR 2019-03-08 03:28:46.740442\", # Type3 sensitivity\n",
    "\"fold0_noFIR 2019-03-08 14:50:52.332924\", # type4 val_acc\n",
    "\"fold0_noFIR 2019-03-09 01:34:03.547265\", # gamma stage 1\n",
    "\"fold0_noFIR 2019-03-06 14:21:29.823568\", # zero stage2\n",
    "]\n",
    "lognames=[\n",
    "\"Potes-CNN\",\n",
    "\"Potes-CNN DBT\",\n",
    "\"Type I tConv\",\n",
    "\"Type II tConv\",\n",
    "\"Type III tConv\",\n",
    "\"Type IV tConv\",\n",
    "\"Gammatone tConv\",\n",
    "\"Zero Phase tConv\",\n",
    "]\n",
    "colors = [\n",
    "'#434B77',\n",
    "'#669966',\n",
    "'#c10061',\n",
    "'#ff51a5',\n",
    "'k',\n",
    "'#ffbe4f',\n",
    "#'#008080',\n",
    "'#DBBBBB',\n",
    "'#008080',\n",
    "         ]\n",
    "plot_metric(logs,lognames=lognames,smoothing=0.5,metric='val_loss',colors=colors,figsize=(10,8.5))\n",
    "plt.ylabel('Validation Loss per Cardiac Cycle')\n",
    "plt.ylim([0.44,0.65])\n",
    "# plt.savefig('validationLoss.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[\n",
    "#     'acc_a',\n",
    "    'acc_b',\n",
    "#     'acc_c',\n",
    "#     'acc_d',\n",
    "    'acc_e'\n",
    "]\n",
    "labels=[\n",
    "    'subset-a',\n",
    "#     'subset-b',\n",
    "#     'subset-c',\n",
    "#     'subset-d',\n",
    "    'subset-e'\n",
    "]\n",
    "ax = plot_metric([logs[0],logs[2]],metrics,smoothing=0.7,legendLoc=0,ylim=[.4,1.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylabel('Subset-wise Validation Accuracy')\n",
    "ax.legend(['Subset-a w/o DBT','Subset-e w/o DBT','Subset-a w/ DBT','Subset-e w/ DBT'],loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Activations and TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_labels = np.asarray([ord(each)- 97 for each in train_files+val_files+list(test_files)])\n",
    "meta_labels[meta_labels == 23] = 6\n",
    "y = np.argmax(np.concatenate([y_train,y_val,y_test]),axis=-1)\n",
    "\n",
    "for idx,each in enumerate(np.unique(meta_labels)):\n",
    "        indices = np.where(np.logical_and(y==1,meta_labels == each))\n",
    "        meta_labels[indices] = 7 +idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activations = np.array(get_activations(model,np.concatenate([x_train,x_val,x_test],axis=0),\n",
    "                                       batch_size=64,layer_name='flatten_1'))\n",
    "if activations.ndim > 2:\n",
    "    activations = np.reshape(activations,(len(activations),-1))\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_labels=meta_labels[0:len(activations)]\n",
    "quality_labels=np.concatenate([q_train,q_val,q_test],axis=0)[0:len(activations)]\n",
    "\n",
    "# rus = RandomUnderSampler(random_state=1,return_indices=True)\n",
    "# x,y,idx = rus.fit_resample(activations[quality_labels>0],meta_labels[quality_labels>0])\n",
    "np.random.seed(1)\n",
    "idx = np.random.choice(range(len(meta_labels)),size=(3792,),replace=False)\n",
    "x = activations[idx]\n",
    "y = meta_labels[idx]\n",
    "X_embed = scale(x)\n",
    "\n",
    "# X_embedded = PCA(n_components=50).fit_transform(X_embed)\n",
    "\n",
    "X_embedded = TSNE(n_components=2,\n",
    "#                   learning_rate=60,\n",
    "#                   early_exaggeration=1140.,\n",
    "                  perplexity=480, #480-2, 150-3 without exagg and lr\n",
    "                  init='random',\n",
    "                  n_iter=4000,\n",
    "                  verbose=1,\n",
    "                  ).fit_transform(X_embed)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "import matplotlib.font_manager as font_manager\n",
    "font_prop = font_manager.FontProperties(size=14)\n",
    "font_title = font_manager.FontProperties(size=20)\n",
    "\n",
    "colors = ['#434B77',\n",
    "          '#669966',\n",
    "          '#c10061',\n",
    "          '#ff51a5',\n",
    "          'k',\n",
    "          '#ffbe4f',\n",
    "#           '#008080',\n",
    "          '#DBEEEE',\n",
    "          '#008080',\n",
    "         ]\n",
    "# y_ = y_>6\n",
    "subsets = [\"Eko CORE Bluetooth\",\n",
    "\"Welch Allyn Meditron\",\n",
    "\"3M Littmann E4000\",\n",
    "\"AUDIOSCOPE\",\n",
    "\"Infral Corp. Prototype\",\n",
    "\"MLT201/Piezo\",\n",
    "\"JABES\",\n",
    "\"3M Littmann\"]\n",
    "parser = dict(zip(np.unique(y_),subsets))\n",
    "fig = plt.figure(figsize=(11,8))\n",
    "for stage,color in zip(np.unique(y_),colors):\n",
    "    mask = y_ == stage\n",
    "    plt.scatter(X_embedded[mask,0],X_embedded[mask,1],c=color,label=parser[stage])\n",
    "plt.legend(markerscale=2,fontsize=14)\n",
    "fig.set_tight_layout(tight=1)\n",
    "plt.xlabel('TSNE Component 1',fontproperties=font_prop)\n",
    "plt.ylabel('TSNE Component 2',fontproperties=font_prop)\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('potesTSNE.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_labels = np.asarray([ord(each)- 97 for each in train_files+val_files+list(test_files)])\n",
    "meta_labels[meta_labels == 23] = 6\n",
    "y = np.argmax(np.concatenate([y_train,y_val,y_test]),axis=-1)\n",
    "\n",
    "for idx,each in enumerate(np.unique(meta_labels)):\n",
    "        indices = np.where(np.logical_and(y==1,meta_labels == each))\n",
    "        meta_labels[indices] = 7 +idx\n",
    "meta_labels=meta_labels[0:len(activations)]\n",
    "np.random.seed(1)\n",
    "idx = np.random.choice(range(len(meta_labels)),size=(3792,),replace=False)\n",
    "y_= meta_labels[idx]\n",
    "y_[y_==11] = 14\n",
    "y_[y_>6] = y_[y_>6] - 7 # 0-7 steth labels\n",
    "y_ = y_+1\n",
    "y_[y_==7] = 0\n",
    "y_[y_==8] = 7\n",
    "print(np.unique(y_),np.bincount(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "conf = model_confidence(model,x_val,y_val)\n",
    "conf = cc2parts(conf,val_parts)\n",
    "plt.hist(conf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potes = \"/media/taufiq/Data1/Heart_Sound/Physionet/answers.txt\"\n",
    "pdf = pd.read_csv(potes, header=None)\n",
    "pdf.set_index(0,inplace=True)\n",
    "gt = \"/media/taufiq/Data1/Heart_Sound/Physionet/2016-07-25_Updated files for Challenge 2016/Online Appendix_training set.csv\"\n",
    "gtdf = pd.read_csv(gt)\n",
    "gtdf.set_index('Challenge record name',inplace=True)\n",
    "pdf = pdf.join(gtdf,how='left')\n",
    "files = pdf.index.str\n",
    "calc_metrics(true=pdf['Class (-1=normal 1=abnormal)']>0,pred=pdf[1]>0,files=pdf.index.str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam_rec(model,layer_name,cc,label,output_class='true',normalize=True,verbose=0):\n",
    "    '''\n",
    "    Generate class activation maps for whole recording\n",
    "    \n",
    "    Inputs:\n",
    "    model: model object\n",
    "    layer_name: layer to take grads of\n",
    "    cc: segmented cardiac cycles\n",
    "    label: corresponding class for each cc to generate activations w.r.t\n",
    "    \n",
    "    Outputs:\n",
    "    rec: concatenated cc\n",
    "    acti: concatenated CAMs\n",
    "    '''\n",
    "    \n",
    "    rec = []\n",
    "    activations = []\n",
    "    for idx,data in enumerate(cc):\n",
    "        if verbose:\n",
    "            print(\"Grad-CAM on CC-%d\" % idx)\n",
    "        data = np.expand_dims(data,axis=0)\n",
    "        \n",
    "        if output_class == 'true':\n",
    "            output = model.output[:,-(int(label[idx])+1)]\n",
    "        elif output_class == 'pred':\n",
    "            pred = np.argmax(model.predict(data,verbose=0),axis=-1)\n",
    "            output = model.output[:,-(int(pred)+1)]\n",
    "        else:\n",
    "            raise ValueError('output_class should be `true` or `pred`')\n",
    "        \n",
    "        last_conv_layer = model.get_layer(layer_name) ##### have to change the name here\n",
    "        grads = K.gradients(output, last_conv_layer.output)[0]\n",
    "        pooled_grads = K.mean(grads, axis=(0, 1)) ### no idea what to do here\n",
    "        iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "\n",
    "        pooled_grads_value, conv_layer_output_value = iterate([data])\n",
    "        for i in range(pooled_grads_value.shape[0]):\n",
    "            if verbose:\n",
    "                print(\"Iteration %d\" % i)\n",
    "            conv_layer_output_value[ :, i] *= pooled_grads_value[i]\n",
    "        heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "        if normalize:\n",
    "            print('normalizing')\n",
    "            try:\n",
    "                heatmap /= (np.std(heatmap)+ 1E-10)\n",
    "            except RuntimeWarning:\n",
    "                heatmap = heatmap\n",
    "        \n",
    "        \n",
    "        x = np.linspace(0, data.shape[1], num=len(heatmap))\n",
    "        y = heatmap\n",
    "        f1 = interp1d(x, y, kind='cubic')\n",
    "        xnew = np.linspace(0, data.shape[1], num=data.shape[1])\n",
    "        ynew = f1(xnew)\n",
    "        \n",
    "        end_idx = np.where(data!=0)[1][-1]\n",
    "        data = data[0,:end_idx,0]\n",
    "        ynew = ynew[:end_idx]\n",
    "        rec.append(data)\n",
    "        activations.append(ynew)\n",
    "    \n",
    "    return np.asarray(np.hstack(rec)),np.asarray(np.hstack(activations))\n",
    "\n",
    "def grad_cam_logs(logs,layer_name,cc,label,min_epoch=80,min_metric=.7,output_class='true',normalize=True,\n",
    "                  xlim=None,figsize=(12,8),lognames=None,colors=None,window='flat',win_size=50,\n",
    "                metric='val_macc',model_dir='/media/taufiq/Data1/heart_sound/models/',verbose=0):\n",
    "    '''\n",
    "    Plot Grad_CAM for logs with predictions\n",
    "    '''\n",
    "    parser={0:'Normal',1:'Abnormal'}\n",
    "    \n",
    "    if not type(logs) == list:\n",
    "        logs = [logs]\n",
    "    \n",
    "    activations = []\n",
    "    predictions = []\n",
    "    for log_name in logs:\n",
    "        model = load_model(log_name,verbose=verbose)\n",
    "        weights = get_weights(log_name,min_epoch=min_epoch,\n",
    "                              min_metric=min_metric,verbose=verbose)\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "        if verbose:\n",
    "            print(\"GRAD CAM for %s\" % log_name)\n",
    "        _,acti = grad_cam_rec(model,layer_name,cc,label,\n",
    "                              verbose=verbose,\n",
    "                              normalize=normalize,\n",
    "                              output_class=output_class)\n",
    "        pred = cc2rec_labels(cc,model.predict(cc)[:,1])\n",
    "        activations.append(acti)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    rec = cc2rec(cc)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = plt.GridSpec(3, 4, hspace=0.2, wspace=0.2,)\n",
    "    main_ax = fig.add_subplot(grid[0,:],\n",
    "                              ylabel='PCG',\n",
    "#                               ylabel='%s PCG'%parser[label[0]]\n",
    "                             )\n",
    "    pred_ax = fig.add_subplot(grid[1, :], ylabel='Predictions', sharex=main_ax)\n",
    "    acti_ax = fig.add_subplot(grid[2, :], ylabel='Activations', sharex=main_ax)\n",
    "    \n",
    "\n",
    "    t = np.linspace(0,len(rec)/1000,num=len(rec))\n",
    "    main_ax.plot(t,rec)\n",
    "    main_ax.set_xlim([0,t[-1]])\n",
    "    \n",
    "    if colors is not None:\n",
    "        for acti,pred,color in zip(activations,predictions,colors):\n",
    "            acti_ax.plot(t,smooth_win(acti/np.std(acti),window_len=win_size,window=window),color=color)\n",
    "            pred_ax.plot(t,pred,color=color)\n",
    "            \n",
    "    else:\n",
    "        for acti,pred in zip(activations,predictions):\n",
    "            acti_ax.plot(t,smooth_win(acti/np.std(acti),window_len=win_size,window=window))\n",
    "            pred_ax.plot(t,pred)\n",
    "    \n",
    "    pred_ax.set_ylim([0,1])\n",
    "    if xlim is not None:\n",
    "        main_ax.set_xlim(xlim)\n",
    "    if lognames is not None:\n",
    "        acti_ax.legend(lognames)\n",
    "        \n",
    "    return [main_ax,pred_ax,acti_ax]\n",
    "\n",
    "def smooth_win(x,window_len=11,window='hanning'):\n",
    "        if x.ndim != 1:\n",
    "                raise ValueError, \"smooth only accepts 1 dimension arrays.\"\n",
    "        if x.size < window_len:\n",
    "                raise ValueError, \"Input vector needs to be bigger than window size.\"\n",
    "        if window_len<3:\n",
    "                return x\n",
    "        if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "                raise ValueError, \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\"\n",
    "        s=np.r_[2*x[0]-x[window_len-1::-1],x,2*x[-1]-x[-1:-window_len:-1]]\n",
    "        if window == 'flat': #moving average\n",
    "                w=np.ones(window_len,'d')\n",
    "        else:  \n",
    "                w=eval('np.'+window+'(window_len)')\n",
    "        y=np.convolve(w/w.sum(),s,mode='same')\n",
    "        return y[window_len:-window_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "conf = model_confidence(model=model,data=x_val,labels=y_val, verbose=1)\n",
    "conf = cc2parts(conf,val_parts)\n",
    "plt.hist(conf,40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = np.logical_and(conf>.8,conf<.9)\n",
    "_,idx = np.where([cond])\n",
    "print('Number of Recordings within condition',len(idx))\n",
    "\n",
    "target_idx = np.random.randint(len(idx))\n",
    "print('Target Recording from subset-',cc2parts(val_files,val_parts)[idx[target_idx]])\n",
    "\n",
    "cc_idx = idx_parts2cc([idx[target_idx]],val_parts)\n",
    "\n",
    "target_data = x_val[cc_idx]\n",
    "target_labels = y_val[:,1][cc_idx]\n",
    "print('Target Recording Class',target_labels[0])\n",
    "target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Training Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,len(target_data)/1000,num=len(target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc2parts(train_files,train_parts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'a0182.wav'\n",
    "\n",
    "filenames = pd.read_csv('/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/train_files.txt',header=None)\n",
    "idx = np.where(filenames[0]==target)[0]\n",
    "print(idx)\n",
    "cc_idx = idx_parts2cc(idx,train_parts)\n",
    "target_data = x_train[cc_idx]\n",
    "target_labels = y_train[:,1][cc_idx]\n",
    "print('Target Recording Class',target_labels[0])\n",
    "print('Number of cc',target_data.shape[0])\n",
    "\n",
    "fig = plt.figure()\n",
    "rec = cc2rec(target_data[:6])\n",
    "plt.plot(np.linspace(0,len(rec)/1000,num=len(rec)),rec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Validation Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'b0003'\n",
    "filenames = pd.read_csv('/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/validation0.txt',header=None)\n",
    "idx = np.where(filenames[0]==target)[0]\n",
    "cc_idx = idx_parts2cc(idx,val_parts)\n",
    "\n",
    "target_data = x_val[cc_idx]\n",
    "target_labels = y_val[:,1][cc_idx]\n",
    "print('Target Recording Class',target_labels[0])\n",
    "print('Target Recording cc',target_labels.shape)\n",
    "target_data.shape\n",
    "\n",
    "fig = plt.figure()\n",
    "rec = cc2rec(target_data)\n",
    "plt.plot(np.linspace(0,len(rec)/1000,num=len(rec)),rec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 188\n",
    "cc_idx = idx_parts2cc(idx,test_parts)\n",
    "\n",
    "target_data = x_test[cc_idx]\n",
    "target_labels = y_test[:,1][cc_idx]\n",
    "print('Target Recording Class',target_labels[0])\n",
    "target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs=[\n",
    "\"potes_fold0_noFIR 2019-03-16 18:44:45.597226\", # potes non balanced\n",
    "\"potes_fold0_noFIR 2019-03-02 13:01:33.636778\", # potes\n",
    "\"fold0_noFIR 2019-02-27 19:52:21.543329\", # Type1 macc\n",
    "# \"fold0_noFIR 2019-03-07 14:44:47.022240\", # Type2 macc 80 epoch\n",
    "# \"fold0_noFIR 2019-03-08 03:28:46.740442\", # Type3 sensitivity\n",
    "# \"fold0_noFIR 2019-03-08 14:50:52.332924\", # type4 val_acc\n",
    "# \"fold0_noFIR 2019-03-09 01:34:03.547265\", # gamma stage 1\n",
    "\"fold0_noFIR 2019-03-06 14:21:29.823568\", # zero stage2\n",
    "]\n",
    "lognames=[\n",
    "\"Potes-CNN\",\n",
    "\"Potes-CNN DBT\",\n",
    "\"Type I tConv\",\n",
    "# \"Type II tConv\",\n",
    "# \"Type III tConv\",\n",
    "# \"Type IV tConv\",\n",
    "# \"Gammatone tConv\",\n",
    "\"Zero Phase tConv\",\n",
    "]\n",
    "colors = [\n",
    "'#434B77',\n",
    "'#669966',\n",
    "'#c10061',\n",
    "'#ff51a5',\n",
    "'k',\n",
    "'#ffbe4f',\n",
    "'#DBBBBB',\n",
    "'#008080',\n",
    "         ]\n",
    "\n",
    "cc_start = 0\n",
    "cc_end = 8\n",
    "ax = grad_cam_logs(logs,'concatenate_1',target_data[cc_start:cc_end],target_labels[cc_start:cc_end],win_size=10,\n",
    "                   lognames=lognames,colors=colors,output_class='pred',normalize=True)\n",
    "ax[1].set_yticks([0,.25,.5,.75,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(3.5,5)\n",
    "ax[1].set_yticks([0,.25,.5,.75,1])\n",
    "ax[2].set_ylim([-.1,6])\n",
    "# fig.savefig('Normal.eps')\n",
    "\n",
    "# plt.savefig('MRgradCAM.eps')\n",
    "# plt.xlim([0,4.7])\n",
    "\n",
    "ax[2].legend_ = None\n",
    "# ax[2].legend(lognames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_logs(logs,lognames,bins=5,figsize=(10,2),verbose=0):\n",
    "    fig,ax = plt.subplots(1,len(logs),sharey='row',figsize=figsize)\n",
    "    for axes,log_name,model_name in zip(ax,logs,lognames):\n",
    "        model = load_model(log_name,verbose=0)\n",
    "        weights = get_weights(log_name,min_epoch=100,min_metric=.7)\n",
    "        metric = 'val_macc'\n",
    "        model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "\n",
    "        for subset in np.unique(val_files):\n",
    "            mask = np.asarray(val_files) == subset\n",
    "            part_mask = cc2parts(val_files,val_parts) == subset\n",
    "            conf = model_confidence(model=model,data=x_val[mask],labels=y_val[mask], verbose=verbose)\n",
    "            conf = cc2parts(conf,val_parts[part_mask])\n",
    "            sns.distplot(conf,bins=bins,label=\"Subset-%s\"%subset,ax=axes)\n",
    "\n",
    "        conf = model_confidence(model=model,data=x_test,labels=y_test, verbose=verbose)\n",
    "        conf = cc2parts(conf,test_parts)\n",
    "        sns.distplot(conf,bins=bins,label='HSSDB',ax=axes)\n",
    "        axes.set_title('%s C-DIST'%model_name)\n",
    "#         axes.legend(loc='upper center', bbox_to_anchor=(0.5, 1.00),&nbsp; shadow=True, ncol=2)\n",
    "    return ax\n",
    "ax = plot_confidence_logs(logs,lognames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chartBox = ax[2].get_position()\n",
    "# ax[0].set_position([chartBox.x0, chartBox.y0, chartBox.width*0.6, chartBox.height])\n",
    "ax[2].legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), shadow=True, ncol=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
